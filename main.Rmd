---
title: "Programming with Data"
subtitle: "Spring 2021"
author: "Brenton M. Wiernik"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
biblio-style: apalike
link-citations: yes
#github-repo: org/repo
cover-image: "datasci_prog_class_icon.png"
favicon: "favicon.ico"
---

# About This Guide {-}

Welcome to the class guide for Programming with Data for Spring 2021! 
This guide organizes what we will be doing in each class meeting. 
So you can expect it to be updated regularly -- in fact, the date listed above is the last time this guide was updated. 

This course was developed in part based on the resources provided by Jenny Bryan found at <https://stat545.com>.

<!--chapter:end:index.Rmd-->

# R, RStudio, RMarkdown

## Outline

We'll cover two topics today:

- Working with R scripts
  - Functions
  - Objects
  
- Writing with RMarkdown
  - Documents
  - Code blocks
  - Knitting

We'll end class with a to-do list before next class.


## Learning Objectives

By the end of today's class, you will be able to:

- Write an R script to perform simple calculations
- Access R help files to figure out how a function works
- Perform basic R tasks
  - Functions and operators
  - Subset vectors
  - Explore a data frame
  - Load packages
- Write documents in RMarkdown (Rmd) and render these documents to HTML with RStudio.
- Style an Rmd document by editing the YAML header


## Participation

Each class meeting, you will complete coding activities. 
These will include both scripts you write locally and guided activities with the _progdata_ package.

In the **Participation** section at the top of each day's chapter of this guide, I will list the activities you should complete and any deliverables that you should retain for homework.


For today's participation, you will make two files:
  1. exploring_r.R
  1. exploring_markdown.Rmd
  

## Resources

Here are some useful resources for getting oriented with R.

- Jenny Bryan's [stat545.com: hello r](http://stat545.com/block002_hello-r-workspace-wd-project.html) page for exploring R roughly follows today's outline.
- Want to practice programming in R? Check out [R Swirl](https://swirlstats.com/) for interactive lessons.
- For a list of R "vocabulary", see [Advanced R - Vocabulary](http://adv-r.had.co.nz/Vocabulary.html)
- For a list of R operators, see [Quick-R](https://www.statmethods.net/management/operators.html).


## Working with R Scripts

Make a new R script in RStudio: 
  - File → New File → R Script 
  - Or type Ctrl/Cmd + Shift + N
  
An R script file name ends with `.R` and it contains **only R commands**. 
When you program your data analysis projects, one project might involve many individual R script files.

Save your R script file by clicking the Save icon or typing Ctrl/Cmd + S.
Give it an informative name like `exploring_r.R`.

Let's explore some basic R programming concepts with this script file.

### Objects

Everything is an **object**. An object has a _name_ and _contents_. 
Objects can be values (e.g., `4`, `pi`), data (e.g., `mtcars`), or functions (e.g., `mean`).

You call an object by typing its name. Try it:

```{r}
3

pi

mtcars

mean(c(1, 3, 3))
```

When we just type an object's name like this, R will **print** out its value in the console.
This is the same as typing out the print function: `print( object )`

If you try to call an object that doesn't exist, you get an error:

```{r, error=TRUE}
apple
```

You **assign** something to an object using the arrow operator `<-`.
RStudio has a keyboard shortcut to insert the arrow quickly: Alt + -

```{r}
number <- 4
```

Once you've assigned a value to an object, you can perform additional operations with that object by calling its name:

```{r}
number 
number * 2

number

number <- number * 2
number
```

Notice that the stored value of `number` does not change when we perform an operation with it.
**The value of an object only changes when you _assign_ it!**


### Vectors

_Vectors_ store multiple entries of one data type, like numbers or characters. 
You'll discover that they show up just about everywhere in R.

Let's collect some data and store this in a vector called `times`. 

**How many hours did you sleep last night?** 

Drop your answer in the chat.

Here's starter code:

```{r, eval=FALSE}
times <- c()
```

```{r, include=FALSE}
times <- round(runif(20, 4, 10))
```

The `c()` function is how we make a vector in R. The "c" stands for "concatenate".

Operations happen component-wise. Change those times to minutes. How can we "save" the results?

All parts of a vector have the same **type**. 
There are many types of variables in R.
The most common types are:
  1. numeric (numbers)
    1. double (numbers with decimal values; `2`, `3.4`, `1000`)
    1. integer (`1L`, `2L`, `100L`)
  1. character (words or strings; `"a"`, `"foo"`, `"lastname"`)
  1. logical (`TRUE` or `FALSE`)
  1. factor (categorical variable; `factor(c("control", "experiment"))`)
  
  
### Functions

R comes with many many **functions**.
Functions take one or more _inputs_ and return one or more _outputs_.
You can think of functions as prewritten R programs.

Functions all take the general form:

```{r, eval = FALSE, tidy = FALSE}
functionName(arg1 = val1, arg2 = val2, and so on)
```

To call a function, type its name, then parentheses `()`. 
Inside the `()`, type the arguments and values to use as input.

What's the average sleep time? Let's compute that using the `mean()` function. 

```{r}
mean(times)
```

To learn how a function works, we can look at its **help file**. 
Open the help file for `mean()` by typing `?mean` or `help(mean)` in the console.

The help file will includes the following:
  1. A brief description of the function
  1. A list of the arguments and how to call the function
  1. A detailed description of the arguments
  1. (Optionally) Other usage details
  1. Coded examples
  
We can see that `mean()` has 4 **arguments**. 
  1. `x`: A vector to compute the mean of
  1. `trim`: the fraction (0 to 0.5) of observations to be trim from each end
  1. `na.rm`: `TRUE` or `FALSE`--should missing values be removed before computing?
  1. `...`: Other arguments. More on that later.
  
**Default values** for arugments are given in the Usage section by ` = `.
If an argument has no default (like `x` in `mean()`), it usually means it's `required`.

Let's compute the trimmed mean of `times`, dropping 10% of values from each end.

```{r, eval=FALSE}

```

You can either enter arguments **in order**:

```{r}
mean(times, .1)
```

Or **by name**:
```{r}
mean(times, trim = .1)
```

It's good practice to _name all arguments after the first (or maybe second if its clear)_.

Try out some other functions, such as `sd()`, `range()`, and `length()`.

Much of R is about becoming familiar with R's "vocabulary". 
A nice list can be found in [Advanced R - Vocabulary](http://adv-r.had.co.nz/Vocabulary.html).


### Comparisons

How many people slept less than 6 hours? Let's answer that using _comparisons_.

We can compare the values of `times` to another value using `<`.

```{r}
times < 6
```

Comparisons return a vector of **logical** values. 

We can do other logical comparisons:

```{r}
times > 6

times == 5

times <= 7

times != 2
```

We can combine multiple comparisons using `&` (AND), `|` (OR), and `!` (NOT).

```{r}
(times < 4) | (times > 9)
```

Try out these functions that work with logical values.

```{r}
any(times < 6)

all(times < 6)

which(times < 6)
```


### Subsetting

Use `[]` to subset values from a vector. 
You can subset with an integer (by position) or with logicals.

```{r}
times[4]
times[c(2, 5)]

times[-6]
times[-c(2, 3)]

times[4:8]

times[times >= 7]
```

Subset `times`:

1. Extract the third entry.
2. Extract everything except the third entry.
3. Extract the second and fourth entry. The fourth and second entry.
4. Extract the second through fifth entry.
5. Extract all entries that are less than 4 hours Why does this work? Logical subsetting!


### Modifying a vector

You can change the vector by combining `[]` with `<-`.

Let's say that we learned that the second time was incorrect and we wanted to replace it with missing data.
In R, missing data is noted by `NA`. 

Replace the second entry in `times` with `NA`.

```{r}

```

Now, let's "cap" all entries that are bigger than 8 at 8 hours.

```{r}

```

If this is more than one value, why don't we need to match the number of values? Recycling!
Be careful of recycling!

Let's compute the mean of these new times:

```{r}
mean(times)
```

What happened? How do we compute the mean of the non-missing values?

```{r}

```


### Data frames

We usually work with more than one variable at a time. 
When we do that, we will work with **data frames**. 
A data frame is a **list** of vectors, all of the same length.

R has some data frames "built in". For example, some car data is attached to the variable name `mtcars`. 

Print `mtcars` to screen. Notice the tabular format. 

__Your turn__: Finish the exercises of this section:

  1. Use some of these built-in R functions to explore `mtcars`:
    - `head()`
    - `tail()`
    - `str()`
    - `nrow()`
    - `ncol()`
    - `summary()`
    - `row.names()`
    - `names()`

  2. What's the first column name in the `mtcars` dataset? 
  3. Which column number is named `"wt"`?
  
With data frames,each column is its own vector.
You can extract a vector by name using `$`. 
For example, we can extract the `cyl` column with `mtcars$cyl`.

You can also extract columns using ``[[]]`. 
For example, try `mtcars[["cyl"]]` or `mtcars[[2]]`.

  4. Extract the vector of `mpg` values. What's the mean `mpg` of all cars in 
     the dataset?


### R packages

Often, the suite of functions that "come with" R are not enough to do an analysis. 

Sometimes, the suite of functions that "come with" R are not very convenient.

In these cases, R _packages_ come to the rescue. 
These are "add ons", each coming with their own suite of functions and objects, usually designed to do one type of task. 
[CRAN](https://cran.r-project.org/) stores many R packages that. 
It's easy to install packages from CRAN using the `install.packages()` function. 

Run the following lines of code to install the `tibble` and `gapminder` packages. 
(But don't include `install.packages()` lines in your scripts—it's not very nice to others!)

```{r, eval=FALSE}
install.packages("tibble")
install.packages("palmerpenguins")
```

- `tibble`: a data frame with some useful "bells and whistles"
- `palmerpenguins`: a package with the penguins dataset (as a `tibble`!)

After you install a package, you need to _load_ it to use it.
Use the `library()` function to load a package. 

(Note: Do not use the similar function `require()` to load packages. This has some different, undesirable, behavior for normal usage.)

Run the following lines of code to load the packages. (Put these in your scripts, and near the top.)

```{r}
library(tibble)
library(palmerpenguins)
```

You can explore the objects in a package in the Environment pane.

Try the following two approaches to access information about the `tibble` package. 
Run the lines one-at-a-time. Vignettes are your friend, but do not always exist. 

```{r, eval=FALSE}
?tibble
browseVignettes(package = "tibble")
```

Print out the `penguins` object to screen. 
It's a tibble—how does it differ from a data frame in terms of how it's printed?


## Authoring with RMarkdown

In the first part of class, we practiced writing R code. 
In this half, we will practice combining our R scripts with other text, such as descriptions of the analyses, explanations, interpretations, or even a whole manuscript or report.
Communication of a data analysis is just as important as the analysis itself. 
We will use a scripting language called "Markdown" to write text for our R scripts. 
Markdown is a "lightweight" and very readable language for describing text styling.


## Resources

Cheat sheets for "quick reference":

- [GitHub's markdown cheatsheet](https://guides.github.com/pdfs/markdown-cheatsheet-online.pdf)
- [RStudio's RMarkdown cheatsheet](http://www.rstudio.com/wp-content/uploads/2016/03/rmarkdown-cheatsheet-2.0.pdf)

Further reading:

- The [Rmd website](https://rmarkdown.rstudio.com/) has a fantastic walk-through [tutorial](https://rmarkdown.rstudio.com/lesson-1.html) that gives a great overview of RMarkdown. 
- There's also a nice [overview video](https://rmarkdown.rstudio.com/authoring_quick_tour.html) on the Rmd website, too.
- Yihui's [Rmd book](https://bookdown.org/yihui/rmarkdown/) for lots more on RMarkdown.

Other explorations of this content:

- Interactive [tutorial](https://commonmark.org/help/tutorial/) for learning markdown.
- The [Happy Git with R: Rmd test drive](https://happygitwithr.com/rmd-test-drive.html).


## Output formats

There are generally two prominent file types to display manuscripts of various 
types:

  1. __pdf__: This is useful if you intend to print your work onto a physical sheet of paper, or for presentation slides. 
    If this is not the primary purpose, then try to avoid it, because formatting things so that it fits to the page can be more effort than its worth (unless you're making presentation slides). 
    - Example: Most journals articles and preprints.
  2. __html__: This is what you see when you visit a webpage. Content does not need to be partitioned to pages. 
    - Example: My [website main page](https://wiernik.org), and its corresponding [html file](https://github.com/bwiernik/wiernik_org/blob/master/public/index.html).

We'll be treating pdf and html files as _output_ that should not be edited. 

What's the source, then? (R)__Markdown__!


### Word Processor Formats

It is also possible to output files to word processor formats, such as Word (.docx), LibreOffice/OpenDocument (.odt), or Rich Text (.rtf). You can also output to other slideshow software, such as PowerPoint (.pptx) or LibreOffice/OpenDocument Slides (.odp).

We aren't going to use these in this class, and it's a good idea to avoid them.
It's hard to integrate these formats into a reproducible workflow that lets you re-run your analyses as you update your data and code. 

There are times when you have to use these formats (e.g., a journal requires Word, a conference requires PowerPoint, your advisor or collaborator requires Word). 
If you have to do this, try to avoid _editing in these formats_. 
If you need to make revisions, go back and make changes to the source code, rather than the rendered document. Later on in the semester, if we have time, we will talk about some ways to incorporate going to Word and similar formats and back into your workflow.

### Other Output Formats

RMarkdown can be rended to _many_ other formats that we won't have time to cover
(see the [RMarkdown documentation](https://bookdown.org/yihui/rmarkdown/output-formats.html)
and the [pandoc documentation](https://pandoc.org/MANUAL.html#option--to)).


## Markdown

Markdown is plain text with a straightforward, readable way of marking up your text. 
Let's see [GitHub's cheat sheet](https://guides.github.com/pdfs/markdown-cheatsheet-online.pdf). 

Various software convert markdown to either pdf or html. 
The main tool we will use is called pandoc, which is helpfully built into RStudio.

File extension: .md (no R code) or .Rmd (with R code)

### Make a Markdown document

  1. Make a new RMarkdown file in RStudio, then save it as `exploring_markdown.Rmd`.
  2. Add some text, such as introducing yourself and what your favorite animal is. 
  3. Mark up the text with some markdown features (e.g., bold, italic, bullets, a link to a URL on the internet).

### Render `exploring_markdown.Rmd`

We can use RStudio to convert our plain text Markdown document into various output formats. 
Above the script editor in RStudio, click the `Preview` or `Knit` button and convert your file to HTML.


## RMarkdown

RMarkdown (Rmd) is a "beefed up" version of markdown. 
It has two special features that we can use for powerful programming. 

  1. We can integrate code (from R, Python, C++, SQL, etc.) into a document.
  1. We can specify document options a _YAML header_.
    - This contains _metadata_ about the document to guide how the Rmd document is rendered.

Here's [RStudio's cheat sheet](http://www.rstudio.com/wp-content/uploads/2016/03/rmarkdown-cheatsheet-2.0.pdf) on Rmd. 

You can see that it has more features than "regular" markdown!


### Code Chunks 

The parts of your document inside the "fences" <code>```</code> are **code chunks**.
When you render the RMarkdown document, R will run the code in the chunks and show the output in the rendered document.

You can run the code from a chunk interactively by placing your cursor on the line and typing Ctrl/Cmd + Enter/Return or by clicking the green "play" button at the top right of the code chunk.

Add a new code chunk by doing one of these:
  - Clicking the Insert button and choosing R or by typing -> "R"
  - Typing Mac: `Cmd + Option + I` or Windows: `Ctrl + Alt + I`
  - Manually typing three back ticks followed by {r} in curly brackets: `` ```{r} ``, 
    then typing three back ticks on a later line to "close" the code block:
    `` ``` ``.

Add a code chunk near the top of the file and load the _tibble_ package.

```{r}
library(tibble)
library(knitr)
```

If you don't have _knitr_ installed, install it with `install.packages("knitr")`.


### Rendering output

In a new code chunk, convert the `mtcars` data frame to a tibble using the `tibble::as_tibble()` function and assign it as a new object (e.g., called `mtcars_tbl`). Print it out using the `print()` function.
   
When you print with just the `print()` function, your table will look like R console script in your output HTML or PDF. 
To make your tables look nicer in the output, use the `knitr::kable()` function to convert the results to a Markdown table. 
In a new code chunk, print the `mtcars_tbl` using `knitr::kable()`.

We will explore other table tools in future classes.
   
Add some markdown commentary about the tables you are showing. 
Your markdown commentary needs to go outside of the code chunks.

You can also include R code "in-line" with markdown text. 
This is useful, for example, in your results section of a paper to report the results of your analyses without having to copy-paste them (and make errors). 
   
Add an in-line code chunk specifying the number of rows of the `mtcars` dataset like this:
   - `` The `mtcars` dataset has `r nrow(mtcars)` rows. ``
   
Now, "Knit" to HTML.


### YAML Headers

Now, we'll modify the metadata via the YAML header. 
Check out a bunch of YAML options [from the RMarkdown book](https://bookdown.org/yihui/rmarkdown/html-document.html).

YAML is used to control things about your document, such as the title, the creation date shown, output formats and options, even variables that might be used in R code.

In your Rmd file:

1. Change the output to `html_document`. We'll be specifying settings for the 
   html document, so this needs to go on a new line after the `output:` field:

```
output:
  html_document:
    SETTINGS
    GO
    HERE
```

Note that lines that are a sub-part of the previous line are **indented by two spaces**.

2. Add the following settings under `html_document`:
    - Add a theme. A nice one is paper: `theme: paper`
    - Add a table of contents with `toc: true`
    - Make the toc float: `toc_float: true`
    - Make the code hideable using: `code_folding: "hide"`
    
3. Knit the results.

4. You can even include R code that runs in your YAML header. For example, let's
   add code that will insert today's date into the header. Change the `date:` field
   in your YAML to:
   
   ```{r, eval=FALSE}
   Last updated `r format(Sys.time(), "%d %B, %Y")`
   ```
   
   - Note, if you want to start a YAML field with code, wrap it in quotes
     
     ```{r, eval=FALSE}
     '`r format(Sys.time(), "%d %B, %Y")`'
     ```
     
5. Change the output format to `github_document` and knit again. 
   This is the format we will use for your homework.


### Chunk Options

Just like YAML is metadata for the Rmd document, _code chunk options_ are metadata for the code chunk. 
Specify them within the `{r}` at the top of a code chunk, separated by commas.

Add to your RMarkdown document:

1. Add a name to the code chunk after the `r` and a space.
2. Hide the code from the output with `echo = FALSE`.
3. Prevent warnings from the chunk that loads packages with `warning = FALSE`. 
4. Knit the results.


## Wrap-up

Save the following files. We will use them next class. 

  1. `exploring_r.R`
  2. `exploring_markdown.Rmd` and its output formats (PDF, HTML).


## To do before next class

- Make an account on [GitHub](https://github.com/).
  - Make your username recognizable!
  - Please put up a profile photo or image on GitHub—it makes the class community more personable and easier to work with.
- Finish any in-class activities listed in today's section of the guidebook that you didn't get done.
- Install the software stack for this course, as indicated below.

Optionally, register for the [Student Developer Pack](https://education.github.com/pack) with GitHub for a bunch of free perks for students!

### Software Stack Installation

1. Install R and RStudio.
    - R here: <https://cloud.r-project.org>
    - RStudio here: <https://www.rstudio.com/products/rstudio/download/preview/>
    - Commentary on installing this stuff can be found at [stat545.com: r-rstudio-install](http://stat545.com/block000_r-rstudio-install.html)
2. Install git (this is different from GitHub!). See [happygitwithr: Section 7](http://happygitwithr.com/install-git.html)
3. Install GitHub: <https://desktop.github.com/>

<!--chapter:end:s001_R-RStudio-RMarkdown.Rmd-->

# Introduction to GitHub and Version Control

__Announcements__:

- The first assignment is due for Peer Review on Monday, final on Wednesday
- The first peer review is due on Tuesday
- Please be sure you have:
  1. Made a participation repo
      - You only need 1 for the entire semester!
  1. Submitted your participation repo link on canvas
  1. Made a homework repo
      - You only need 1 for the entire semester!
  1. Accepted your invitation to the class Organization

## Outline

We'll cover four topics today:

- Git and GitHub
- Version control
- Submitting assignments
- Asking questions

## Learning Objectives

By the end of today's class, you will be able to:

- Distinguish and navigate between GitHub repositories, Organization accounts, 
  and user accounts.
- Edit plain text files on GitHub.
- Navigate the commit history of a repository and a file on GitHub.
- Contribute to GitHub Issues, especially for this class.
- Identify whether a software-related question has a reproducible example.

## Resources

If you want to learn more about today's topics, check out:

- The [GitHub guide](https://guides.github.com/) has lots of info about GitHub. 
  If you do go here, I recommend you start with 
  ["Hello, World!"](https://guides.github.com/activities/hello-world/). You'll 
  see stuff about branching there -- we'll be discussing that next week.
- Dr. Jenny Bryan's ["How to get unstuck"](https://wiernik-datasci.netlify.com/help-general/) 
  page is useful for getting help online.

## Topic 2: GitHub (35 min)

(2 min)

We will be using [GitHub](https://github.com) a lot in this course:

- All course-related work will go on GitHub.
- Discussion will happen on GitHub.
- Class materials are on GitHub.

But why GitHub? Because it's tremendously effective for developing a project. Examples:

- [Apple](https://github.com/apple) uses it.
- [Uber](https://github.com/uber) uses it.
- [Netflix](https://github.com/Netflix) uses it.
- [This Guidebook](https://github.com/USF-Psych-DataSci/Classroom) and [the class website](https://github.com/USF-Psych-DataSci/DataSci-home) use it.
- Prominent R packages like [`ggplot2`](https://github.com/tidyverse/ggplot2) use it. 

Today, we'll check out:

1. GitHub as cloud storage;
2. GitHub for collaboration; and
3. GitHub for version control with git.

### Register a GitHub account - Activity (4 min)

Your turn:

1. Register for a free account on [github.com](https://github.com). 
    - You'll be using this account for the duration of the course.
    - Give your username [some thought](https://happygitwithr.com/github-acct.html#username-advice) -- ideally, should include your name.
2. Tell me what your username is by filling out [this survey](https://usf.az1.qualtrics.com/jfe/form/SV_8kAdI0XraWU6geN).

### GitHub as cloud storage (4 min)

At the very least, GitHub allows for cloud storage, like Google Drive and Dropbox do. There's a bit more structure than just storing files under your account:

- __Repositories (aka "repo")__: All files must be organized into _repositories_. Think of these as self-contained projects. These can either be _public_ or _private_.
- __User Accounts__ vs. __Organization Accounts (aka "Org")__: All repositories belong to an account:
    - A _user account_ is the account you just made, and typically holds repositories related to your own work. 
    - An _Organization_ account can be owned by multiple people, and typically holds repositories relevant to a group (like STAT 545).

Examples: 

- The [`ggplot2`](https://github.com/tidyverse/ggplot2) repo, within its corresponding `tidyverse` Org. 
- My [website](https://github.com/bwiernik/wiernik_org) repo, within my own user account.

Want to read more about GitHub accounts? [Check out this help page on GitHub](https://help.github.com/en/articles/types-of-github-accounts).

### GitHub as cloud storage - Activity (10 min)

__Together: Make a participation repo__

- Follow the [setup instructions](https://wiernik-datasci.netlify.com/evaluation/participation/#setup) on the participation page. 

__Navigating GitHub__

1. Together: Make a new file on your participation repository:
    - Click on the "Create New File" button on your repository's home page.
    - Call it `navigating_github.md`
    - Leave it blank, and commit ("save") the file by clicking on green "commit new file" button at the bottom of the page.
2. Together: Add the following URL's to your `navigating_github.md` file (click on the pen button to edit), together with some commentary:
    - The repository for the class home page, called `DataSci-home` (use this if the site ever goes down!)
    - The account it's under.
    - Whether the account is a _user account_ or an _Org_.
3. Together: Commit the changes.
4. Your turn: Continue the exercise, and add more URL's (with more commentary):
    - The URL to your participation repo
    - The URL to your user account page
5. Your turn: Commit the changes.

### GitHub for collaboration (4 min)

The "traditional" way to collaborate involves sending files over email. Problems:

- Easily lose track of who has the most recent version.
- Emails get buried.

Addressed by GitHub:

- GitHub repository treated as the "master version".
- Use [_GitHub Issues_](https://guides.github.com/features/issues/) instead of email.

_Issues_ are a discussion board corresponding to a particular repository. One "thread" is called an Issue. Some features:

- Tag other GitHub users using `@username`.
- Get email notifications if you are tagged, or are `Watch`ing a repository.

As an example, check out the Issues in the [`ggplot2`](https://github.com/tidyverse/ggplot2) repository.

More on collaboration next Thursday.

### GitHub for collaboration - Activity (1 min)

__Together: `Watch`ing the `Announcements` repo__

1. Navigate to class [Announcements](https://github.com/USF-Psych-DataSci-2020/Announcements) repository.
2. Click `Watch` on the upper-right corner of the repo

You should now get an email notification whenever an Issue is posted.

### GitHub for version control with git (5 min)

GitHub uses a program called `git` to keep track of the project's history (more about `git` next Thursday).

- Users make "commits" to form a _commit history_.
- `git` only tracks the _changes_ associated with a commit, so it doesn't need to take a snapshot of all your files each time.
- The actual changes are called a _diff_.

Demostration:

- View commit history of the [STAT545-home](https://github.com/STAT545-UBC/STAT545-home) repository by clicking on the "commits" button on the repo home page.
- View a recent diff by clicking on the button with the _SHA_ or _hash_ code (something like `6c0a5f1`).
    - This is also useful for collaborators to see exactly what you changed.
- View the repository from a while back with the `<>` button.
- View the history of a file by clicking on the file, then clicking "History".

Why version control?

- Don't fret removing stuff
- Leave a breadcrumb trail for troubleshooting
- "Undo" and navigate a previous state
- Helps you define your work
- ...

### GitHub for version control with git - Activity (5 min)

__Your turn: History of the [`STAT545-UBC/Classroom`](https://github.com/STAT545-UBC/Classroom) repository.__

1. Use the commit history of the [`STAT545-UBC/Classroom`](https://github.com/STAT545-UBC/Classroom) repository to find Assignment 01 that was delivered last year in STAT 545A (Note: the course ended in mid October 2018, and the assignments were held in a folder called `assignments`). 
2. Add the URL of this assignment to your `navigating_github.md` file in your participation repository. Keep up with the commentary within the file, too. When was the assignment due? 

Note: These are not exactly the assignments you will be doing in this class.

## Topic 3: Asking effective questions online (10 min)

(5 min)

We all get stuck sometimes. If you try taking [preliminary measures](https://wiernik-datasci.netlify.com/help-general/) such as googling, you may have to turn to writing a question on a discussion board. Making your question _effective_ is an art. 

To make your question effective, the idea is to make things as easy as possible for someone to answer. 

- Will they have to dig to find a resource you're talking about, or do you provide links?
- If your code isn't doing what you expect, or you don't know how to obtain an output, do you provide a [__reproducible example__](https://stackoverflow.com/help/minimal-reproducible-example) (aka "reprex")?
  - Ideally, someone should be able to copy and paste a chunk of code to reproduce the problem you are talking about.
- Is your reproducible example _minimal_, meaning you've removed all the unnecessary parts to reproduce the problem?

You'll probably find that the act of writing an effective question causes you to answer your own question!

### Asking questions - Activity (5 min)

__Commenting on some online questions__

1. My turn: Start an Issue on the [Announcements repo](https://github.com/USF-Pssych-DataSci-2020/Announcements/issues) called `Asking effective questions`.
2. Your turn: Find a question/issue or two that someone has posed online. Check out [Stack Overflow](https://stackoverflow.com/) for inspiration.
3. Your turn: Add a comment to the newly opened Issue with the following:
    - The URL to the thread/question
    - A few brief points on how the question is worded effectively or ineffectively. What would make it better, if anything?

We'll talk about some examples after you're done. 

# The version control workflow

The rest of today's topic is version control..

## Learning Objectives

From this lesson, you will be able to demonstrate the git/GitHub functionality listed here.

## Working with git and GitHub

Before we dive into concepts, it's important to distinguish between the __local__ 
and __remote__ copies of a repo.

- __Local__ refers to things on your own computer. A local repo is a repo found on your hard drive.
- __Remote__ refers to things on the internet. A remote repo lives on GitHub (or other similar sites). 

You can connect a local repo to a remote repo, then push and pull ("fetch") changes
between them. You have already been doing this using the GitHub Desktop program.

Note that you can connect your local repo to more than one remote repo! 
Because of this, git gives the remote repos names (e.g., `origin`, `upstream`). 
In this class, we will only ever be using one remote. By default, this remote is 
named __origin__. 


### The typical workflow (8 min)

The majority of your interaction with version control will be a 
**pull/stage/commit/push workflow**, explained here. For another resource on 
this, check out [happygitwithr: rstudio-git-github](http://happygitwithr.com/rstudio-git-github.html).

0. __Clone__ your repository if you don't have a local copy.

Once you have a local copy of the repository, then working on a project involves 
frequent use of these three steps:

1. __Pull__ the remote repo to make sure your local copy is up to date.
2. Make changes to your files.
   1. __Stage__ your changes (GitHub Desktop does this for you).
3. __Commit__ your changes.
4. __Push__ the changes (after perhaps pulling again).

Git treats the remote repository as the "official" version of the repository. 
This means that your local copy is a second class citizen -- the repository you 
have locally must be up-to-date with the remote repository before you are 
allowed to push your work. If there are commits on the remote repository that 
are not present locally, git will throw an error if you try to push your changes.

**Integrate version control as you do work!**

Keep track of your changes by committing them frequently as you go, rather than
waiting until the end.

- Workflow without version control: save your files spontaneously.
- Workflow with version control: 
  - save your files spontaneously, 
  - commit your changes after every "step" in your work, and 
  - push your changes [in case of fire](https://github.com/louim/in-case-of-fire). 

Committing often ensures that you can trace back all the work you did. 
This results in transparency with the way your project has developed, 
which is a very effective workflow. 

You might feel self-conscious about half-finished work being viewable.
Try to push past that. Folks aren't going to judge you based on in-progress work.



### Git Clients (3 min)

We have been using the GitHub Desktop program ("client") to work with git. 
It's also possible to work with git directly from RStudio (see the bottom of this
page) or using other programs. You can also "directly" interact with git, by
typing commands in the terminal (cmd, Terminal, bash), such as `git clone`, 
`git commit`, etc. Working in the terminal opens a lot of extra functionality
with git, but the GitHub Desktop program will get us most of what we need.


### Merge conflicts (5 min)

If you change a file locally, and that same file (_and_ the same lines) get 
changed on the remote repo in a different way, you'll end up with a _merge conflict_.

You will need to resolve this conflict before you can continue working. Remember 
that your local copy is a second class citizen compared to the remote version, 
so you'll have to resolve things locally before pushing to the remote. 

### Merge conflicts: Activity (5 min)

Let's make a merge conflict, then fix it. 

1. Edit a line of your README both locally and remotely (on GitHub) to something 
   different in both cases. Commit both changes.
2. Try pulling your remote changes. You'll get a _merge conflict_. 
3. Update the file that has the conflict, commit your changes, and push.


### Branching (8 min)

Sometimes, it's a good idea to work on changes to one part of your code separately
before putting them back into your master document. For example, let's say you
want to experiment with a new analysis without messing up your existing work.

To do that, we use **branches**. A branch is a copy of the repo that sits separately
from your `master` branch. Git keeps track of how each of your branches differs.
  - When did a branch break off from `master`?
  - What commits have been made to the branch since the break?
  - What commits have been made to `master` since the break?
  
Let's take a look at some branches on the [psychmeta repo](https://github.com/psychmeta/psychmeta).

After you make changes to the branch, at some point, you probably want to **merge**
the branch back into `master`. On GitHub, we do this using a **pull request**.
  - That name means you want the parent branch (`master`) to _pull_ in all the 
    commits made on child branch. You _request_ that this happens from the 
    owners or collaborators on the repo (for your homework, this is yourself).
  - Once a pull request has been opened, it's a good time to review the changes
    to the code, run tests, make comments, and correct any errors. You can review
    yourself and also request code reviews from others by username.
  - You can make additional changes to the pull request by pushing more commits
    to the child branch.
  - When you are ready, **merge** the pull request into the parent branch.
  - [Example from psychmeta](https://github.com/psychmeta/psychmeta/pull/75).
  - For more info on pull requests, see this [GitHub tutorial](https://help.github.com/articles/about-pull-requests/).

There are many reasons you may want to branch. Here are some:

- A collaborator wants to make a change to the repo, but the end product of the 
  change requires review from collaborators.
- You want to make changes, but don't want to "deploy" the changes until later 
  (such as if pushing to GitHub triggers a website build).
- If you want to try something "risky", it's just safer to work on a branch.

### Branching: Activity (5 min)

Let's organize our participation repo in a branch.

1. Create a new branch locally, called "organizing" (we could have also made this on GitHub):
    - Click the "Branch" button ![](./img/branch.png) in GitHub Desktop.
    - Type a name for your branch and click "New Branch".
2. Stage and commit the new files.
3. Restructure your repository in a more sensible way, using some folders (locally).
4. Stage and commit the changes; push to GitHub.
5. Explore: 
    - Switch between branches to see that the repo structure is different.
6. Merge the branch to "master" via GitHub by making a pull request. 


### Undoing Changes (5 min)

There are many ways that work can be "undone" in git. We will only investigate 
three of the simpler methods. For more advanced methods, like reverting to a specific
previous commit in your repo history, check out these resources by 
[bitbucket](https://www.atlassian.com/git/tutorials/undoing-changes) and 
[GitHub](https://blog.github.com/2015-06-08-how-to-undo-almost-anything-with-git/). 


The three most useful "undo"s are:

1. Undoing your (uncommited) work and reverting to the previous commit.
2. Reverting a single previous commit.
3. Browsing the repo at previous states, and taking files from there. 

We'll demonstrate (1) and (2) in an activity.

### Undoing Changes: Activity (2 min)

Here's how to go back to the most recent commit:

1. First, make and save a change to (say) a README file in your participation repo.
2. In GitHub desktop, right-click on the file whose changes you want to discard.
   Click "Discard Changes". You can even select multiple files at once if you like.
   
Here's how to undo a specific previous commit:

1. In GitHub desktop, click on the History tab.
2. Right-click on the commit you want to revert and choose "Revert this commit".


### Getting weird errors? (3 min)

It's common to sometimes experience some errors in git, especially if you're first 
learning how to use it. Try to get yourself unstuck with the concepts we've discussed here first. 

But, you might find yourself stuck. The git documentation is full of jargon, 
making it difficult to read and therefore difficult to debug things. There's even 
a [parody](https://git-man-page-generator.lokaltog.net/) on it. If you are in 
this position, it's best to just [burn it all down](http://happygitwithr.com/burn.html). 
There's even an [xkcd comic](https://xkcd.com/1597/) on this.


### Tagging a Release (5 min)

Tagging a release on GitHub is like putting a "star" next to a particular commit. 
It highlights a particular point in time of your repository that is noteworthy, 
typically after achieving some milestone. This is easier to come back to than 
having to manually keep track of noteworthy points in your commit history.

Examples:

- At the end of teaching this class, I will tag a release so I can easily come back
  to earlier versions of the course in the future.
- After sufficient development of an R package like [psychmeta](https://github.com/psychmeta/psychmeta/releases), 
  a new release is tagged corresponding to the version of the package. 

### Tagging a Release: Activity (3 min)

Congratulations! We finished the first two weeks of DataSci and Programming, 
which focussed on _tools_. To mark this milestone, let's tag a release on our 
participation repositories.

1. On your GitHub repo online, click "Releases"
2. Click "Create a new release"
3. Fill in the fields:
    - It probably makes sense to use a versioning system like `week_2` here.
4. "Publish Release".


## This week's homework

### Assignment

The assignment is posted on the [Assignments page](https://wiernik-datasci.netlify.com/evaluation/assignments/) 
of the course website.

Your tasks are to create 3 products:
  1. A README.md file for your homework repository to introduce yourself
     and show off some Markdown formatting skills.
  2. An RMarkdown document exploring a dataset, such as `gapminder::gapminder`
     or `psych::bfi`.
  3. An RMarkdown presentation that formats your dataset exploration as a set
     of slides instead a document.

- Follow the instructions in the HW01 description carefully. 
- Commit your code to your homework repo on GitHub **on a new branch called `hw01`**. 
- When you are ready for your peer review (by Monday at 23:59!), create a 
  **Pull Request** to prepare to merge this branch back into your `master` branch.
- Request code reviews from the 2 people assigned to you (see the 
  [Peer Review page](https://wiernik-datasci.netlify.com/evaluation/hw01/pr01/)).
- Submit your peer reivews to your classmat by Tuesday at 23:59.
- Merge your homework branch back into `master` and tag a release called `hw01` 
  by Wednesday at 23:59.


## Optional: Work with git in RStudio

It's possible to work with git/GitHub directly in RStudio, rather than using
GitHub Desktop. I usually work with GitHub Desktop, but do whatever works for you.
I can help you get RStudio working with git if you need a hand.

You might need to [configure your git](http://happygitwithr.com/hello-git.html) 
using the command line or the `usethis` package.

Your RStudio will probably be able to "find" git. But if it can't, you'll 
encounter errors. See [happygitwithr: see-git](http://happygitwithr.com/rstudio-see-git.html) 
for help. 

__Optional__ (but recommended): If you are on a laptop, after class, you might 
want to [cache](http://happygitwithr.com/credential-caching.html) 
your credentials so that you don't have to keep inserting your password.

### The typical workflow git in RStudio

Let's make a change to our repository from local.  

1. Cloning your participation repo.
    - In RStudio, File -> New Project -> Version Control -> Git.
    - You should see a `Git` tab in RStudio, upper-right corner window. 
      If not, see [happygitwithr: see-git](http://happygitwithr.com/rstudio-see-git.html) 
      for help.
    - Take a look at the files you just downloaded!
2. Make your README a little nicer. Maybe fix up the title.
3. Stage and commit the changes:
    - In the Git tab in RStudio, click the checkboxes for the files that you 
      want to commit. This is called "staging".
    - Click the "Commit" button.
    - Enter a commit message.
    - Click "commit". 
4. Push to your remote repository (which is named "origin")
    - Click the up arrow in the Git panel in RStudio.

<!-- 
# Leftover text.

3. Commit your changes. 

Notice that GitHub automatically displays markdown files nicely, but not HTML files.

Note: this exercise employs an effective _local_ git workflow, which we will 
address later on in class.

Together:

1. If you haven't already, clone your participation repository to your desktop.
2. In RStudio, open the file `navigating_github.md`.
    - Yes! RStudio also acts as a plain text editor!
3. Convert the `.md` file to both pdf and html by clicking the `Preview` or `Knit` button.
4. Commit the two new files and push them to GitHub.
-->

<!--chapter:end:s002_git-version-control.Rmd-->

# Intro to plotting with `ggplot2`

__Announcements__:

- How did your homework go? Run into any issues we should discuss?
- Your work should be stored in your _homework_ repository, not your 
  _partication_ repo!

__Recap__:

- Previous two weeks: software for data analytic work: git & GitHub, markdown, and R.
- Next three weeks: fundamental methods in exploratory data analysis: R tidyverse.

__Today__: Introduction to plotting with `ggplot2` 

__Worksheets__: You can find worksheet templates for today here:
  
  - [Worksheet 1](https://raw.githubusercontent.com/USF-Psych-DataSci/Classroom/master/tutorials/s03a_ggplot_p1-exercise.Rmd).
  - [Worksheet 2](https://raw.githubusercontent.com/USF-Psych-DataSci/Classroom/master/tutorials/s03b_ggplot_p2-exercise.Rmd).

Download the worksheet files and save them in a new folder in your participation repo
called "plotting".

Set up the workspace:

```{r, warning = FALSE}
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(gapminder))
suppressPackageStartupMessages(library(scales))
knitr::opts_chunk$set(fig.width = 5, fig.height = 4, fig.align = "center")
```

## Learning Objectives

By the end of this lesson, you will be able to:

- Identify the plotting framework available in R
- Have a sense of why we're learning the `ggplot2` tool
- Have a sense of the importance of statistical graphics in communicating information
- Identify the seven components of the grammar of graphics underlying `ggplot2`
- Use different geometric objects and aesthetics to explore various plot types.

## Resources

For me, I learned `ggplot2` from Stack Overflow by googling error messages or 
"how to ... in ggplot2" queries, together with persistence. It might take you a 
bit longer to make a graph using `ggplot2` if you're unfamiliar with it, but 
persistence pays off. 

Here are some good walk-throughs that introduce `ggplot2`, in a similar way to 
today's lesson:

- [r4ds: data-vis](http://r4ds.had.co.nz/data-visualisation.html) chapter.
    - Perhaps the most compact "walk-through" style resource.
- The [ggplot2 book](http://webcat2.library.ubc.ca/vwebv/holdingsInfo?bibId=8489511), 
  Chapter 2.
    - A bit more comprehensive "walk-through" style resource.
    - Section 1.2 introduces the actual grammar components. 
- [Jenny Bryant's ggplot2 tutorial](https://github.com/jennybc/ggplot2-tutorial).
    - Has a lot of examples, but less dialogue.

Here are some good resource to use as a reference:

- [`ggplot2` cheatsheet](https://github.com/rstudio/cheatsheets/blob/master/data-visualization-2.1.pdf)
- [R Graphics Cookbook](http://www.cookbook-r.com/Graphs/)
    - Good as a reference if you want to learn how to make a specific type of plot.

## Orientation to plotting in R (7 min)

TL;DR: We're using `ggplot2` in DataSci.

Traditionally, plots in R are produced using "base R" methods, the main function 
here being `plot()`. This method tends to be quite involved, and requires a lot 
of "coding by hand". That said, for simple plots and some built-in functions, 
using `plot()` is great. For example, R has built-in regression diagnostics:

```{r}
mod <- lm(dist ~ speed, data = cars)
par(mfrow = c(2, 2))
plot(mod)
```

Then, an R package called `lattice` was created that aimed to make it easier to 
create multiple "panels" of plots. It seems to have gone by the wayside in the 
R community.

After `lattice` came `ggplot2`, which provides a very powerful and relatively 
un-fiddly framework for making plots. It has a theoretical underpinning, too, 
based on the Grammar of Graphics, first described by Leland Wilkinson in his 
["Grammar of Graphics" book](http://resolve.library.ubc.ca/cgi-bin/catsearch?bid=5507286). 
With `ggplot2`, you can make a great many type of plots with minimal code. 
It's been a hit in and outside of the R community.

  - A big advantage of `ggplot2` is that many people have written extensions for
    it, such as [`tidybayes`](https://mjskay.github.io/tidybayes/articles/tidybayes.html) (for plotting distributions and intervals),
    [`gganimate`](https://gganimate.com/) (for animations and gifs) and
    [`ggforce`](https://ggforce.data-imaginist.com/) (lots of very customizable features).

Check out [this comparison of the three](http://www.jvcasillas.com/base_lattice_ggplot/) 
by Joseph V. Casillas.

A newer tool is called [plotly](https://plot.ly/), which was actually developed 
outside of R, but the `plotly` R package accesses the plotly functionality. 
Plotly graphs allow for interactive exploration of a plot. 
You can convert ggplot2 graphics to a plotly graph, too.

There are also some non-R plotting interfaces that are useful when you need
highly customized solutions, such as [D3](https://d3js.org/), accessible in R 
using the [r2d3 package](https://rstudio.github.io/r2d3/).

## Just plot it (7 min)

The human visual cortex is a powerful thing. If you're wanting to point 
someone's attention to a bunch of numbers, you probably won't get any "aha" 
moments by displaying a large table [like this](https://i.stack.imgur.com/2JdLt.png), 
either in a report or (especially!) a presentation. 

Make a plot to communicate your message!

If you really feel the need to tell your audience exactly what every quantity 
evaluates to, consider putting your table in an appendix. Because chances are, 
the reader doesn't care about the exact numeric values. Or, perhaps you just 
want to point out one or a few numbers, in which case you can put that number 
directly on a plot.

## The grammar of graphics (15 min)

You can think of the grammar of graphics as a systematic approach for describing 
the components of a graph. It has seven components (the ones in bold are required 
to be specifed explicitly in `ggplot2`):

- __Data__
  - The data that you're feeding into a plot.
- __Aesthetic mappings__
  - How are variables (columns) from your data connect to a visual dimension?
  - Horizontal positioning, vertical positioning, size, colour, shape, etc.
  - These visual dimensions are called "aesthetics"
- __Geometric objects__
  - What are the objects that are actually drawn on the plot? 
  - A point, a line, a bar, a histogram, a density, etc. 
- Scales
  - How is a variable mapped to its aesthetic?
  - Will it be mapped linearly? On a log scale? Something else?
  - This includes things like the color scale
    - e.g., c(control, treatment_1, treatment_2) -> c("blue", "green", "red")
- Statistical transformations
  - Whether and how the data are combined/transformed before being plotted
  - e.g., in a bar chart, data are transformed into their frequencies; 
          in a box-plot, data are transformed to a five-number summary.
- Coordinate system
  - This is a specification of how the position aesthetics (x and y) are depicted on the plot. 
    For example, rectangular/cartesian, or polar coordinates.
- Facet
  - This is a specification of data variables that partition the data into 
    smaller "sub plots", or panels. 

These components are like parameters of statistical graphics, defining the 
"space" of statistical graphics. In theory, there is a one-to-one mapping 
between a plot and its grammar components, making this a useful way to specify 
graphics.

### Example: Scatterplot grammar

For example, consider the following plot from the `gapminder` data set. For now, 
don't focus on the code, just the graph itself.

```{r}
ggplot(gapminder, aes(gdpPercap, lifeExp)) +
  geom_point(alpha = 0.1) +
  scale_x_log10("GDP per capita", labels = scales::dollar_format()) +
  theme_bw() +
  ylab("Life Expectancy")
```

This scatterplot has the following components of the grammar of graphics. 

| Grammar Component     | Specification |
|-----------------------|---------------|
| __data__              | `gapminder`   |
| __aesthetic mapping__ | __x__: `gdpPercap`, __y:__ `lifeExp` |
| __geometric object__  | points  |
| scale                 | x: log10, y: linear |
| statistical transform | none  |
| coordinate system     | rectangular  |
| facetting             | none  |

Note that `x` and `y` aesthetics are required for scatterplots (or "point" 
geometric objects). In general, each geometric object has its own required set 
of aesthetics. 


### Activity: Bar chart grammar

Fill out __Exercise 1: Bar Chart Grammar (Together)__ in your Worksheet 1.

Click [here](https://raw.githubusercontent.com/USF-Psych-DataSci/Classroom/master/tutorials/s03a_ggplot_p1-exercise.Rmd) if you don't have it yet.
<!-- TODO: Clean this up -->

## Working with `ggplot2`

First, the `ggplot2` package comes with the `tidyverse` meta-package. So, loading that is enough.

There are two main ways to interact with `ggplot2`:

1. The `qplot()` or `quickplot()` functions (the two are identical): Useful for 
   making a quick plot if you have vectors stored in your workspace that you'd 
   like to plot. Usually not worthwhile using.
2. The `ggplot()` function: use to access the full power of `ggplot2`.

Let's use the above scatterplot as an example to see how to use the `ggplot()` function.

First, the `ggplot()` function takes two arguments:
  - `data`: the data frame containing your plotting data.
  - `mapping`: aesthetic mappings applying to the entire plot. 
               Expecting the output of the `aes()` function.
               
The `aes()` function tells R to look in the `data` for variable names. Notice 
that the `aes()` function has `x` and `y` as its first two arguments, so we 
don't need to explicitly name these aesthetics. Beyond these basic arguments,
you can also specify others, such as `color`, `alpha` (transparency), `size`,
`group`.

Different plot shapes (`geom_SOMETHING`) accpet different `aes()` arguments.
These are listed in the help file for the geom function: `?geom_point`, 
`?geom_line`, etc.


You can specify values for aesthetic parameters using constants or variables 
from outside `data` by specifying them outside of `aes()`. For example, to make 
all of the shapes of plot blue, you can add: `color = "blue"`.


```{r}
ggplot(gapminder, aes(gdpPercap, lifeExp))
```

This just _initializes_ the plot. It has the "data" and "aesthetics" part of the 
grammar of the plot, but we don't see anything becaue we haven't yet specified the
geometries or other parts of the grammer.

You'll notice that the aesthetic mappings are already in place (the x and y axes). 
Now, we need to add components by adding layers, literally using the `+` sign. 
These layers are functions that have further specifications. 

For our next layer, let's add a geometric object to the plot, which have the 
syntax `geom_SOMETHING()`. There's a bit of overplotting (overlapping symbols), 
so we can specify some alpha transparency using the `alpha` argument 
(you can interpret `alpha` as neeing `1/alpha` points overlaid to achieve an 
opaque point).

```{r}
ggplot(gapminder, aes(gdpPercap, lifeExp)) +
  geom_point(alpha = 0.1)
```

That's the only `geom` that we want to add. Now, let's specify a scale 
transformation, because the plot would really benefit if the x-axis is on a log 
scale. These functions take the form `scale_AESTHETIC_TRANSFORM()`. As usual, 
you can tweak this layer, too, using this function's arguments. In this example, 
we're re-naming the x-axis (the first argument), and changing the labels to have 
a dollar format (a handy function thanks to the `scales` package).

```{r}
ggplot(gapminder, aes(gdpPercap, lifeExp)) +
  geom_point(alpha = 0.1) +
  scale_x_log10("GDP per capita", labels = scales::dollar_format())
```

I'm tired of seeing the ugly default grey background, so I'll add a `theme()` 
layer. I like `theme_bw()` (you can tweak themes later, too!). Then, I'll re-label the y-axis using the `ylab()` 
function. Et voilà!

```{r}
ggplot(gapminder, aes(gdpPercap, lifeExp)) +
  geom_point(alpha = 0.1) +
  scale_x_log10("GDP per capita", labels = scales::dollar_format()) +
  theme_bw() +
  ylab("Life Expectancy")
```


### Activity: Build some plots!

1. Go to your worksheet
2. Set up the workspace by following the instructions in the "Preliminary" section.
3. Fill out __Exercise 2: `ggplot2` Syntax (Your Turn)__ in your worksheet.

_Bus stop_: Did you lose track of where we are? You can still do the exercise!

- Click [here](https://raw.githubusercontent.com/USF-Psych-DataSci/Classroom/master/tutorials/s03a_ggplot_p1-exercise.Rmd) 
  to get the worksheet if you don't have it.
- You're all set! Hint for completing the exercise: use the information from 
  this section ("Working with `ggplot2`") to complete the exercise.


## A tour of some important `geom`s

Here, we'll explore some common plot types, and how to produce them with 
`ggplot2`.

### Histograms: `geom_histogram()`

Useful for depicting the distribution of a continuous random variable. 
Partitions the number line into bins of certain width, counts the number of 
observations falling into each bin, and erects a bar of that height for each bin.

Required aesthetics:

- `x`: A numeric vector.
  - By default, a histogram plots the _count_ on the y-axis. If you want to use 
    proportion (i.e., "density"), specify the `y = ..density..` aesthetic. 

You can change the smoothness of the plot via two arguments (your choice):

- `bins`: the number of bins/bars shown in the plot.
- `binwidth`: the with of the bins shown on the plot.

Example:

```{r}
ggplot(gapminder, aes(lifeExp)) +
  geom_histogram(bins = 50)
```


### Density: `geom_density()`

Essentially, a "smooth" version of a histogram. Uses [kernels](https://en.wikipedia.org/wiki/Kernel_density_estimation) to produce the curve.

Required aesthetics:

- `x`: A numeric vector.

Good to know:

- `bw` argument controls the smoothness: Smaller = rougher.

Example:

```{r}
ggplot(gapminder, aes(lifeExp)) +
  geom_density()
```

### Jitter plots: `geom_jitter()`

A scatterplot, but with minor random perturbations of each point. Useful for 
scatterplots where points are overlaying, or when one variable is categorical.

Required aesthetics:

- `x`: any vector
- `y`: any vector

Example:

```{r}
ggplot(gapminder, aes(continent, lifeExp)) +
  geom_jitter()
```

### Box plots: `geom_boxplot()`

This geom makes a boxplot for a numeric variable in each of a category. 
Useful for visualizing probability distributions across different categories.

Required aesthetics:

- `x`: A factor (categorical variable)
- `y`: A numeric variable

Example:

```{r}
ggplot(gapminder, aes(continent, lifeExp)) +
  geom_boxplot()
```


### Ridge plots: `ggridges::geom_density_ridges()`

A (superior?) alternative to the boxplot, the ridge plot (also known as the 
"joy plot", but don't use that term) places a kernel density for each group, 
instead of the box. 

You'll need to install the `ggridges` package. You can do lots more with ridges -- check out 
[the ggridges intro vignette](https://cran.r-project.org/web/packages/ggridges/vignettes/introduction.html).

Required aesthetics (reversed from boxplots!)

- `x`: A numeric variable
- `y`: A factor (categorical variable) 

Example:

```{r}
ggplot(gapminder, aes(lifeExp, continent)) +
  ggridges::geom_density_ridges()
```

### Bar plots: `geom_bar()` or `geom_col()`

These geom's erect a bar over each category.

`geom_bar()` automatically determines the height of the bar according to the 
count of each category.

`geom_col()` just takes the supplied values as the bar heights.

Required aesthetics:

- `x`: A categorical variable
- `y`: A numeric variable (only required for `geom_col()`!)
  - By default, a bar plot plots the _count_ on the y-axis. If you want to use 
    proportion, specify the `y = ..prop..` aesthetic. 

Example: number of 4-, 6-, and 8- cylinder cars in the `mtcars` dataset:

```{r}
ggplot(mtcars, aes(cyl)) +
  geom_bar()
```

### Line charts: `geom_line()`

A line plot connects points with straight lines, from left-to-right. 
Especially useful if time is on the x-axis.

Required aesthetics:

- `x`: a variable having some ordering to it.
- `y`: a numeric variable.

Although not required, the `group` aesthetic will come in handy here. 
This aesthetic produces a plot independently for each group, and overlays the 
results (i.e., a separate line for each group).

```{r}
tsibble::as_tsibble(co2) %>% 
  rename(yearmonth = index,
         conc = value) %>% 
  mutate(month = lubridate::month(yearmonth, label = TRUE),
         year  = lubridate::year(yearmonth)) %>% 
  ggplot(aes(month, conc)) +
  geom_line(aes(group = year), alpha = 0.5) +
  ylab("CO2 Concentration")
```



### Path plots: `geom_path()`

Like `geom_line()`, except connects points in the order that they appear in the 
dataset.


## Activity: Fix the Plots (40 min)

Fill out [Worksheet 2](https://raw.githubusercontent.com/USF-Psych-DataSci/Classroom/master/tutorials/s03b_ggplot_p2-exercise.Rmd) together.

<!--chapter:end:s003_plotting-ggplot2.Rmd-->

# Wrangle yo' data

## Announcements

### Comments on assignments

- Great job overall!
- Add `keep_md: TRUE` to the `html_document` output parameters in you `.Rmd` files

__Worksheet__: You can find the worksheet template for today
[here](https://raw.githubusercontent.com/USF-Psych-DataSci/Classroom/master/tutorials/s04_data-wrangling-exercise.Rmd).


## Today's Topics

Today we'll get started with learning to "wrangle" data—that is, to subset it, 
rearrange it, transform it, summarize it, and otherwise make it ready for 
analysis. We are going to be working with the [`dplyr`](https://dplyr.tidyverse.org/) 
package. Specifically, we're going to consider three lessons today:

- Intro to `dplyr` syntax
- The `%>%` pipe and the `dplyr` advantage
- `filter`; relational/comparison and logical operators in R

- Specific `dplyr` functions we will cover
  - [ ] `select()`
  - [ ] `arrange()`
  - [ ] `filter()`
  - [ ] `mutate()`
  - [ ] `summarize()`
  - [ ] `group_by()`
      - [ ] grouped `mutate()`
      - [ ] grouped `summarize()`

Depending on time, we might also talk about:
- GitHub pages


## Resources

STAT 545 chapters:
  - [stat545: dplyr-intro](http://stat545.com/block009_dplyr-intro.html)
  - [stat545: dplyr-single](https://stat545.com/dplyr-single.html)

More detail can be found in the [r4ds: transform](http://r4ds.had.co.nz/transform.html) chapter.

Here are some supplementary resources:

- A similar resource to the r4ds one above is the [intro to dplyr vignette](https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html).
- Want to read more about piping? See [r4ds: pipes](http://r4ds.had.co.nz/pipes.html).

Some advanced topics you might find useful:
- For window functions and how dplyr handles them, see the [window-functions](https://cran.r-project.org/web/packages/dplyr/vignettes/window-functions.html) vignette for the `dplyr` package. 
- For time series data, see the [tsibble demo](https://tsibble.tidyverts.org/)


## Participation

To get participation points for today, you will need to:
  - Make a new folder in your participation repo called `wrangling`
  - Fill out the worksheet, knit it, and push the files to this folder:
    - [s04_data-wrangling-exercise.Rmd](https://raw.githubusercontent.com/USF-Psych-DataSci/Classroom/master/tutorials/s04_data-wrangling-exercise.Rmd).


## Intro to `dplyr` syntax

### Learning Objectives

Here are the concepts we'll be exploring in this lesson:

- tidyverse
- `dplyr` functions:
    - select
    - arrange
- piping

By the end of this lesson, students are expected to be able to:

- subset and rearrange data with `dplyr`
- use piping (`%>%`) when implementing function chains

### Preamble

Let's talk about:

- The history of `dplyr`: `plyr`
  - Don't use both in one script!
  - My recommendation, don't use `plyr` at all at this point.
- tibbles are a special type of data frame
- The [tidyverse](https://www.tidyverse.org/)
- Package functions and masking
  - Load the `tidyverse` package: `library(tidyverse)`

### Demonstration

Let's get started with the exercise:

1. Open the `s04_data-wrangling-exercise.Rmd` worksheet in RStudio.
2. Follow along in the `.Rmd` file until the *Back to Guide* section.


## The `dplyr` advantage

### Learning Objectives

By the end of this lesson, you will:

- Have a sense of why `dplyr` is advantageous compared to the "base R" way with respect to good coding practice.

Why?

- Having this in the back of your mind will help you identify qualities of and produce a readable analysis.

### Compare base R to `dplyr`

__Self-documenting code__. 

This is where the tidyverse shines.

Example of `dplyr` vs base R:

```
gapminder[gapminder$country == "Cambodia", c("year", "lifeExp")]
```

vs.

```
gapminder %>%
  filter(country == "Cambodia") %>%
  select(year, lifeExp)
```

![Morning Routine Pipie](https://raw.githubusercontent.com/USF-Psych-DataSci/Classroom/master/img/pipe_routine.jpg)

### The workflow:

1. Wrangle your data with `dplyr` first
2. Pipe `%>%` your data into a plot/analysis

### Basic principles:

1. Do one thing at a time
  - Transform variables OR select variables OR filter cases
2. Chain multiple operations together using the pipe `%>%` 
3. Use readable object and variable names
4. Subset a dataset (i.e., select variables) by **name**, not by "magic numbers"
5. _Note that you need to use the assignment operator `<-` to store changes!_

### Tangent: Base R workflow

- We are jumping right into the tidyverse way of doing things in R, instead of the base R way of doing things. Our first week was about "just enough" base R to get you started. If you feel that you want more practice here, take a look at [the R intro stat videos by MarinStatsLectures](https://www.youtube.com/playlist?list=PLqzoL9-eJTNARFXxgwbqGo56NtbJnB37A).


## Relational/Comparison and Logical Operators in R

### Learning Objectives

Here are the concepts we'll be exploring in this lesson:

- Relational/comparison operators
- Logical operators
- `dplyr` functions:
    - filter
    - mutate

By the end of this lesson, you will be able to:

- Predict the output of R code containing the above operators.
- Explain the difference between `&`/`&&` and `|`/`||`, and name a situation 
  where one should be used over the other.
- Subsetting and transforming data using filter and mutate

### R Operators

**Arithmetic** operators allow us to carry out mathematical operations:

| Operator | Description |
|------|:---------|
| + | Add |
| - | Subtract |
| * | Multiply |
| / | Divide |
| ^ | Exponent |
| %/% | Integer division |
| %% | Modulus (remainder from integer division) |

**Relational** operators allow us to compare values:

| Operator | Description |
|------|:---------|
| < | Less than |
| > | Greater than |
| <= | Less than or equal to |
| >= | Greater than or equal to |
| == | Equal to |
| != | Not equal to |

* Arithmetic and relational operators work on vectors.

There is another very useful relational function, `%in%`:

```{r}
c(1, 2, 3, 4, 5) %in% c(1, 2)
```

**Logical** operators allow us to carry out boolean operations:

| Operator | Description |
|---|:---|
| ! | Not |
| \| | Or (element_wise) |
| & | And (element-wise) |
| \|\| | Or |
| && | And |

* The difference between `|` and `||` is that `||` evaluates only the first 
element of the two vectors, whereas `|` evaluates element-wise. 

### Demonstration

Continue along with the worksheet until **Back to Guide Again**.


## Picking up where we left off

__Worksheet__: You can find the worksheet template for today
[here](https://raw.githubusercontent.com/USF-Psych-DataSci/Classroom/master/tutorials/s05_data-wrangling-exercise_part-2.Rmd).

## `summarize()`

Like `mutate()`, the `summarize()` function also creates new columns, but the 
calculations that make the new columns must reduce down to a single number. 

For example, let's compute the mean and standard deviation of life expectancy 
in the gapminder data set:

```{r}
gapminder %>% 
  summarize(mu    = mean(lifeExp),
            sigma = sd(lifeExp))
```

Notice that all other columns were dropped. This is necessary, because there's 
no obvious way to compress the other columns down to a single row. This is 
unlike `mutate()`, which keeps all columns, and more like `transmute()`, which 
drops all other columns.

As it is, this is hardly useful. (Though it is useful for creating Table 1 in 
your papers.) But summarizing is more useful in the context of _grouping_, 
coming up next.


## `group_by()` (20 min)

The true power of `dplyr` lies in its ability to group a tibble, with the 
`group_by()` function. As usual, this function takes in a tibble and returns a 
(grouped) tibble. 

Let's group the gapminder dataset by continent and year:

```{r}
gapminder %>% 
  group_by(continent, year)
```

The only thing different from a regular tibble is the indication of grouping 
variables above the tibble. This means that the tibble is recognized as having 
"chunks" defined by unique combinations of continent and year:

- Asia in 1952 is one chunk.
- Asia in 1957 is another chunk.
- Europe in 1952 is another chunk.
- etc...

Notice that the data frame isn't _rearranged_ by chunk! The grouping is something
stored internally about the grouped tibble.

Now that the tibble is grouped, operations that you do on a grouped tibble 
_will be done independently within each chunk_, as if no other chunks exist. 

You can also create new variables and group by that variable simultaneously. 
Try splitting life expectancy by "small" and "large" using 60 as a threshold:

```{r}
gapminder %>% 
  group_by(smallLifeExp = lifeExp < 60)
```

### Grouped `summarize()`

Want to compute the mean and standard deviation for each year for every 
continent? No problem:

```{r}
gapminder %>% 
  group_by(continent, year) %>% 
  summarize(mu    = mean(lifeExp),
            sigma = sd(lifeExp))
```

Notice:

- The grouping variables are kept in the tibble, because their values are unique 
  within each chunk (by definition of the chunk!)
- With each call to `summarize()`, the grouping variables are "peeled back" from 
  last grouping variable to first.

This means the above tibble is now only grouped by continent. What happens when 
we reverse the grouping?

```{r}
gapminder %>% 
  group_by(year, continent) %>%    # Different order
  summarize(mu    = mean(lifeExp),
            sigma = sd(lifeExp))
```

The grouping columns are switched, and now the tibble is grouped by year instead 
of continent. 

`dplyr` has a bunch of convenience functions that help us write code more 
eloquently. We could use `group_by()` and `summarize()` with `length()` to find 
the number of entries each country has:

```{r}
gapminder %>% 
  group_by(country) %>% 
  transmute(n = length(country))
```

Or, we can use the more elegant `dplyr::n()` to count the number of rows in each 
group:

```{r}
gapminder %>% 
  group_by(country) %>% 
  summarize(n = n())
```

Or better yet, if this is all we want, just use `dplyr::count()`:

```{r}
gapminder %>% 
  count(country)
```

### Grouped `mutate()`

Want to get the increase in GDP per capita for each country? No problem:

```{r}
gap_inc <- gapminder %>% 
  arrange(year) %>% 
  group_by(country) %>%
  mutate(gdpPercap_inc = gdpPercap - lag(gdpPercap))
print(gap_inc)
```

The tibble is still grouped by country.

Drop the `NA`s with another convenience function, this time supplied by the 
`tidyr` package (another tidyverse package that we'll see soon):

```{r}
gap_inc %>% 
  tidyr::drop_na()
```

You can specify specific columns to drop `NA`s from in the `drop_na()` function.

## Function types

We've seen cases of transforming variables using `mutate()` and `summarize()`, 
both with and without `group_by()`. How can you know what combination to use? 
Here's a summary based on one of three types of functions.


| Function type | Explanation | Examples | In `dplyr` |
|------|-----|----|----|
| Vectorized functions | These take a *vector*, and operate on each component independently to return a vector of the same length. In other words, they work element-wise. | `cos()`, `sin()`, `log()`, `exp()`, `round()` | `mutate()` |
| Aggregate functions | These take a vector, and return a vector of length 1 | `mean()`, `sd()`, `length()` | `summarize()`, esp with `group_by()`. |
| Window Functions | these take a vector, and return a vector of the same length that depends on the vector as a whole. | `lag()`, `rank()`, `cumsum()` | `mutate()`, esp with `group_by()` |


## `dplyr` Exercises

Let's go to the worksheet and work on the exercises.

<!--chapter:end:s004_wrangling-dplyr.Rmd-->

# Tidy Data and Pivoting


```{r, warning = FALSE, message = FALSE}
library(tidyverse)
```


## Orientation


### Worksheet

You can find a worksheet template for today
[here](https://raw.githubusercontent.com/USF-Psych-DataSci/Classroom/master/tutorials/s06_pivot-exercise.Rmd).


### Today

Today's concept is __tidy data__ and the `tidyr` package.

In fact `tidyr` Version 1.0.0 _just came out_ 19 days ago with some great new additions that we'll be looking at. We'll focus on:

- Reshaping data by pivoting with `tidyr::pivot_longer()` and `tidyr::pivot_wider()`.
- Making tibbles using `tibble::tibble()` and `tidyr::expand_grid()`.


### Resources

For concepts of tidy data: 

- [Jenny Bryan's intro to tidy data](https://github.com/jennybc/lotr-tidy/blob/master/01-intro.md) is short and sweet.
  - the repo this links to has some useful exercises too, but uses the older `spread()` and `gather()` functions.
- `tidyr` [vignette on tidy data](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html).
- [Hadley's paper on tidy data](https://vita.had.co.nz/papers/tidy-data.pdf) provides a thorough investigation.

For pivoting with `tidyr`, check out the [pivot vignette](https://tidyr.tidyverse.org/articles/pivot.html).

I also recommend reading the new additions that come with the new `tidyr` Version 1.0.0 in [this tidyverse article](https://www.tidyverse.org/articles/2019/09/tidyr-1-0-0/).


## Tidy Data

A data set is _tidy_ if:

- Each row is an __observation__;
- Each column is a __variable__;
- Each cell is a __value__.

This means that each value belongs to exactly one variable and one observation.

Why bother? Because doing computations with untidy data can be a nightmare. Computations become simple with tidy data. 

This also means that tidy data is relative, as it depends on how you define your observational unit and variables.

```{r, echo = FALSE}
haireye <- as_tibble(HairEyeColor) %>% 
  count(Hair, Eye, wt = n) %>% 
  rename(hair = Hair, eye = Eye)
```

As an example, consider this example derived from the `datasets::HairEyeColor` dataset, containing the number of people having a certain hair and eye colour.

If one observation is identified by a _hair-eye colour combination_, then the tidy dataset is:

```{r}
haireye %>% 
  knitr::kable()
```

If one observation is identified by a _single person_, then the tidy dataset has one pair of values per person, and one row for each person. We can use the handy `tidyr::uncount()` function, the opposite of `dplyr::count()`:

```{r}
haireye %>% 
  tidyr::uncount(n) %>% 
  knitr::kable()
```


### Untidy Examples

The following are examples of untidy data. They're untidy for either of the cases considered above, but for discussion, let's take a hair-eye colour combination to be one observational unit. 

Note that untidy does not always mean "bad", especially when the data set is too wide.

__Untidy Example 1__: The following table is untidy because there are multiple observations per row. It's _too wide_.

Imagine calculating the total number of people with each hair colour. You can't just `group_by()` and `summarize()`, here!

```{r, echo = FALSE}
haireye_untidy <- haireye %>% 
  mutate(eye = str_c(eye, "_eyed")) %>% 
  pivot_wider(id_cols = hair, names_from = eye, values_from = n)
knitr::kable(haireye_untidy)
```

__Untidy Example 2__: The following table is untidy for the same reason as Example 1 -- multiple observations are contained per row. It's _too wide_.

```{r, echo = FALSE}
haireye %>% 
  mutate(hair = str_c(hair, "_haired")) %>% 
  pivot_wider(id_cols = eye, names_from = hair, values_from = n) %>% 
  knitr::kable()
```

__Untidy Example 3__: This is untidy because each observational unit is spread across multiple columns. It's _too long_. In fact, we needed to add an identifier for each observation, otherwise we would have lost which row belongs to which observation! 

Does red hair ever occur with blue eyes? Can't just `filter(hair == "red", eye == "blue")`!

```{r, echo = FALSE}
haireye %>% 
  mutate(obs = 1:n()) %>% 
  pivot_longer(cols = hair:eye, names_to = "body_part", values_to = "colour") %>% 
  select(-n, n) %>% 
  DT::datatable(rownames = FALSE)
```

__Untidy Example 4__: Just when you thought a data set couldn't get any longer! Now, each variable has its own row: hair colour, eye colour, and `n`. This demonstrates that there's no such thing as "long" and "wide" format, since these terms are relative. 

```{r, echo = FALSE}
haireye %>% 
  mutate(obs = 1:n(),
         n   = as.character(n)) %>% 
  pivot_longer(cols = c(hair, eye, n), names_to = "variable", values_to = "value") %>% 
  DT::datatable(rownames = FALSE)
```

_This is the sort of format that is common pulling data from the web or other "Big Data" sources._


### Pivoting tools

The task of making tidy data is about making data either _longer_, by stacking two or more rows, or _wider_, by putting one or more columns alongside each other based on groups. This is called __pivoting__ (or, reshaping).

Sometimes, tidy data is incorrectly referred to as data in _long format_ as opposed to _wide format_, where "length" refers to the number of rows, and "width" the number of columns. But Example 3 of untidy data (above) is in fact too long and needs to be made wider! However, usually the task of tidying data involves lengthening, and usually the task of widening is useful for turning data into something more friendly for human eyes.

The (new!) easiest and most powerful way to widen or lengthen data are with the functions `tidyr::pivot_wider()` and `tidyr::pivot_longer()`.

History: R has seen many attempts at reshaping, all that's progressively gotten better. First came the `reshape` package. Then the `reshape2` package. Both were finicky. And they used function names that I could never remember: `melt()` and `cast()`. Then, the `tidyr::spread()` and `tidyr::gather()` functions provided a simple interface (and are still part of the `tidyr` package!), but used awkward terminology and weren't as flexible as they ought to be.


## Univariate Pivoting

Let's start with pivoting in the simplest case where only one variable is "out of place". We'll use the hair and eye colour example from before, using the untidy data version from Example 1:

```{r}
haireye_untidy <- haireye %>% 
  mutate(eye = str_c(eye, "_eyed")) %>% 
  pivot_wider(id_cols = hair, names_from = eye, values_from = n)

haireye_untidy
```

The _eye colour_ variable is spread out across columns. To fix this, we need to convert the eye colour columns to two columns:

- one column to hold the eye colour (column names),
- one column to hold the values.

Doing this, we obtain:

```{r, echo = FALSE}
haireye_untidy %>% 
  pivot_longer(contains("eyed"), names_to = "eye", values_to = "n")
```

For the reverse operation, we take the column `eye` and make each unique entry a new column, and the values of those columns take on `n`.


### `pivot_longer()`

`pivot_longer()` takes a data frame, and returns a data frame. The arguments after the data argument that we'll need are:

- `cols` for the column names that we want to turn into a single column.
- `names_to`: the old column names are going to a new column. What should this new column be named? (optional, but highly recommended)
- `values_to`: the values underneath the old columns are going to a new column. What should this new column be named? (optional, but highly recommended)

Possibly the trickiest bit is in identifying the column names. We could list all of them, but it's not robust to changes:

```{r}
haireye_untidy %>% 
  pivot_longer(cols      = c(Blue_eyed, Brown_eyed, Green_eyed, Hazel_eyed),
               names_to  = "eye",
               values_to = "n")
```

We could identify a range. This is more robust, but still not very robust.

```{r}
haireye_untidy %>% 
  pivot_longer(cols      = Blue_eyed:Hazel_eyed,
               names_to  = "eye",
               values_to = "n")
```

Better is to use helper functions from the `tidyselect` package. In this case, we know the columns contain the text "eyed", so let's use `tidyselect::contains()`:

```{r}
haireye_untidy %>% 
  pivot_longer(cols      = contains("eyed"),
               names_to  = "eye",
               values_to = "n")
```

Yet another way is to indicate everything except the `hair` column:

```{r}
haireye_untidy %>% 
  pivot_longer(cols      = -hair,
               names_to  = "eye",
               values_to = "n")
```


### `pivot_wider()`

Like `pivot_longer()`, `pivot_wider()` takes a data frame and returns a data frame. The arguments after the data argument that we'll need are:

- `id_cols`: The columns you would like to keep. If widening to make data tidy, then this is an identifier for an observation.
- `names_from`: the new column names are coming from an old column. Which column is this?
- `values_from`: the values under the new columns are coming from an old column. Which column is this?

```{r}
haireye %>% 
  pivot_wider(id_cols     = hair, 
              names_from  = eye, 
              values_from = n)
```


### Activity

Fill out __Exercise 1: Univariate Pivoting__ in the 
[worksheet](https://raw.githubusercontent.com/USF-Psych-DataSci/Classroom/master/tutorials/s06_pivot-exercise.Rmd).


## Multivariate Pivoting

Now let's consider the case when more than one variable are "out of place" -- perhaps there are multiple variables per row, and/or multiple observations per row.

For example, consider the (lightly modified) `iris` data set that we'll call `iris2`:

```{r, echo = FALSE}
iris2 <- iris %>%
  mutate(id = 1:n()) %>% 
  rename(species = Species) %>% 
  pivot_longer(c(-species, -id), 
               names_to  = "variable", 
               values_to = "measurement") %>% 
  mutate(variable = variable %>% 
           str_replace("\\.", "_") %>% 
           tolower()) %>% 
  pivot_wider(c(id, species), 
              names_from  = variable, 
              values_from = measurement)
DT::datatable(iris2, rownames = FALSE)
```

Although we probably wouldn't, we could view this as having _two variables bundled into the column names_:

- "Plant part", either `sepal` or `petal`.
- "Dimension", either `length` or `width`.

The resulting tidy data frame would then be:

```{r, echo = FALSE}
iris2_longest <- iris2 %>% 
  pivot_longer(cols      = c(-species, -id), 
               names_to  = c("part", "dimension"),
               names_sep = "_",
               values_to = "measurement")
```

```{r}
iris2_longest
```

More realistic is the situation where there are _multiple observations per row_:

- An observation of (length, width) of the sepal.
- An observation of (length, width) of the petal.

The resulting tidy data frame has a length that's in between the above two:

```{r, echo = FALSE}
iris2_longer <- iris2 %>% 
  pivot_longer(cols      = c(-id, -species), 
               names_to  = c("part", ".value"), 
               names_sep = "_")
```

```{r}
iris2_longer
```


### `pivot_longer()`

To obtain the case where two (or more) variables are contained in column names, here's how we specify the arguments of `pivot_longer()`:

- `cols`: As usual.
- `names_sep`: What is separating the variables in the column names?
- `names_to`: The old columns are going to be put into new columns, after being separated. What should those columns be named?
- `values_to`: As usual. 

Here is the code:

```{r}
iris2 %>% 
  pivot_longer(cols      = c(-species, -id), 
               names_to  = c("part", "dimension"),
               names_sep = "_",
               values_to = "measurement")
```

To obtain the case where multiple observations are contained in one row, here's how to specify the arguments of `pivot_longer()`:

- `cols`: As usual.
- `names_sep`: As above.
- `names_to`: As above, except this time, one part of the old column names are going to stay as columns (in this case, "length" and "width"). Indicate `".value"` instead of a new column name.  
- `values_to`: Not needed! You've already indicated that using the `".value"` placeholder. 
 
```{r}
iris2 %>% 
  pivot_longer(cols      = c(-id, -species), 
               names_to  = c("part", ".value"), 
               names_sep = "_")
```


### `pivot_wider()`

If two or more columns contain parts of a variable name (i.e., each unique combination of these columns gives rise to a new variable), here's how we can use `pivot_wider()`:

- `id_cols`: as usual.
- `names_from`: the new variable names are coming from old columns. Which old columns?
- `names_sep`: What character should you separate the entries of the old columns by?
- `values_from`: as usual.

Here is the code to go from the longest form to the original:

```{r}
iris2_longest %>% 
  pivot_wider(id_cols     = c(id, species),
              names_from  = c(part, dimension), 
              names_sep   = "_", 
              values_from = measurement)
```

If variables are spread out amongst rows _and_ columns (for example, "sepal width" has "sepal" in a column, and "width" as a column name), here's how we can use `pivot_wider()`:

- `id_cols`: as usual
- `names_from`: Which column contains the part of the variable?
- `names_sep`: As before, what character should you separate the entries of the old columns by?
- `values_from`: Which column names contain the other part of the variable?

Here is the code to go from the "semi-long" form to the original:

```{r}
iris2_longer %>% 
  pivot_wider(id_cols     = c(id, species), 
              names_from  = part, 
              names_sep   = "_",
              values_from = c(length, width))
```


### Activity

Fill out __Exercise 2: Multivariate Pivoting__ in the 
[worksheet](https://raw.githubusercontent.com/USF-Psych-DataSci/Classroom/master/tutorials/s06_pivot-exercise.Rmd).


## Making tibbles

In base R, we can make data frames using the `data.frame()` function. The tidyverse version is `tibble::tibble()`, which also has backwards referencing to variables you make on the fly. It's also stricter by not allowing recycling unless the vector is of length 1:

Good:

```{r}
tibble(x = 1:6,
       y = min(x))
```

Bad:

```{r, error = TRUE}
tibble(x = 1:6,
       y = 1:2)
```

Truly manual construction of tibbles is easy with `tibble::tribble()`:

```{r}
tribble(
  ~Day, ~Breakfast,
  1, "Apple",
  2, "Yogurt",
  3, "Yogurt"
)
```

Check out the `datapasta` package for ways to reproducibly copy-paste data from 
spreadsheets into R. `datapasta` uses `tribble()`.

List columns are easy with tibbles!

```{r}
(list_col <- tibble(n = 1:2,
                    y = list(iris, mtcars)))
```

Often obtained with `nest()` and `unnest()`:

```{r}
(iris_nest <- iris %>% 
  group_by(Species) %>% 
  nest())
```

```{r}
iris_nest %>% 
  unnest(data)
```

`expand_grid()` to obtain all combinations:

```{r}
expand_grid(x = 1:2, y = 1:2, z = 1:2)
```

In conjunction with `nesting()`:

```{r}
expand_grid(nesting(x = 1:2, y = 1:2), z = 1:2)
```


## Implicit `NA`'s

Sometimes there's "hidden" missing data in a tibble. Here's an example  
from the documentation of `tidyr::expand()`:

```{r}
(df <- tibble(
  year   = c(2010, 2010, 2010, 2010, 2012, 2012, 2012),
  qtr    = c(   1,    2,    3,    4,    1,    2,    3),
  return = rnorm(7)
))
```

Here, there are values for `qtr` 1, 2, 3, 4 in `year` 2010 and 
`qtr` 1, 2, 3 in `year` 2012, but no `qtr` 4 in `year` 2012. 
The value of `return` in `year` 2012, `qtr` 4 is "implicilty" `NA`.

We can use functions to fill out this data frame and make these "implicit" missing
values explicit.

We can consider all existing combinations by invoking the column names in 
`expand()` or `complete()` (which either _drops_ or _keeps_ all other columns):

```{r}
df %>% 
  expand(year, qtr)
df %>% 
  complete(year, qtr)
```

We can consider new combinations by specifying an expectation of possible values:

```{r}
df %>% 
  expand(year = 2010:2012, qtr)
df %>% 
  complete(year = 2010:2012, qtr)
```

Want to link two or more columns when looking for combinations? Use `nesting()`.


## Activity (10 min)

Fill out __Exercise 3: Making tibbles__ in the 
[worksheet](https://raw.githubusercontent.com/USF-Psych-DataSci/Classroom/master/tutorials/s06_pivot-exercise.Rmd).

<!--chapter:end:s006_pivoting-tidyr.Rmd-->

# Tibble Joins: A Tale of Two Tibbles

Today's topic is on operations with two or more tibbles. These operations are 
are used to combine two different data tables, compare two different data tables,
verify or filter data in one data table against another, etc.


## Worksheet

You can find a worksheet template for today [here](https://raw.githubusercontent.com/USF-Psych-DataSci/Classroom/master/tutorials/s07_tibble-join-exercise.Rmd).


## Resources

- [Jenny Bryan's join cheatsheet](https://stat545.com/join-cheatsheet.html)
- The `dplyr` "two-table verbs" [vignette](https://cran.r-project.org/web/packages/dplyr/vignettes/two-table.html)
- [Relational Data chapter](https://r4ds.had.co.nz/relational-data.html) in "R for Data Science".
- [dplyr cheatsheet](https://rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf)

For an overview of operations involving multiple tibbles, check out Jenny Bryan's 
[Chapter 14](https://stat545.com/multiple-tibbles.html) in stat545.com.

For more activities, see the matrials from  
[Rashedul Islam](https://github.com/Rashedul/stat545_guest_lecture).


## Join Functions

Often, we need to work with data living in more than one table. There are four 
main types of operations that can be done with two tables (as elaborated in 
[r4ds Chapter 13 Introduction](https://r4ds.had.co.nz/relational-data.html#introduction-7)):

- [__Mutating joins__](https://r4ds.had.co.nz/relational-data.html#mutating-joins) 
  add new columns to the "original" tibble.
- [__Filtering joins__](https://r4ds.had.co.nz/relational-data.html#filtering-joins) 
  filter the "original" tibble's rows.
- [__Set operations__](https://r4ds.had.co.nz/relational-data.html#set-operations) 
  work as if each row is an element in a set. 
- __Binding__ stacks tables on top of or beside each other, with 
  `bind_rows()` and `bind_cols()`.

Let's navigate to each of these three links, which lead to the relevant r4ds 
chapters, and go through the concepts there. These have excellent visuals to 
explain what's going on.

We will also look at the visuals of these concepts [here](https://github.com/gadenbuie/tidyexplain).

Then, let's go through [Jenny Bryans's join cheatsheet](https://stat545.com/join-cheatsheet.html) for examples. 


## Activity

Let's complete [today's worksheet](https://raw.githubusercontent.com/USF-Psych-DataSci/Classroom/master/tutorials/s07_tibble-join-exercise.Rmdd).

<!--chapter:end:s007_table-joins.Rmd-->

# File input/output (I/O)

Today's class is all about reading into R and writing out data files out of R.

Because sometimes you want to work with your own damn data, instead of data 
about flowers, cars, life expectancy, or the Big Five personality scores of 2800
of your closest friends.

## Worksheet

Normally there would be a separate worksheet file, but we'll do everything 
in-line in the guide for this class.

## Resources

### References and tutorials

* Jenny Bryan's [notes](https://stat545.com/import-export.html) on file I/O
* [Tutorial](https://beanumber.github.io/sds192/lab-import.html#data_from_an_excel_file) 
  on importing Excel files
* [Tutorial](http://jenrichmond.rbind.io/post/how-to-use-the-here-package/) on 
  relative paths and how RStudio treats .R script files and .Rmd files

### Package documentation

* [readr::read_csv()](https://readr.tidyverse.org/reference/read_delim.html)
* [readxl::read_excel()](https://readxl.tidyverse.org/reference/read_excel.html)
* [googlesheets4::read_sheet()](https://googlesheets4.tidyverse.org/reference/read_sheet.html)
* [haven](https://haven.tidyverse.org/) package for importing SPSS, SAS, Stata files
* [rio::import()](https://cloud.r-project.org/web/packages/rio/vignettes/rio.html) for importing all manner of formats with one command!
* [here::here()](https://here.r-lib.org)

<!-- The following chunk allows errors when knitting -->

```{r allow errors, echo = FALSE}
knitr::opts_chunk$set(error = TRUE)
```

## Writing data to disk

Let's first load the built-in gapminder dataset and the tidyverse:

```{r, echo = FALSE, warning = FALSE, message = FALSE}
library(gapminder)
library(tidyverse)
```

Next, let's filter the data to only 2007 and only the Asia continent and 
save it to a new object.

```{r}
(gap_asia_2007 <- gapminder %>% filter(year == 2007, continent == "Asia"))
```

We can write this to a comma-separated value (csv) file with just one command:

```
write_csv(gap_asia_2007, "exported_file.csv")
```

But where did this file go? We should save the file in a sensible location. 
We need to practice controlling where R is running, where it looks for files, 
and where it writes out your files. To that end, let's talk about the working 
directory and RStudio Projects.

## The working directory and RStudio Projects

When you open, it "runs" in some folder on your computer. This is the place
it will look for files to import and write files as output. Think about where
your participation and homework files end up when you knit them.

If you have R/RStudio closed, and you open a .R or .Rmd file, R/RStudio will start
in the folder holding that file.

If you open R/RStudio from the Windows Start menu, the Mac dock, the Mac Spotlight,
etc., R/Studio will start in its default location (probably your user home directory,
see Tools → Global Options → General → Default working directory…).

### The working directory

When I say "R/Studio will start in…", what I am referring to is R's "working 
directory". Like I say above, this is the place R will look for files to import 
and write files as output. You can check what R's current working directory 
is using the `getwd()` function:

```{r}
getwd()
```

You can also change the working directory using the `setwd()` function:

```{r, eval = FALSE}
setwd(file.path("path", "to", "folder"))
```

Do not use `setwd()`! You should always write your R scripts so that the entire
project is self-contained in a folder. All of the scripts, folders, data, output, 
etc. should all "live" within this project folder. 

Write all of your R scripts assuming:

  1. You are running them by opening a fresh new R session
     - Don't use `rm(list = ls())` to clean the workspace--the workspace is already clean
     - You need to load required pacakges with `library()`
     - Don't work on several different projects in one R session at the same time!
  2. You have all of the necessary packages installed
     - Don't include `install.packages()` calls unless they are commented out
       or otherwise set not to run
  3. The script will run without human input
     - You need to import or load any data you are working with
     - Load data and write output using R commands, not `file.choose()`, 
       `read.clipboard()`, the buttons in RStudio, etc.
  4. All of the needed files live in your project folder
     - Write _relative_ paths, rather than _absolute_ paths
     - `data/cats-data_2020-03-04.xlsx` or `file.path("data", "cats-data_2020-03-04.xlsx")`
     - Not: `/Users/brenton/Research/cats_project/data/cats-data_2020-03-04.xlsx` 
       or `C:\\Users\\brenton\\Research\\cats_project\\data\\cats-data_2020-03-04.xlsx`  
       or`file.path("C:", "Users", "brenton", "Research", "cats_project", "data", "cats-data_2020-03-04.xlsx")`
  5. Your script might run on any system
     - Write the paths to files using `file.path()` or `here::here()`, rather
       than typing out the whole path (Windows and Mac/Linux have different 
       syntaxes for file paths):
       - Mac/Linux: `path/to/folder`
       - Windows: `path\\to\\folder`

This approach has several advantages:

  1. Frictionless running on different computers
  2. Less breakage (e.g., if you move a folder around, the relative paths will still work)
  3. Less surprising or weird behaviors due to session crud
  4. Easy to tweak data/code and update results


### RStudio projects

RStudio projects can help with following these best practices. Let's make a project.

Click `File → New Project…` and then make a new project in your participation repo
folder. Give it a useful name like `DataSci-participation.Rproj`.

When you double click on a .Rproj file, it:
  1. Opens a new fresh R session, with
  2. The working directory set to the location of the .Rproj file, and
  3. No connection whatsoever to any other R sessions you already have open
  
You can also set specfic options for each RStudio project (e.g., number of spaces
to insert when you type Tab, etc.).

Let's practice closing RStudio and re-opening it by opening the RStudio project.
Run `getwd()` to see where R is running. 


## here::here()

You can make your projects very portable by using .Rproj RStudio projects and 
relative paths with `file.path()`. This still can be a little fiddly:

  1. Need to remember to open the .Rproj file, not a .Rmd or .R file
  2. If you open a .Rmd file contained in a subfolder, the working directory
     won't be right for working with `file.choose()` for relative paths.
     
Enter the `here` package. `here` takes the idea of relative paths and extends it
in very useful ways. 

The major function is `here::here()`. Like `file.path()`, `here::here()` lets you
specify a path to a file and then adds the system-appropriate separators (`/` or `\\`).
Where `here::here()` shines is that it figures out better where the relative paths
should start from. It looks round in the folders in your directory and finds
the .Rproj file, then constructs the relative file paths from there.

For example, create a new folder "data" in your participation repo, and within 
it make a subfolder called "s008_data". Then, save `gap_asia_2007` using the
`here::here()` and `write_csv()` functions:

```{r, eval = FALSE}
write_csv(gap_asia_2007, here::here("data", "s008_data", "exported_file.csv"))
```

More details on `here` are available in this 
[short article](http://jenrichmond.rbind.io/post/how-to-use-the-here-package/).


## Reading data from disk

The same csv file that we just saved to disk can be imported into R again by 
specifying the path where it exists:

```{r, eval = FALSE}
read_csv(here::here("data", "s008_data", "exported_file.csv"))
```

Normally we would store the imported data into a new object that you can use in
subsequent analyses using the assignment function `<-`. 

Notice that the output of the imported file is the same as the original tibble, 
and `read_csv()` was intelligent enough to detect the types of the columns. 
This won't always be true so it's worth checking! The 
[read_csv package](https://readr.tidyverse.org/reference/read_delim.html) has 
many additional options including the ability to specify column types (e.g., 
is "1990" a year or a number?), skip columns, skip rows, rename columns on 
import, trim whitespace, and more.

One of the most important options to set is the `na` argument, which specifies 
what values to treat as `NA` on import. By default, `read_csv()` treats blank 
cells (i.e., `""`) and cells with `"NA"` as missing. You might need to change
this (e.g., if missing values are entered as `-999`). Note that `readxl::read_excel()` 
by default only has `na = c("")` (no `"NA"`)!


## Import a file from the web/cloud

### Import a CSV file from the internet

To import a CSV file from a web, assign the URL to a variable

```{r}
url <- "http://gattonweb.uky.edu/sheather/book/docs/datasets/magazines.csv"
```

and then apply read_csv file to the `url`.

```{r}
read_csv(url)
```

You can do this in one step if you like:

```
read_csv("http://gattonweb.uky.edu/sheather/book/docs/datasets/magazines.csv")
```

(This doesn't work for all import functions, so I usually keep them separate
as two steps.)


### Import an Excel file (.xls or .xlsx) from the internet

First, we'll need the package to load in Excel files:

```{r}
library(readxl) 
```

Datafiles from this tutorial were obtained from:
https://beanumber.github.io/sds192/lab-import.html#data_from_an_excel_file

Unlike with a CSV file, tto import an .xls or .xlsx file from the internet, you 
first need to download it localy. 

**Note: The folder you want to save the file to _has to exist_!. If it doesn't,**
**you will get an error.**
  
Either create the folder path in Finder/Windows Explorer, or use the `dir.create()`
function:

```{r, eval = FALSE}
dir.create(here::here("data", "s008_data"), recursive = TRUE)
```

Next, you download the file. To download it, create a new object called `xls_url` 
and then using `download.file` to dowload it to a specified destination path.

```{r, eval = FALSE}
xls_url <- "http://gattonweb.uky.edu/sheather/book/docs/datasets/GreatestGivers.xls"
download.file(xls_url, here::here("data", "s008_data", "some_file.xls"), mode = "wb")
```

**NOTE: The `mode = "wb"` argument at the end is really important if you are on **
**Windows. If you omit, you will probabbly get a message about downloading a **
**corrupt file. More details about this behaviour can be found **
**[here](https://github.com/HenrikBengtsson/Wishlist-for-R/issues/30).**

Naming a file "some_file" is extremely bad practice (hard to keep track of the 
files), and you should **always** give it a more descriptive name. Often, it's 
a good idea to name the file similarly (or the same as) the original file
(sometimes that might not be a good idea if the original name is non-descriptive). 

There's handy trick to extract the filename from the URL:

```{r, eval = FALSE}
file_name <- basename(xls_url)
download.file(xls_url, here::here("data", "s008_data", file_name), mode = "wb")
```

Now we can import the file:

```{r, eval = FALSE}
read_excel(here::here("data", "s008_data", file_name))
```


## Read in a sample SPSS file. 

Let's load a sample SPSS file and work with it. Download the file from 
[here](https://raw.githubusercontent.com/USF-Psych-DataSci/Classroom/master/data/clevel.sav)
and save it in your `data/s008_data` folder.

These data are a random subset of the data used in [this paper](https://doi.org/10.1016/j.jvb.2018.02.005).
This was a study looking at personality traits that distinguish C-level executives
from lower-level managers among men and women. 

The subset of data here consists of 200 cases, with variables indicating:
  1. The language of assessment (English, Dutch, or French)
  2. Gender
  3. C-level or not
  4. Extraversion level, as well as 4 facet traits (Leading, Communion, Persuasive, Motivating)
  
Let's load in the data using the `haven` package.

```{r, eval = FALSE}
(clevel <- haven::read_spss(here::here("data", "s008_data", "clevel.sav")))
```

Notice that this tibble looks a little different for the `language` and `gender`
variables than normal. It has **labels** for the numeric values. This format
is what SPSS uses, but it's not standard for R. Let's convert those variables, 
and `isClevel` as well, to factors:

```{r, eval = FALSE}
clevel_cleaned <-
  clevel %>% 
  mutate(language = as_factor(language),
         gender = as_factor(gender),
         isClevel = factor(isClevel, 
                           levels = c(0, 1), 
                           labels = c("No", "Yes"))
  ) %>% 
  print()
```

Notice how the variables are now factors with labels as the entries, instead of the 
original code numbers. 


### Saving data frames

Let's save this file as a CSV file so that it's a smaller file and easier to 
import again in the future.

```{r, eval = FALSE}
write_csv(clevel_cleaned, here::here("data", "s008_data", "clevel_cleaned.csv"))
```


### Saving plots

Now let's make a plot.

```{r, eval = FALSE}
clevel_plot <-
  clevel_cleaned %>% 
  mutate(isClevel = recode(isClevel, 
                           No = "Below C-level", 
                           Yes = "C-level"),
         gender = recode(gender,
                         Female = "Women",
                         Male = "Men")) %>% 
  ggplot(aes(paste(isClevel, gender, sep = "\n"), Extraversion, color = gender)) +
  geom_boxplot() +
  geom_jitter(height = .2) +
  scale_color_manual(values = c("#1b9e77", "#7570b3")) +
  ggtitle("Extraversion Stan Scores") +
  scale_y_continuous(breaks = 1:9) +
  ggthemes::theme_fivethirtyeight() %>% 
  print()

```

Let's save the plot in several formats. This is useful if we want to use the plot
outside of Markdown. Save plots using the `ggsave()` funnction.

`ggsave()` has many options. See the help function `?ggsave` for full details.
The main arguments are `filename`, `plot`, `width` and `height` (inches by default),
and `dpi` (dots per inch; for bitmap formats).

`ggsave()` will try to guess what format you want based on the file name. If you
want, you can specify a specific format or R graphics device to save with using
the `device` argument.

```{r, eval = FALSE}
dir.create(here::here("output", "figures"))
ggsave(here::here("output", "figures", "clevel_extraversion.svg"), clevel_plot)
```

You can save to several formats. Generally, work with a _vector_ format like
`.svg`, `.eps`, or `.pdf`. Vector graphics represent the image as a series of
data points and equations. This means that they can be made smaller or larger or
zoomed in on without damaging the image quality.

If you can't use a vector format for some reason, you can also export to a _bitmap_
format. Bitmap graphs represent the image as colored dots or pixels. This means
that the image quality will suffer if you make the image larger or zoom in on it
(making it smaller can also sometimes compromise quality). With bitmap images,
you need to be concerned with *resolution* (how many pixels/dots per inch when
printed). Always use at least **300 DPI** resolution. 

There are several popular bitmap image formats. `.tiff`/`.tif` is the highest
quality, but also the largest. Use it for print graphics, but you should probably
avoid it for images to be hosted on the web. `.png` is a bit smaller, and it 
should be your go to for charts, figures, line drawings, etc. More complex 
images (e.g., photos) can get pretty big with `.png`, though. `.jpg`/`.jpeg` is
probably the most popular bitmap format. It works well for photographs hosted on
the web, but its compression often makes line drawing and charts look terrible.
`.jpg`/`.jpeg` also degrades in quality each time you edit/save it, so don't 
use it for images you intend to edit. Generally avoid `.gif` unless you are
making an animation or you need very small file size for a simple image. `.gif` 
supports very few colors, so always check your image quality after making a `.gif`. 

Let's save to some other formats:

```{r, eval = FALSE}
ggsave(here::here("output", "figures", "clevel_extraversion.eps"), clevel_plot)
ggsave(here::here("output", "figures", "clevel_extraversion.pdf"), clevel_plot)
ggsave(here::here("output", "figures", "clevel_extraversion.tiff"), clevel_plot)
ggsave(here::here("output", "figures", "clevel_extraversion.png"), clevel_plot)
```


## Organizing your project folders

Follow a consistent folder structure for all of your projects. This will make it
easier for you to say organized and make your code, data, and projects easy to 
share. 

In your project's root folder, you should have a README.md file and a .Rproj
project. Then, you should have folders that separate different types of files:

  - `data`: Stores all of your data files for a project
    - Have subfolders for different dates, waves, groups, etc. as needed
    - If you plan on saving cleaned data, having separate `data-raw` and `data`
      folders is a good idea. 
  - `markdown`: Stores your RMarkdown docuemnts. 
  - `output`: Stores any output your scripts generate
    - Depending on how many figures and other output files you will create, 
      you might want to split/subfolder this into `figures`, `reports`, etc.
    
You might add additional folders, such as:

  - `tests`: A folder that includes tests to check that your scripts or results
    are accurate. Check out the `testthat` package.
  - `templates`: A folder to hold template files (e.g., RMarkdown templates,
    Word templates, CSS files for HTML output, TeX templates for PDF output)
  - `admin`: For adminstrative documents (e.g., IRB approval, grant information)
  - `doc`: For documentation (e.g., variable codebooks, style guides), 
  - `scripts` or `src` or `R`: Folders to store functions and scripts that 
    you call from your markdown (e.g., a data import and cleaning script)
    - Use folders for each language if you are programming in multiple languages:
      `R`, `python`, `sql`, `C`, etc.
    - Can put them all in a generic `scripts` or `src` folder if not too many

## Bonus Activity: `rio`

Do the `clevel` activity again, but this time use the `rio::import()` and 
`rio::export()` functions instead of the `read_*()` and `write_*()` functions. 
In `rio::import()`, be sure to specify: `rio::import(..., setclass = "tibble")`.

<!--chapter:end:s008_input-output-project-management.Rmd-->

# Fun with factors

Today's class is working with a very important component of R -- factors
(i.e., categorical or ordinal variables).

## Worksheet

The worksheet on the factors portion of today's class is
[here](https://raw.githubusercontent.com/USF-Psych-DataSci/Classroom/master/tutorials/s09_factors-exercise.Rmd).
It's mostly spaces for you to code along with the lecture notes.

## Resources

### References and tutorials

* Jenny Bryan's notes on [factors](https://stat545.com/factors-boss.html)

### Package documentation

* [forcats](https://forcats.tidyverse.org) package

<!---The following chunk allows errors when knitting--->

```{r allow errors2, echo = FALSE}
knitr::opts_chunk$set(error = TRUE)
```

Load the required libraries.

```{r, echo = TRUE}
library(gapminder)
library(tidyverse) # includes, among others, `forcats`
```

## Motivating the need for factors in R

### Using factors for plotting

**1.1** Let's look again into `gapminder` dataset and create a new cloumn, 
`life_level`, that contains five categories ("very high", "high","moderate", 
"low", "very low") based on life expectancy in 1997. Assign categories accoring 
to the table below:

| Criteria | life_level| 
|-------------|-----------|
| less than 23 | very low |
| between 23 and 48 | low |
| between 48 and 59 | moderate |
| between 59 and 70 | high |
| more than 70 | very high |

Function `case_when()` is a tidier way to vectorise multiple `if_else()` statements. 
You can read more about this function [here](https://dplyr.tidyverse.org/reference/case_when.html).

```{r}
gapminder %>% 
  filter(year == 1997) %>% 
  mutate(life_level = case_when(lifeExp < 23 ~ 'very low',
                                lifeExp < 48 ~ 'low',
                                lifeExp < 59 ~ 'moderate',
                                lifeExp < 70 ~ 'high',
                                # else
                                TRUE ~ 'very high')) %>% 
  ggplot() + 
  geom_boxplot(aes(x = life_level, y = gdpPercap)) +
  labs(y = "GDP per capita, $", x = "Life expectancy level, years") +
  theme_bw() 
```

Do you notice anything odd/wrong about the graph?

We can make a few observations:

- It seems that none of the countries had a "very low" life-expectancy in 1997. 

- However, since it was an option in our analysis it should be included in our plot. Right?

- Notice also how levels on x-axis are placed in the "wrong" order.

**1.2** You can correct these issues by explicitly setting the levels parameter 
in the call to `factor()`. Use, `drop = FALSE` to tell the plot not to drop 
unused levels.

```{r}
gapminder %>% 
  filter(year == 1997) %>% 
  mutate(life_level = factor(
                        case_when(lifeExp < 23 ~ 'very low',
                                  lifeExp < 48 ~ 'low',
                                  lifeExp < 59 ~ 'moderate',
                                  lifeExp < 70 ~ 'high',
                                  # else
                                  TRUE ~ 'very high'),
                        levels = c("very low", "low", "moderate", "high", "very high")
                        )
         ) %>%  
  ggplot() + 
  geom_boxplot(aes(x = life_level, y = gdpPercap)) +
  labs(y = "GDP per capita, $", x = "Life expectancy level, years") +
  theme_bw() +
  scale_x_discrete(drop = FALSE)
```

## Inspecting factors

In Activity 1, we created our own factors, so now let's explore what 
categorical variables that we have in the `gapminder` dataset.

### Exploring `gapminder$continent`

**2.1** Use functions such as `str()`, `levels()`, `nlevels()` and `class()` to 
answer the following questions:

- What class (type of object) is `continent` (a factor or character)?
- How many levels? What are they?
- What integer is used to represent factor "Asia"?

```{r}
class(gapminder$continent)
levels(gapminder$continent)
nlevels(gapminder$continent)
str(gapminder$continent)
gapminder
```

### Exploring `gapminder$country`

**2.2** Let's explore what else we can do with factors. Answer the following questions: 

- How many levels are there in `country`?
- Filter `gapminder` dataset by 5 countries of your choice. How many levels are 
  in your filtered dataset?

```{r}
nlevels(gapminder$country)

h_countries <- c("Egypt", "Haiti", "Romania", "Thailand", "Venezuela")

h_gap <- gapminder %>%
  filter(country %in% h_countries)

nlevels(h_gap$country)
```

## Dropping unused levels

What if we want to get rid of some levels that are "unused" -- how do we do that? 

The function `droplevels()` operates on all the factors in a data frame or on a 
single factor. The function `forcats::fct_drop()` operates on a factor and does 
not drop `NA` values.

```{r}
h_gap_dropped <- h_gap %>% 
  droplevels()

h_gap_dropped$country %>%
  nlevels()
```

## Changing the order of levels

Let's say we wanted to re-order the levels of a factor using a new metric - say, count().

We should first produce a frequency table as a tibble using `dplyr::count()`:

```{r}
gapminder %>% 
  count(continent)
```

The table is nice, but it would be better to visualize the data.
Factors are most useful/helpful when plotting data.
So let's first plot this:

```{r}
gapminder %>%
  ggplot() +
  geom_bar(aes(continent)) +
  coord_flip() +
  theme_bw() +
  ylab("Number of entries") + xlab("Continent")
```

Think about how levels are normally ordered. 
It turns out that by default, R always sorts levels in alphabetical order. 
However, it is often preferable to order the levels according to some principle:

  1. Frequency/count (`fct_infreq()`)
  2. Order of appearance (`fct_inorder()`)
  3. Sequence of the underlying integers (`fct_inseq()`)
  
- Make the most common level the first and so on. Function like `fct_infreq()` 
  might be useful.
- The function `fct_rev()` will sort them in the opposite order.

```{r}
gapminder %>%
  ggplot() +
  geom_bar(aes(fct_infreq(continent))) +
  coord_flip() +
  theme_bw() +
  ylab("Number of entries") + xlab("Continent")
```

Section 9.6 of Jenny Bryan's [notes](https://stat545.com/factors-boss.html#reorder-factors) has some helpful examples.

  4. Another variable. 
  
  - For example, if we wanted to bring back our example of ordering `gapminder` 
    countries by life expectancy, we can visualize the results using `fct_reorder()`. 

```{r}
##  default summarizing function is median()
gapminder %>%
  ggplot() +
  geom_bar(aes(fct_reorder(continent, lifeExp, max))) +
  coord_flip() +
  theme_bw() +
  xlab("Continent") + ylab("Number of entries") 
```

Use `fct_reorder2()` when you have a line chart of a quantitative _x_ against 
another quantitative _y_ and your factor provides the color. 

```{r}
## order by life expectancy 
ggplot(h_gap, aes(x = year, y = lifeExp,
                  color = fct_reorder2(country, year, lifeExp))) +
  geom_line() +
  labs(color = "Country")
```

## Change order of the levels manually

This might be useful if you are preparing a report for say, the state of affairs 
in Africa.

```{r}
gapminder %>%
  ggplot() +
  geom_bar(aes(fct_relevel(continent, "Oceania"))) +
  coord_flip() +
  theme_bw() 
```

More details on reordering factor levels by hand can be found [here](https://forcats.tidyverse.org/reference/fct_relevel.html).

## Recoding factors

Sometimes you want to specify what the levels of a factor should be.
For instance, if you had levels called "blk" and "brwn", you would rather they 
be called "Black" and "Brown" -- this is called recoding.

Lets recode `Oceania` and the `Americas` in the graph above as abbreviations 
`OCN` and `AME` respectively using the function `fct_recode()`.

```{r}
gapminder %>%
  ggplot() +
  geom_bar(aes(fct_recode(continent, "OCN" = "Oceania", "AME" = "Americas"))) +
  coord_flip() +
  theme_bw()
```

## Grow a factor

Let’s create two data frames,`df1` and `df2` each with data from two countries, 
dropping unused factor levels.

```{r}
df1 <- gapminder %>%
  filter(country %in% c("United States", "Mexico"), year > 2000) %>%
  droplevels()
df2 <- gapminder %>%
  filter(country %in% c("France", "Germany"), year > 2000) %>%
  droplevels()
```

The country factors in `df1` and `df2` have different levels.
Can you just combine them using `c()`?

```{r}
c(df1$country, df2$country)
```

Use `fct_c()` to perform `c()`, but also combine the levels of the two factor
variables:

```{r}
fct_c(df1$country, df2$country)
```

Explore how different forms of row binding work behave here, in terms of the 
country variable in the result. 

```{r}
bind_rows(df1, df2)
rbind(df1, df2)
```


<!--chapter:end:s009_factors.Rmd-->

```{r, include=FALSE}
knitr::opts_chunk$set(fig.width = 7, fig.height = 5)
```

# Effective Visualizations

Now that you know how to create graphics and visualizations in R, you are armed 
with powerful tools for scientific computing and analysis. But with great power 
comes great responsibility. Effective visualizations is an incredibly important 
aspect of scientific research and communication. There have been several books 
(see references) written about these principles. In class today we will be going 
through several case-studies trying to develop some expertise into making 
effective visualizations. 

## Worksheet

Not much coding here, but there is a supplemental worksheet you will do as 
homework [here](https://raw.githubusercontent.com/USF-Psych-DataSci/Classroom/master/tutorials/s10_plot-theme-exercise.Rmd)

## Resources

1. [Fundamentals of Data Visualization](https://serialmentor.com/dataviz/introduction.html) by Claus Wilke
1. [Visualization Analysis and Design](https://www-taylorfrancis-com.ezproxy.library.ubc.ca/books/9780429088902) by Tamara Munzner
1. [STAT545.com - Effective Graphics](https://stat545.com/effective-graphs.html) by Jenny Bryan
1. [ggplot2 book](https://ggplot2-book.org) by Hadley Wickam
1. [Callingbull.org](https://callingbull.org/tools.html) by Carl T. Bergstrom and Jevin West

A great resource for selecting the right plot: https://www.data-to-viz.com/.
I encourage you all to consult it when building visualizations.

## Part 1: Principles of effective visualizations

1. Apply the [principle of proportional ink](https://callingbullshit.org/tools/tools_proportional_ink.html)
    - Definition: "The amount of ink used to indicate a value should be 
      proportional to the value itself."
    - Example: Truncating the y-axis on a bar chart to exaggerate the difference 
      between bars violates the principle of proportional ink
1. Maintain a high data-to-ink ratio: [Less is more](https://speakerdeck.com/cherdarchuk/remove-to-improve-the-data-ink-ratio)
    - Definition: Remove distracting visual elements to focus attention on the data
    - Examples: Lighten line weights, remove backgrounds, never use 3D or 
      special effects, remove unnecessary/redundant labels, etc.
1. Reduce the cognitive load on viewers
    - Don't make viewers search for information or meanings
    - Examples: Use direct labels rather than legends, arrange elements in logical 
      order, don't violate expectations
1. Always update axes labels and titles on your plots
1. Choose your scale-type carefully
    - Whether you choose a linear, logarithm, sqrt scale depends on your data, 
      context, and purpose
1. Choose your graph-type carefully
    - Examples: [here](https://serialmentor.com/dataviz/directory-of-visualizations.html) is a great directory of plots
1. Choose colours with accessibility, data-fidelity, and readability in mind
    - I am colorblind! Red and black lines are identical to me.
    - Examples: [here](http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/#a-colorblind-friendly-palette) 
      is a great set of colour schemes that are colour-blind friendly and perceptually uniform

### Make a great plot worse

Instructions: Below is a code chunk that shows an effective visualization. 
First, copy this code chunk into a new code blockk. Then, modify it to purposely 
make this chart "bad" by breaking the principles of effective visualization above. 
Your final chart still needs to run/compile and it should still produce a plot. 

```{r, message = FALSE, warning = FALSE}
library("tidyverse")

ggplot(airquality, aes(`Month`, `Temp`, group = `Month`)) +
    geom_boxplot(outlier.shape = NA) +
    geom_jitter(alpha = 0.3) +
    labs(x = "",
         y = "",
         title="Maximum temperature by month") +
    theme_bw() + 
    scale_x_continuous(breaks = c(5, 6, 7, 8, 9), 
                       labels = c("May", "June", "July", "August", "September")) +
    annotate("text", x = 4.08, y = 95, label="°F", size = 8) +
    coord_cartesian(xlim = c(4.5, 9.5),
                    clip = 'off') +
    theme(panel.grid.minor = element_blank(),
          panel.background = element_blank(), 
          axis.line = element_line(colour = "gray"),
          panel.border = element_blank(),
          text = element_text(size=18)
          )
```

How many of the principles did you manage to break?

## Part 2: Plot theming: The glamour of graphics

Let's watch a great video explaining and demonstrating some important principles
of the aesthetic/design choices for visualization:
[here](https://resources.rstudio.com/rstudio-conf-2020/the-glamour-of-graphics-william-chase).

You can customize the design elements of a ggplot using themes. The `theme()`
function is a general way to customize all design elements. There are also
many themes already available, such as in the `ggthemes` package.

## Part 3: Arranging plots with `patchwork`

In Assignment 3, I asked you to try to arrange two plots side by side. There is 
recently a new package called `patchwork` that makes this super easy!

```{r}
library(patchwork)
library(gridExtra)

p_mpg <- ggplot(mtcars, aes(factor(cyl), mpg, color = factor(cyl))) + 
    geom_boxplot() + theme_bw() + xlab("No. of cylinders") + ylab("MPG")

p_hp <- ggplot(mtcars, aes(factor(cyl), hp, color = factor(cyl))) + 
    geom_boxplot() + theme_bw() + xlab("No. of cylinders") + ylab("Horsepower")

p_mpg + p_hp 

p_mpg / p_hp

p_mpg + p_hp + plot_layout(guides = 'collect')
```

Package docs: https://patchwork.data-imaginist.com/index.html
Nice tutorial: [here](https://cmdlinetips.com/2020/01/tips-to-combine-multiple-ggplots-using-patchwork/)

Some other packages you might find useful for making awesome data displays:

  - `ggtext`: Lets you use (_some_) Markdown formatting in text on ggplot 
    (e.g., use images in your factor labels!)
     - italics, bold, color, superscript, subscript, font family, images
     - Install: 
         - Currently only available on GitHub: remotes::install_github("wilkelab/ggtext")
         - You _might_ also need to install the development version of `ggplot2`:
           `remotes::install_github("tidyverse/ggplot2")
     - Package docs: https://wilkelab.org/ggtext/
     - Amazing presentation:
       [here](https://resources.rstudio.com/rstudio-conf-2020/spruce-up-your-ggplot2-visualizations-with-formatted-text-claus-wilke)
       
  - `ggstatsplot`: Annotate plots with statistical test results easily
    - Install: `install.packages("ggstatsplot")`
    - Package docs: https://indrajeetpatil.github.io/ggstatsplot/
       
  - `ggpubr`: An older package for arranging plots, adding annotations and 
    data tables, etc. A bit harder to use than patchwork, but lots of code online.
    - Install: `install.packages("ggpubr")`
    - Package docs: https://rpkgs.datanovia.com/ggpubr/index.html
    - Nice tutorial: 
      [here](http://www.sthda.com/english/articles/24-ggpubr-publication-ready-plots/81-ggplot2-easy-way-to-mix-multiple-graphs-on-the-same-page/)

## Supplemental worksheet: Practice scales and theming (Homework)

For homework this week, review and practice making beautiful visualizations with
ggplot. There is an extra participation worksheet you can work through to go
over the basic mechanics of formatting scales, specifying accessible colors, and 
setting theme elements in ggplots. There is also a short guided activity showing 
you how to make a ggplot interactive using plotly.

- [Supplemental worksheet here](https://raw.githubusercontent.com/USF-Psych-DataSci/Classroom/master/tutorials/s10_plot-theme-exercise.Rmd)

<!--chapter:end:s010_effective-viz.Rmd-->

# The Model-Fitting Paradigm in R

## Update Regarding Covid-19

I hope you are all doing okay and staying safe. I've posted some guidelines for 
the rest of the semester on the 
[course website](https://wiernik-datasci.netlify.com/#covid-19).

A few additional points:

  1. To be maximally flexible with people's schedules and obligations at this 
     time, there will not be live class sessions for the rest of the semester.
  2. There will not be graded homework or peer review for the rest of the semester.
  3. The only remaining graded activities will be:
     1. The final project
     2. Your participation repo 
        - Only activities before Spring Break are required; the remaining 
          activities are optional.
  4. I will post class materials tomorrow morning for you to work through on 
     your own schedule. 
  5. I am available to talk via your preferred platform (Skype, Teams, Google 
     Hangouts, FaceTime, Zoom, Phone, Smoke Signals, Carrier Pigeons) with 
     questions, feedback, etc. 
     - Please message me (text, GitHub, Twitter, email) to arrange a time.
  6. Final grading will be generous and cognizant of the current situation.


## Today

Up to now, we have been developing skills for data wrangling and 
_exploratory data analysis_ (e.g., making summary tables, visualizing trends).
Sometimes though, we would like to actually fit formal statistical models and 
use these to draw inferences and conclusions. 

Today, we are going to look at the model-fitting paradigm in R. We will see how
to fit formal models (e.g., regression models [including things like _t_ tests,
ANOVA, correlation analysis, etc.]). We will also see how to work with the
results of R model objects.

To that end, today 

1. Introduction and motivation for model-fitting in R.
1. Distinguish exploratory data analysis from model-fitting.
1. Practice a full model-fitting analysis workflow.


## Resources

- [`broom` vignette](https://cran.r-project.org/web/packages/broom/vignettes/broom.html)

- [*R for Data Science* chapter on modeling](https://r4ds.had.co.nz/model-basics.html)

If you're interested in learning more about the actual statistical/machine 
learning methods for fitting models, I highly recommend the books 
[*Learning Statistics with R*](https://learningstatisticswithr.com/) and
[*An Introduction to Statistical Learning*](https://www-bcf.usc.edu/~gareth/ISL/).
Both are freely available online and use R. *Learning Statistics with R* is
particularly well-written and funny.

```{r packages, cache = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(broom)
set.seed(1234)

theme_set(theme_minimal())
```


## What is Model-Fitting?[^thanks]

We often want to use information on one (or more) variable (the "predictor(s)") 
to learn or make judgments about another variable (the "criterion" or "response").
For example, if I know someone's height, what can I say about their weight? In
general, someone who is 7 feet tall probably weighs more than someone who is 4
feet tall.

**Example:** Consider the scatterplot below.

1. A car weighs 4000 lbs. What can we say about its mpg?
2. A car weights less than 3000 lbs. What can we say about its mpg?

```{r, fig.width=5, fig.height=3}
ggplot(mtcars, aes(wt, mpg)) +
  geom_point() +
  labs(x = "Weight (1000's of lbs)")
```

In the `mtcars` data, a car's mpg and its weight are _correlated_ (they are not
"independent"). So, we could make decent predictions about its miles per gallon
if all we knew was its weight.

**Example:** What can we say about rear axle ratio if we know something about 
quarter mile time?

```{r, fig.width=5, fig.height=3}
ggplot(mtcars, aes(qsec, drat)) + 
  geom_point() +
  labs(x = "Quarter mile time",
       y = "Rear axle ratio")
```

This approach we use above relies on graphs and visual inspection to make
inferences. This graphical approach is often called _exploratory data analysis_.
(They are many methods for exploratory data analysis [EDA], but graphical methods
are some of the most common.)

Sometimes, EDA isn't enough. Perhaps we want to make more specific or precise
predictions (e.g., what is the specific range of mpg values I can expect for a
car that weights 4000 lbs?). Perhaps the relationship between two variables is
fairly weak (but not zero!), so it's hard to see a pattern just with your eyes.

In these cases, we can answer questions more precisely by _fitting a model_: 
a curve that predicts a response variable, $y$, using a predictor variable (or
variables), $x$. This is also called a __regression curve__ or a 
__machine learning model__. 

(There are more comprehensive models too, such as modeling entire distributions, 
but we will keep things simpler here.)

There are typically two goals of fitting a model:

1. Make predictions.
   - If I know a patient's diagnostic test score, how likely are they 
     attempt suicide?
   - If I know a medical school applicant's MCAT score, what GPA are they 
     expected to obtain?
2. Interpret variable relationships ("explanation").
   - Is age related to personality scores?
   - Does political orientation influence the effectiveness of different
     messaging techniques?
     
The advantage of formal modeling is that it (1) provides more specific numeric
estimates of variable relationships and predicted outcomes and (2) provides a
numeric indication of uncertainty (using uncertainty intervals).


## The Linear Model

A linear model is a specific and very useful type of model. A linear model says
that if we change the predictor variable $x$ by a certain amount, we expect the
response variable $y$ to also change by a specific (constant) amount. Most statistics
we use in psychology are linear models (e.g., correlations, t-tests, regression,
ANOVA, ANCOVA, …)

(We aren't going to dive into all of the details of linear models [or other types
of models] or their assumptions. You can learn more about those in your stats
classes or using the books in the **Resources** section. For now, focus on the 
basic idea of the linear model as quantifying the [linear] relationship between
variables and estimating numeric values to accompany your plots.)

Linear models have the generic form

$$y = \beta_0 + \beta_1 \times x$$

where $y$ is the **outcome of interest**, $x$ is the **explanatory** or **predictor**
variable, and $\beta_0$ and $\beta_1$ are **parameters** that reflecct the relationship
between the two variables (i.e., _"how do I use information on $x$ to predict a 
value on $y$?"). $\beta_1$ is the _slope_ (how much does $y$ change for each change
of 1 in $x$?). $\beta_0$ is the _intercept_ (what is the expected value of $y$ 
when $x$ is _0_?). 

We use the observed data for $x$ and $y$ to generate a **fitted model**, where 
we make our best guess as to the true values for the $\beta_0$ and $\beta_1$ 
parameters by picking values that best fit the data.

Let's look at some simulated data with predictor $x$ and response $y$. 

```{r sim-plot}
ggplot(modelr::sim1, aes(x, y)) + 
  geom_point()
```

This looks like a linear relationship. We could randomly draw prediction lines. 
(That is, we could randomly generate values for $\beta_0$ and $\beta_1$ for the 
formula $y = \beta_0 + \beta_1 \times x$ to try and explain or predict the 
relationship between $x$ and $y$.):

```{r sim-random-fit}
models <- tibble(
  a1 = runif(250, -20, 40),
  a2 = runif(250, -5, 5)
)

ggplot(modelr::sim1, aes(x, y)) + 
  geom_abline(aes(intercept = a1, slope = a2), data = models, alpha = 1/4) +
  geom_point()
```

Obviously some of these lines are better than others (some values of $\beta_0$ 
and $\beta_1$ better capture the true relationship between $x$ and $y$). 

We need a definition of "better" to separate good sets of parameter values from 
bad sets of parameter values. One approach that is widely used called **least squares**.
Least squares means that $\beta_0$ and $\beta_1$ are chosen to _minimize_ the 
_sum of the squares of the errors_ of the predictions made by the model. That is,
which line can we draw that will get as many of the data points as close as possible
to that line? The errors in prediction (how far are the points from their predicted
values [the line]?) are the _vertical_ difference between the actual values for 
$y$ and the predicted values for $y$ (the points on the line).

```{r sim-error, echo = FALSE}
dist1 <- modelr::sim1 %>% 
  mutate(
    dodge = rep(c(-1, 0, 1) / 20, 10),
    x1 = x + dodge,
    pred = 7 + x1 * 1.5
  )

ggplot(dist1, aes(x1, y)) + 
  geom_abline(intercept = 7, slope = 1.5, color = "grey40") +
  geom_point(color = "grey40") +
  geom_linerange(aes(ymin = y, ymax = pred), color = "#3366FF")
```

[R for Data Science](http://r4ds.had.co.nz/model-basics.html) gives a good 
introduction to performing these calculations manually by writing your own 
functions. I encourage you to read through and practice some of this code, 
especially if you have no experience with linear models.

However for our purposes, let's focus on using existing R tools to fit a linear
model. You can use `ggplot2` to draw the best-fit line:

```{r sim-plot-lm}
ggplot(modelr::sim1, aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm")
```

The line in blue is the best-fit line. The gray band is the 95% confidence/compatibility 
intervals for the predicted values. A confidence interval indicates the range of 
true parameter values that are reasonably compatible with your data (1) assuming 
your model assumptions are correct (e.g., the relationship between the variables 
is linear) and (2) with a specified Type 1 error rate (e.g., a 95% confidence 
interval is expected to miss the true parameter value 5% of the time). (For more 
discussion on the meaning and interpretation of confidence/compatibility, see
[this discussion](https://doi.org/10.1136/bmj.l5381).)

The `method = "lm"` part of `geom_smooth()` specifies what type of model to use
to fit the line. `"lm"` stands for "linear model" and uses R's function for 
fitting linear models, `lm()`, to fit the line. `lm()` is one of many modeling 
function in R. 

## Model-Fitting in R

Model fitting methods usually use a common format in R:

```
method(formula, data, options)
```

They also tend to have a common output object: a special _list_. 

__method__:

The modeling function, such as:

- Linear Regression: `lm()`
- Generalized Linear Regression: `glm()` (e.g., logistic regression, Poisson regression)
- Local regression (aka "smoother lines"): `loess()`
- Quantile regression: `quantreg::rq()`
- Meta-analysis: `metafor::rma()`
- ...

For now, we are only goint to use `lm()`.

__formula__:

Formulas are used to describe the structure of the model you want to fit. What
is the response variable you are predicting? What are the predictor variables?

In R, formulas take the form `y ~ x1 + x2 + ... + xp`.

The left side of the formula (before `~`) is the response variable. The right side
of the formula (after `~`) are the predictors. Separate multiple predictors using
`+`. (We will get to some more complex formulas later down the page.)

`y`, `x1`, `x2`, etc. are the column names in your data frame. R will look into 
your data frame and look for the columns with the names given in your formula
(much like `aes()` in ggplot).

__data__: The data frame.

__options__: Additional arguments specific to the method and function.


### Example model using `gapminder`:

1. Fit a linear regression model to life expectancy ("Y") from year ("X") by 
   filling in the formula. 
   
First, create a subset of the `gapminder` dataset containing only data from Europe

```{r}
(gapminder_europe <- filter(gapminder, continent == "Europe"))
```

Now, use the `lm()` function to fit a linear model:

```{r}
(mod_europe <- lm(lifeExp ~ year, data = gapminder_europe))
```

When we print `mod_europe`, it shows us the "coefficients" for the model (the
estimated values of $\beta_0$ and $\beta_1$). Does that mean that the life 
expectency at "year 0" was equal to -397.7646?!

No, that doesn't make much sense. We need to be careful to only interpret the
predictions of the model for the general range of the variables we actually
have in our data. 

We can modify how we fit the model to make the value of the `(Intercept)` ($\beta_0$)
more meaningful. Let's fit it so that it reflects the predicted life expectancy 
value for the first year in the data, 1952. To do that, we use the `I()` function. 
`I()` lets you tell R to fit the model to an arithmetic transformation of a 
variable. For example:

```{r}
(mod_europe <- lm(lifeExp ~ I(year - 1952), data = gapminder_europe))
```

In this model, the `(Intercept)` coefficient ($\beta_0$) is the predicticed life
expectancy in 1952, and the `I(year - 1952)` coefficient ($\beta_1$) is the expected
increase in life expectancy each year.

What type of R object is the output of `lm()`? Let's see.

```{r}
class(mod_europe)
is.list(mod_europe)
```

`mod_europe` is an object of class `"lm"` (an `lm` model object). This is a special
type of `list`. A `list` is a vector than can contain all manner of different types
of values, and the values don't have to all be the same or be the same length.

Let's look at the structure of an `"lm"` model object:

```{r}
str(mod_europe)
```

An `"lm"` model object contains a lot of important information, some of which 
you may recognize but most of it you do not. It's a bit of a mess inside (don't 
tell its mom). It can be easier to see what slots an object like this contains 
using the `names()` function:

```{r}
names(mod_europe)
```

We can view a summary of the model results using the `summary()` function:

```{r}
summary(mod_europe)
```

Under the hood, the result of `summary(mod_europe)` is another list that contains
even more information:

```{r}
names(summary(mod_europe))
```

As you can see, model objects are decidely _*not*_ tidy. So, we aren't going to 
pull results directly from them. Instead, we are going to use functions from base
R and the `broom` package to tidy up and navigate model results.

### Predicted Values

One basic thing we might want to do is extract the predicted (or "fitted") values
for the model. We can do this using the `predict()` function:

```{r}
predict(mod_europe)
```

`predict(mod_europe)` alone returns just the fitted values. We can additionally
get a confidence interval for these fitted values by adding `interval = "conf"`:

```{r}
predict(mod_europe, interval = "confidence") # can abbreviate to "conf"
```

Or we can predict on a new dataset:

```{r}
new_data_france <- tibble(year = c(1955, 1964, 1971, 1998, 2000, 2006))
predict(mod_europe, newdata = new_data_france, interval = TRUE)
```

(If you want a _prediction interval_ [what is the whole predicted _range_ of 
values, not just the predicted mean value, we can expect in new data], 
specify `interval = "prediction"` or `interval = "pred"` instead.)

### Residuals

We might also want to extract the model *residuals*; the differences between
the model predicted values and the actual data for each case. Residuals are useful
for evaluating model fit and other diagnostics.

```{r}
resid(mod_europe)
```

### `broom::augment`

The `augment()` function from the `broom` package can compute fitted and residual
values, as well as several model diagnostic statistics for each data point.
`augment()` always returns a data frame, so it is more convenient to use for things
like plotting than `predict` or `resid`. It doesn't currently return 
confidence/prediction intervals for `lm` objects, though.

```{r}
augment(mod_europe, data = gapminder_europe)
```

A linear model assumes that the residuals are normally distributed, so looking at
a plot of the residuals is useful to determine if our model is poorly fit:

```{r}
augment(mod_europe, data = gapminder_europe) %>% 
  ggplot(aes(x = .resid)) +
  geom_histogram() +
  theme_minimal() +
  xlab("Model residuals") +
  ylab("Count")
```

A plot of the residuals against the observed or fitted values can also help to 
show if a specific range of values is more poorly predicted than others:

```{r}
augment(mod_europe, data = gapminder_europe) %>% 
  ggplot(aes(x = year, y = .resid)) +
  geom_point() +
  theme_minimal() +
  xlab("Year") +
  ylab("Model residuals")
```


### Model coefficients

If we are interested in using our model to understand relationships between the
variables, we will also want to examine and interpret the model coefficients
$\beta_0$ and $\beta_1$ themselves. 

We can view the model coefficients using the `coef()` function:

```{r}
coef(mod_europe)
```

We can get more information about the coefficients, such standard errors and
hypothesis tests using `summary()`:

```{r}
summary(mod_europe)
```

We can get confidence intervals for the coefficients using the `confint()` function:

```{r}
confint(mod_europe)
```

Rather than using all of these indivdual functions, the `broom::tidy()` function
puts them all together and conveniently returns the results as a data frame
(great for tabling or plotting):

```{r}
tidy(mod_europe, conf.int = TRUE)
```

Looking at these results, we can see that the predicted average life expectancy
for European countries in 1952 is `r  tidy(mod_europe, conf.int = TRUE)[1, "estimate"]` years
[with a 95% confidence interval of `r  tidy(mod_europe, conf.int = TRUE)[1, c("conf.low", "conf.high")]`, indicating a range of true values that are reasonably compatible with our
data]. Each year, the predicted life expectancy increases by `r  tidy(mod_europe, conf.int = TRUE)[2, "estimate"]` years [95% CI `r  tidy(mod_europe, conf.int = TRUE)[1, c("conf.low", "conf.high")]`].


### Model fit

Finally, we might want to evaluate overall how well our model fits the data. For
example, how well does `year` alone account for all of the variability in life
expectancy in European countries over time? We can obtain a variety of model fit
statistics using the `broom::glance()` function.

```{r}
glance(mod_europe)
```

In this data frame, `adj.r.squared` is the squared correlation between the 
predicted values and the actual values for life expectancy. The square root of this
value is useful for evaluating model fit. 
Here, _R_ = `r sqrt(glance(mod_europe)$adj.r.squared)`, indicating a very strong 
relationship between year and life expectancy, but there is still some variability
left over across countries in a single year. The `sigma` value is the left over
(residual) standard deviation of the response variable (life expectancy) after
accounting for the predictors (year). Here, within a single year, countries'
life expectancies have a standard deviation of `r glance(mod_europe)$sigma`.

If you want a full ANOVA table for your model, use the `anova()` function:

```{r}
anova(mod_europe)
```

(I prefer to focus on the adjusted $R^2$ and `sigma` values to the ANOVA table.)

## Categorical and Multivariable Models

If you include a character or factor variable as a predictor, R will turn this
into a series of dummy-coded contrast variables:

```{r}
(mod_europe_country <- lm(lifeExp ~ country, data = gapminder_europe))
```

You can fit models with multiple predictors by adding them to the right side of
your formula:

```{r}
(mod_europe_gdp <- lm(lifeExp ~ year + gdpPercap, data = gapminder_europe))
```

You can transform variables before including them in the model:

```{r}
(mod_europe_lgdp <- lm(lifeExp ~ year + log(gdpPercap), data = gapminder_europe))
```

You can specify squared terms or other arithmetic transformations using the 
`I()` function:

```{r}
(mod_europe_yrsq <- lm(lifeExp ~ year + I(year^2), data = gapminder_europe))
```

You can specify interactions between two variables using `*`. This will include
both variables themselves (`year`, `log(gdpPercap)`) and their product/interaction
(`year:gdpPercap`):

```{r}
(mod_europe_interaction <- lm(lifeExp ~ year * log(gdpPercap), data = gapminder_europe))
```

You can use `anova()` to compare two nested models:

```{r}
anova(mod_europe, mod_europe_yrsq)
```

This comparison suggests that adding a squared term for year only slightly
improves model fit. The year-trend for life expectancy is most linear. 


## Summary

Model-fitting is about making specific numeric predictions about 
(1) individual scores or (2) relationships between variables. It is important
to evaluate models using both (1) the estimated/predicted values for individual
data points and model coefficients, as well as (2) an uncertainty interval that
indicates a range of plausible true values that are compatible with your data.

After fitting a model using `lm()` or another modeling function, you can use
`summary()`, `predict()`, `resid()`, `confint()`, and other functions to explore
model results. The `broom::augment()`, `broom::tidy()`, and `broom::glance()`
packages are particularly useful for extracting a variety of useful statistics
in an easy-to-use format. You can then pass these results to `knitr::kable()`,
`ggplot()`, or other functions to report and present them.


## Activity

Use the `psych::bfi` dataset. Compute mean scores for each of the Big Five scales
for each person. Then, fit linear models to answer the following questions. Present
your results using both tables of results and figures.

1. Do men and women differ on the Big Five traits? How big are the differences?
2. Do the Big Five traits increase or decrease with Age? Is there a linear or 
   squared trend?
3. Do the Big Five traits differ across educational levels? Treat education as 
   a categorical variable.
4. How well do age and gender together predict the Big Five traits?
5. In your models in part (4), do the residuals appear to be normally distributed?
   Are they consistent across age ranges and gender groups?


[^thanks]: Some parts of this tutorial are adapted from a
[tutorial](https://cfss.uchicago.edu/notes/linear-models/) by Dr. Benjamin Soltoff.

<!--chapter:end:s011_model-framework.Rmd-->

# Let's get functional

## Today

Today, we are going to expand on last week's topic of model-fitting to cover
three important topics:
  1. Using a "functional programming" approach to make your code more efficient
     and avoid "repeating yourself"—copying and pasting essentially the same code
     over and over again 
  2. As part of the functional programming approach, how to write your own
     functions in R to do useful things.
  

## Resources

1. Chapter 21 of [R for Data Science](https://r4ds.had.co.nz/iteration.html).
2. [Learn to purr](http://www.rebeccabarter.com/blog/2019-08-19_purrr/) blog post.
3. Chapter 9 of [Advanced R for Data Science](https://adv-r.hadley.nz/functionals.html).

```{r}
library(tidyverse)
library(broom)
library(psychTools)
```

## What is functional programming?

I recommend you start today by watching 
[this presentation](https://youtu.be/bzUmK0Y07ck?t=251) by Hadley Wickham, the 
chief scientist at RStudio and the architect of the tidyverse, on functional 
programming. He uses a really great analogy of baking cupcakes to motivate
why a functional approach is useful, and he demonstrates the skills we will
be using today.

If you don't watch the whole video, at least read through and understand
[the slides](https://speakerdeck.com/hadley/the-joy-of-functional-programming) 
accompanying the presentation.

Until now, you've problem approached each **R** programming task as its own problem:
you figured out what variables you were working with and then wrote code to wrangle,
transform, plot, or model that variable. For example, in last week's activity, 
you looked at gender differences in the Big Five personality traits using the
`psych::bfi` dataset. Your code for that probably looked something like this:

```{r}
bfi <- psychTools::bfi
keys <- replace_na(psychTools::bfi.dictionary$Keying, 1)

bfi_mean <- bfi %>% mutate_at(names(bfi)[keys == -1], ~ 7 - .x) %>% 
  mutate(A = rowMeans(select(., A1:A5), na.rm = TRUE),
         C = rowMeans(select(., C1:C5), na.rm = TRUE),
         E = rowMeans(select(., E1:E5), na.rm = TRUE),
         N = rowMeans(select(., N1:N5), na.rm = TRUE),
         O = rowMeans(select(., O1:O5), na.rm = TRUE),
         gender = recode_factor(gender, `1` = "male", `2` = "female"),
         education = recode_factor(education, `1` = "some hs", 
                                              `2` = "hs", 
                                              `3` = "some college", 
                                              `4` = "college",
                                              `5` = "graduate degree")) %>% 
  select(gender:O)
  
mod_gender_a <- lm(A ~ gender, data = bfi_mean)
mod_gender_c <- lm(C ~ gender, data = bfi_mean)
mod_gender_e <- lm(E ~ gender, data = bfi_mean)
mod_gender_n <- lm(N ~ gender, data = bfi_mean)
mod_gender_o <- lm(O ~ gender, data = bfi_mean)
```

This works, but it can get really tedious. What if you wanted to do this sort
of analysis with 25 varibles? Or 100 variables? What if you wanted to fit a bunch 
of similar, but slightly different, versions of the model (e.g., regressing on
gender, age, education, or a combination of these)? That is a recipe for boredom
and finger cramps at best and typos or errors at worst (potentially with big
consequences if you don't catch them before they are disseminated). 

Functional programming can come to the rescue here. The idea of functional 
programming is that, rather than writing bespoke code for each new analysis 
or each new variable, we can **write the code once** and then 
**apply it over and over again** as needed for new analyses. 

For example, we can streamline the code above using the `map()` function in the
`purrr` package:

```{r}
bfi_just_traits <- select(bfi_mean, A:O)
mods_gender <- 
  map(bfi_just_traits,
      ~ lm(.x ~ gender, data = bfi_mean)
      )

# Or, using the pipe without making an intermediate object

mods_gender <- 
  bfi_mean %>% 
  select(A:O) %>% 
  map(~ lm(.x ~ gender, data = bfi_mean))
```

The result of this code is `mods_gender`, a *list* of models, one for each Big 
Five trait. You can then use these pass these models to other functions, such as
`summary()`, `tidy()`, or `glance()`.

```{r}
mods_gender

map(mods_gender, summary)
```

In this way, `map()` lets us write code that is much shorter, easier to read, 
and less prone to copy-paste errors.

Let's dive into how `map()` works.


## Working with `map()`


`map()` takes a data frame, list, or vector as as input and applies the requested 
function to each column of the data frame or element of the list/vector.

This impage illustrates what `map` does: 

<img src="https://d33wubrfki0l68.cloudfront.net/12f6af8404d9723dff9cc665028a35f07759299d/d0d9a/diagrams/functionals/map-list.png" width=500>

Source: [Advanced R](https://adv-r.hadley.nz/index.html) by Hadley Wickham.

*Note*: In this image, the input is a *list* and the function `f()` is applied
to each element of the list. When the input is a *data frame*, the function `f()`
is applied to each *column* of the data frame.

Its arguments look like this:

`map(.x, .f, ...)`

For example, this code takes each column of the `cars` data set and applies the
`mean` function:

```{r}
map(cars, mean)
```

You can see a similar example above where I applied the `summary()` function to
each element of the `mods_gender` list.

Often, you will want to specify additional arguments for the function you are
mapping. The easiest way to do that is using `~`. With `~`, you can write out
a whole function call, similar to `lm(A ~ gender, data = bfi_mean)`. For example:

```{r}
map(cars, ~ mean(.x, trim = .1))
```

When you use `~`, in the function call, `.x` is a **placeholder** that means
"whatever the current variable from the data frame is". So, the above code means
"Take each column of cars and pass to to the `mean()` function with the argument
`trim = .1`."

Accordingly, the gender modeling code above:

```{r}
mods_gender <- 
  bfi_mean %>% 
  select(A:O) %>% 
  map(~ lm(.x ~ gender, data = bfi_mean))
mods_gender
```

means "Take `bfi_mean` and select columns `A` through `O`. For each of these
columns, predict the column using the `lm()` function with `gender` from the
`bfi_mean` data frame as a predictor."

### More details on `map()`

`map()` is the basic mapping function in the `purrr` package. It always returns
a **list**. This is useful for *a lot* of purposes, but there are additional 
`map_*()` functions that also return other outputs. 

For example, let's say that I wanted to return just the `r.squared` value for each
of the gender–Big Five trait models. I could use the `map_dbl()` function, which
returns a numeric vetor instead of a list:

```{r}
rsq_gender <- 
  bfi_mean %>% 
    select(A:O) %>% 
    map_dbl(~ lm(.x ~ gender, data = bfi_mean) %>% 
                summary() %>% .$r.squared)
rsq_gender
```

`map_dfc()` combines all of the results into the *columns* of a new data frame. 
`map_dfr()` combines all of the results into the *rows* of a new data frame. For
example, to extract the coefficients for all of the models:

```{r}
coef_gender <- 
  bfi_mean %>% 
    select(A:O) %>% 
    map_dfr(~ lm(.x ~ gender, data = bfi_mean) %>% 
                coef(),
            .id = "Trait")
coef_gender
```

The `.id` argument gives the name of the variable where the model each row of 
results came from.

Similarly, to get the whole coefficient results tables for each model:

```{r}
coef_summary_gender <- 
  bfi_mean %>% 
    select(A:O) %>% 
    map_dfr(~ lm(.x ~ gender, data = bfi_mean) %>% 
                tidy(conf.int = TRUE),
            .id = "Trait")
coef_summary_gender
```

For more information on different types of `map_*()` functions, see the `map()`
help file: `?map()`

You can also map across 2 data frames/lists at once using the `map2_*()` functions
and across as many lists as you want at once using the `pmap_*()` functions, but
we aren't going to get into those in detail today. Check out Chapter 21 of 
[R for Data Science](https://r4ds.had.co.nz/iteration.html) for some examples
and exercises if you are interested. 


## Writing your own functions

`map()` is a powerful tool for performing the same analysis repeatedly, but it's
possible for the function code you use inside `map()` to get real messy real fast.
For example, look at this example from above:

```
coef_summary_gender <- 
  bfi_mean %>% 
    select(A:O) %>% 
    map_dfr(~ lm(.x ~ gender, data = bfi_mean) %>% 
                tidy(conf.int = TRUE),
            .id = "Trait")
coef_summary_gender
```

The function inside `map_dfr()` is a pipe:
`~ lm(.x ~ gender, data = bfi_mean) %>% tidy(conf.int = TRUE)`

Especially if a pipe like this gets very long, that can be hard to read.

When you write a function with `~`, that's called an "anonymous function"—you
are making up that function on the spot and it isn't being saved for future use.
If your function is complex, it's better to *write and save your function first*, 
then *call your saved function object inside `map()`*.

For example, we can rewrite the above as:

```{r}
get_coef_summary_gender <- function(.x, data) {
  mod <- lm(.x ~ gender, data = bfi_mean)
  return(tidy(mod, conf.int = TRUE))
}

coef_summary_gender <- 
  bfi_mean %>% 
    select(A:O) %>% 
    map_dfr(get_coef_summary_gender, .id = "Trait")
coef_summary_gender
```

There are two major advanges of writing and saving a function, rather than
using an anonymous function:
  1. Your code is often more readable.
  2. You can reuse the function multiple times throughout your analyses.
  3. Its easier to fix bugs or typos you might make because your code is only
     in one spot.
  
To write a function in R, use the following template:

```
function_name <- function(arguments) {
  # function code
}
```

```
function_name <- function(arg1, arg2, arg3 = FALSE) {
  # function steps go here
  return(result)
}
```

A function as three parts:

  1. Its **name**: how do we call it?
  2. Its **arguments**: what are its inputs?
  3. Its **body**: what does it do with the inputs?

First, pick a name for your function. Like all of your object names, make your
function names clear and unambiguous. `my_awesome_function()` doesn't tell
us anything about what the function does. Usually, I prefer to make my function
names *verbs* to make it clear that they *do* something.

Then, tell R that you are making a function by assigning `function() {}` to your
function name:

```
function_name <- function() {}
```

Your arguments go *inside the parentheses* after `function`. Follow the same basic
guidelines for naming arguments as you do for naming any object in R. Give your 
arguments understandable names and be consistent in your naming scheme.

To give an argument a default value, specify the default with `=`. For example,
this function will take a vector of numbers and draw a histogram of them, 
with the default colorbeing blue:

```{r}
plot_histogram <- function(data, color = "blue") {
  tbl <- tibble(data = data)
  ggplot(tbl, aes(x = data)) +
  geom_histogram(color = color, fill = color)
}
```

If the user of this function doesn't specify a color, the line and fill colors
will be blue:

```{r}
simulated_data <- rnorm(100)
plot_histogram(simulated_data)
```

If the user of the function _does_ specify a color, the line and fill colors will
be the specified color:

```{r}
plot_histogram(simulated_data, color = "black")
```

For more guidance on writing R functions, including advanced features and testing,
see Chapter 19 of [R for Data Science](https://r4ds.had.co.nz/functions.html).
For a great video overview of good functional design, see 
[this keynote](https://www.youtube.com/watch?v=Qne86lxjgtg).


## Activities

### Activity 1: `bfi` Models Part 2: Map Attack!

Use the `psych::bfi` dataset again. Compute mean scores for each of the Big Five 
scales for each person. Then, perform the same analyses as last week. But this
time, rather than writing out the code for each model and variable separately,
use `map()` or related functions to do so in a more functional programatic way.
Fit linear models for each Big Five trait, and present your results using both 
tables of results and figures.

1. Do men and women differ on the Big Five traits? How big are the differences?
2. Do the Big Five traits increase or decrease with Age? Is there a linear or 
   squared trend?
3. Do the Big Five traits differ across educational levels? Treat education as 
   a categorical variable.
4. How well do age and gender together predict the Big Five traits?
5. In your models in part (4), do the residuals appear to be normally distributed?
   Are they consistent across age ranges and gender groups?
   
### Activity 2: Individual `bfi` Reports

Select 10 people from the `bfi` dataset. Prepare an individualized report for
each person presenting the following information:

  1. Their mean score on each of the Big Five scales.
  2. The percentile of their mean score compared to the rest of the full sample
     of 2800 people (hint: use the `quantile()` function).
  3. A profile plot illustrating their level on each Big Five trait.
    - For examples, see [here](https://www.unzcloud.com/wp-content/uploads/2014/02/teddypersonality.png)
      or [here](https://projects.fivethirtyeight.com/personality-quiz/).
  4. Bonus: In addition, provide individualized _narrative_ descriptions of what
     each of their scores means (e.g., based on whether they are "high", "medium",
     or "low" on each trait). See the two examples linked above for ideas
     
To save yourself a bunch of tedious report writing, write a function and consider
using a `map_*()` function to automate the process. Take a look at Hadley's slides
and video for some examples! He has R code linked from his talk as well.


<!--chapter:end:s012_functional-programming.Rmd-->

# Data Simulation: Making Your Perfect Dataset

Today we will discuss various ways to simulate data. Generally, we either simulate data that follows some global distribution (e.g. normal), or some specific pattern of responses at the individual level (e.g. within-person normal).


## Worksheet TODO

You can find a worksheet template for today [here](https://raw.githubusercontent.com/USF-Psych-DataSci/Classroom/master/tutorials/s13_simulation-exercise.Rmd).

## Resources

### Coding Resources
- [An Intro to Simple Simulations](https://bookdown.org/rdpeng/rprogdatascience/simulation.html)
- [Simulating Different Distributions and Sequences](http://michaelminn.net/tutorials/r-simulating-data/)
- [Simulating Simple and Multivariate Data](https://it.unt.edu/simple-data-simulations)
- [https://aosmith.rbind.io/2018/08/29/getting-started-simulating-data/#discrete-counts-with-rpois](An Intro to Simulation Using Different Functions)
- [https://crumplab.github.io/programmingforpsych/simulating-and-analyzing-data-in-r.html#using-the-aov-function-for-within-or-between-subjects-designs](Simulating Data for T-test and ANOVA Designs, Includes Power Analysis)
- [A Brief Introduction to the SimStudy Package](https://cran.r-project.org/web/packages/simstudy/vignettes/simstudy.html)
https://www.youtube.com/watch?v=gxAaO2rsdIs

### Theoretical Resources
For an overview of Directed Acyclic Graphs see
[This Paper](https://journals.sagepub.com/doi/full/10.1177/2515245917745629). These graphs are useful for visualizing any complex relationships you may want to simulate.

For an overview of how simulations can aid decision making in instrument development see [This Paper](https://journals.sagepub.com/doi/full/10.1177/0734282917718062).

For an overview of simulations in clinical psychology see [This Paper](https://journals.sagepub.com/doi/full/10.1177/0734282917690302). Although I think this paper is also useful for those outside of clinical to read.

For an overview of simulations in organizational and management research see [This paper](https://journals.aom.org/doi/10.5465/amr.2007.26586485)

```{r}
library(tidyverse)
```
## Simple Simulations

The first, and most important thing, to remember about simulations is that the process is random. You will never reproduce the same results twice unless you set a seed for your simulation. The `set.seed()` function allows you to specify a 4 digit number. This will fix how random numbers are generated so that every time you run a script the same results will be produced. This is true across any machine you run the analyses on, so others can replicate your results.

### Distributions in R

Base R provides several functions that allow you to sample data from various distributions. For example, the rnorm statement allows you to randomly sample values from a normal distribution with a specified mean and standard deviation.

The code below simulates data with a mean of 4, standard deviation of 1, and a sample size of 200.
```{r}
#This makes it so that all simulations will produce the same numbers
set.seed(1337)

dataNormal <- data.frame("X" = rnorm(200, mean = 10, sd = 1))

ggplot(dataNormal, aes(x=X)) +
  geom_histogram(binwidth=0.5,color="black", fill="white")
```

We can extend this to simulate data from multiple different groups and compare the resulting distributions. This is essentially how you would conduct a power analysis.
- What if you don't want a normal distribution?
- There are functions for several different distributions [built into R](https://www.stat.umn.edu/geyer/old/5101/rlook.html)

#### Go to part 1 of the exercise, `rnorm()`, to practice simulating data from two distributions.

### Simulating Distributions Based on Specific Probabilities

While the functions included in R provide an easy way to simulate lots of different distributions, we may want to be more specific about exactly what our distribution looks like. We can use the `sample()` function to sample from a range of numbers and specify a specific probability that each one is sampled.

We can replicate a normal sampling distribution above using specific scale points. For example, let's say we want to simulate a normal distribution of scores for a 5 item likert scale.
```{r}
set.seed(1337)
                            #The numbers we will sample from
sampleDiscreteNormalDistribution <- data.frame("X" = sample(c(1, 2, 3, 4, 5), #i.e. our scale points
#200 = the number of cases. Replace = T means we can sample the same number multiple times. prob = the probability of each point being sampled. (e.g. 1 has a 6% chance, 2 has a 24% chance)
                      200, replace=T, prob = c(6, 24, 40, 24, 6))) 
ggplot(sampleDiscreteNormalDistribution, aes(x=X)) +
  geom_histogram(binwidth=0.5,color="black", fill="white")

```

How about a skewed distribution?
```{r}
set.seed(1337)

sampleSkewedDistribution <- data.frame("X" = sample(c(1, 2, 3, 4, 5), #scale points
                      200, replace=T, prob = c(4, 12, 20, 28, 38)))
ggplot(sampleSkewedDistribution, aes(x=X)) +
  geom_histogram(binwidth=0.5,color="black", fill="white")
```

If we wanted to, we could even match the normal distribution more precisely using the sample function. Here we use the sequence function to generate a range of numbers from -3.00 to 3.00, counting up by 0.01 for each new number. Thus, the start of the sequence would look like: -3.00, -2.99, -2.98, etc. We then use the same sequence with the dnorm function, which gives us the probability of a given number at a point along the normal curve. (Note: if you wanted you could change the mean and SD of the dnorm function to get different looking results).
```{r}
set.seed(1337)

sampleNormalDistribution <- data.frame("X" = sample(c(seq(-3.00, 3.00, by = 0.01)), #scale points
                      10000, replace=T, prob = c(dnorm(seq(-3.00, 3.00, by = 0.01), mean = 0, sd = 1))))
ggplot(sampleNormalDistribution, aes(x=X)) +
  geom_histogram(binwidth=0.5,color="black", fill="white")
```

#### Go to part 2 of the exercise, `sample()`, so practice using the sample function.

We can also use the `sample()` function to generate data for individual item responses. Below is a script that samples cases for 200 individuals responding to a 10 item measure. The numbers are sampled using the `sample()` function, and are filled by row into a matrix. This data follows a normal distribution globally. (Note: we now need to sample 2,000 cases because each 'person' has 10 responses. Thus, there are 200 * 10 total cases in our data).
```{r}
set.seed(1337)

sampleNormalItemDistribution <- sample(c(1, 2, 3, 4, 5), #scale points
                      2000, replace=T, prob = c(6, 24, 40, 24, 6)) 

matrixNormalItemDistribution <- matrix( 
  c(sampleNormalItemDistribution),         
  nrow=200,              
  ncol=10,              
  byrow = TRUE)
rm(sampleNormalItemDistribution)

dataNormalItemDistribution <- as.data.frame(matrixNormalItemDistribution)

dataNormalItemDistributionLong <- gather(dataNormalItemDistribution, key = "key", value = "value", V1:V10, factor_key = FALSE)

ggplot(dataNormalItemDistributionLong, aes(value)) +
  geom_histogram(binwidth=0.5,color="black", fill="white")

```

This is great. But, this data follows the same global pattern for all individuals. That is, the distribution for each person will be nearly identical. We know this isn't the case for actual data. Some people will respond with low values on the scale and not high values. We can model this behavior within individual's using loops.

### Using Loops to Simulate Individual Behavior

First we need to talk about what loops are. Basically, it's a way to run the same function on something for multiple iterations.

For example, the below for loop will run 10 times. For each time it runs it will save the value of 'i' to the ith row in the first column of the 'dataForLoop' matrix. i = the loop you are currently on. So the values of i will range from 1 to 10. It will start at 1 and count up once for every loop.
```{r}
#Create a matrix we will save our values to
dataForLoop <- matrix(nrow=10,ncol=1)
#The loop length goes in parentheses ()
for(i in 1:10)
#The function the loop executes goes in curly brackets {}
{
  #we use brackets [] to call a specific row and column in our dataset
  #we specify the row we want first, then the column
  #we can open our data frame or matrix and count (starting from 1) to get the number of a row or column we want. 
  #we can also just hover the name of that row or column to see its number.
  dataForLoop[i,1] <- i
}

print(dataForLoop)
  
```

We can extend this to have a loop within another loop. This allows us to select both rows and columns at the same time. Below we will iterate through i 10 times and use that to symbolize rows. We will also iterate through j 5 times, which symbolizes columns. For each cell in our matrix will we print the current value of i plus the current value of j.
```{r}
dataDoubleLoop <-matrix(nrow=10,ncol=5)

for(i in 1:10)
{
  for (j in 1:5)
  {
   dataDoubleLoop[i,j] <- i + j
  }
}

print(dataDoubleLoop)

```

##TODO
What's actually going on here? Let's look at the GIF below.
```{r 1simpson, fig.cap="Example of how two loops can be used to write random numbers to each value of a data frame."}
knitr::include_graphics("/Loop-GIF.gif")
```

What we can do now is extend this functionality to sample values that follow a normal distribution WITHIN every individual. Each starting value is randomly selected from a uniform distribution, so the distribution of all scores should look roughly uniform, but the distribution of scores within every person should be normal.
```{r}
set.seed(1337)
#Creates our matrix
normalWPSample <-matrix(nrow=200,ncol=10)
#for loop to iterate over individuals
for(i in 1:200){
  #for loop to iterate over items within individuals
  for(j in 1:10){
    #first response randomly generated and set as the center point for all remaining responses
    if(j==1){ #remember j is our row, so this will sample a new center point whenever we are  on item 1 for a person.
      wInMean<-sample(1:5,1) #sets our mean. Could save in our matrix if we want to know it
          #We will sample a value that is the center point, or deviates from it by 1 or 2
      normalWPSample[i,j] <- sample(c(wInMean - 2, wInMean - 1, wInMean, 
                                      wInMean + 1, wInMean + 2), 
                                    #probabilities to sample these values
                                    1, replace=T, prob = c(6, 24, 40, 24, 6)) 
    }
    #all responses past first response. This function is the same as above, 
    #but we don't need to sample a value for the center point because we already have it
    else{
      normalWPSample[i,j] <- sample(c(wInMean - 2, wInMean - 1, wInMean, 
                                      wInMean + 1, wInMean + 2), 
                                    1, replace=T, prob = c(6, 24, 40, 24, 6)) 
    }
  }
}
dataNormalWPSample <- as.data.frame(normalWPSample)
dataNormalWPSampleLong <- gather(dataNormalWPSample, key = "key", value = "value", V1:V10, factor_key = FALSE)

ggplot(dataNormalWPSampleLong, aes(value)) +
  geom_histogram(binwidth=0.5,color="black", fill="white")
```

What's the issue here?

We're able to sample values from outside our scale values of 1-5 for people with low or high means. There are a few ways to fix this, but the simplest one is to just replace all values outside our scale with the closet scale point.

```{r}
#If there is a number greater than 5 replace it with 5
dataNormalWPSampleLong[dataNormalWPSampleLong>5]<-5
#If there is a number less than 1 replace it with 1
dataNormalWPSampleLong[dataNormalWPSampleLong<1]<-1

ggplot(dataNormalWPSampleLong, aes(value)) +
  geom_histogram(binwidth=0.5,color="black", fill="white")
```

We could also calculate a sum or average score for our scale.
```{r}

for (i in 1:200)
{
#We can use a loop to grab the mean for all 10 items and write it to a new column
dataNormalWPSample[i,11] <- mean(as.numeric(dataNormalWPSample[i,1:10]))
}

#we can use this to rename our 11th variable to something more meaningful
names(dataNormalWPSample)[11] <- "average"

ggplot(dataNormalWPSample, aes(average)) +
  geom_histogram(binwidth=0.1,color="black", fill="white")

psych::describe(dataNormalWPSample$average)
```

#### Go to part 3 of the exercise, `for()`.

### Simulating Actual Studies

#### Modeling training program effectiveness
Suppose there is an organizational researcher who wants to know the effect of two training programs on job performance. This researcher will test how these two programs (Training 1, and Training 2) perform against a control group. This researcher also believes that the trait conscientiousness will impact how effective these training methods are. Finally, the researcher has training administered by 3 different trainers and thinks that the specific relationship someone has with that trainer will impact the effectiveness of the training. Each person will receive their training from ONE trainer, but will receive EVERY training program.

We will first setup a data frame with all of the variables we will need for the study. The training group someone is in, their performance scores after training is received (3 scores for each person), their personal ID (1-300), the trainer they received training from, and whether they are high or low on conscientiousness.
```{r}
#Generates our data
dataTrainingMain <- data.frame(training = rep(c("Control", "Training 1", "Training 2")[1:3]),
                       perfScores = 0,
                       person = rep(1:300, each = 3),
                       Cgroup = rep(c("Low Conscientious","High Conscientious"), times=1, each = 450))

head(dataTrainingMain, n=5)
tail(dataTrainingMain, n=5)
```

We will first generate performance scores for each person based on the training they have received. All groups will have an SD of 1. The control group has a mean of 5, the group that receives the first training program has a mean 1 SD higher of 6, and the group that receives the second training program has a mean of 1SD higher than that of 7.
```{r}
set.seed(1337)
#training effects
for(i in 1:900){
    if(dataTrainingMain[i,1] == "Control"){
      dataTrainingMain[i,2] <- dataTrainingMain[i,2] + rnorm(1,mean = 5, sd=1)
    }
    else if(dataTrainingMain[i,1] == "Training 1"){
      dataTrainingMain[i,2] <- dataTrainingMain[i,2] + rnorm(1,mean = 6, sd=1)
    }
    else if(dataTrainingMain[i,1] == "Training 2"){
      dataTrainingMain[i,2] <- dataTrainingMain[i,2] + rnorm(1,mean = 7, sd=1)
    }
}
```

Let's take a look at our results.
```{r}
#Color blind friendly pallete
cbPalette <- c("#999999", "#0072B2", "#D55E00", "#F0E442", "#000000", "#CC79A7", "#FF00FF")

ggplot(dataTrainingMain, aes(x=training, y=perfScores)) + 
  geom_boxplot(aes(group=training, color=training)) +
  scale_fill_manual(values=cbPalette) + scale_colour_manual(values=cbPalette)

```

Next we want to make training method 1 less effective for people with high conscientious and more effective for people with low conscientious. We also want to do the opposite for people who are low on conscientious.
```{r}
#conscientiousness effects
for(i in 1:900){
  #We check if someone recieved training method 1 and if they belong to the high conscientiousness group. Column 1 contains their training value, and column 5 contains their conscientiousness group. If both are true we will subtract 1 from their score, which is contained in column 2.
  if(dataTrainingMain[i,1] == "Training 1" && dataTrainingMain[i,4] == "High Conscientious"){
    dataTrainingMain[i,2] <- dataTrainingMain[i,2] - 1
  }
  #Now we check if someone received training method 2 and if they belong to the high conscientiousness group. If both are true we will add 1 to their score.
  #NOTE: We need to call their current performance value and then add 1 to it. If we simply did <- +1, or <- -1 instead that would set their performance score to 1 or -1. 
  else if(dataTrainingMain[i,1] == "Training 2" && dataTrainingMain[i,4] == "High Conscientious"){
    dataTrainingMain[i,2] <- dataTrainingMain[i,2] + 1
  }
  else if(dataTrainingMain[i,1] == "Training 1" && dataTrainingMain[i,4] == "Low Conscientious"){
    dataTrainingMain[i,2] <- dataTrainingMain[i,2] + 1
  }
  else if(dataTrainingMain[i,1] == "Training 2" && dataTrainingMain[i,4] == "Low Conscientious"){
    dataTrainingMain[i,2] <- dataTrainingMain[i,2] - 1
  }
}
```

Let's see what things look like with these added effects.
```{r}
ggplot(dataTrainingMain, aes(x=training, y=perfScores)) + 
  geom_boxplot(aes(group=training, color=training)) +
  scale_fill_manual(values=cbPalette) + scale_colour_manual(values=cbPalette)

ggplot(dataTrainingMain, aes(x=training, y=perfScores)) + 
  stat_summary(aes(group=Cgroup, color=Cgroup), fun.y=mean, geom = "line") +
  labs(x = "Training", y = "Performance", color = "Conscientiousness Level") +
  scale_fill_manual(values=cbPalette) + scale_colour_manual(values=cbPalette)
```

We can also take a look at these results using ANOVA. We see that there is a significant main effect of training, and that training also interacts with someones conscientiousness.
```{r}
fit1 <- lm(perfScores ~ training*Cgroup, data=dataTrainingMain)
summary(fit1)

```

#### Modeling an educational intervention

We are working for a small high school that has lower than average student performance. The school wants to develop an intervention that will increase the number of students who get a C+ or higher. Their goal is to find an intervention that triples the amount of kids who receive a C+ or higher. How big of an effect size does this intervention need to have?

We are told that the current average performance of students is a C-, 70%. We will assume that grades follow a normal distribution with a SD that is approximately half a letter grade. Thus, our initial sample will have a mean of 70 and SD of 4.

Given that we know the distribution of our data we can calculate the percentage of kids who are currently receiving a C+ or better. Since our SD is 4 all we have to do is figure out what percent of cases fall above 1 SD of a normal curve.
```{r}
#First we simulate some data following our paramters
dataQuantileTest <- rnorm(50000, mean = 70, sd = 4)

#Then we find the number of values greater than or equal to 75 and devide it by our total number of values
sum(dataQuantileTest >= 75) / length(dataQuantileTest)
```

About 10% of our students are currently getting a C+ or better, or 20 * .10 = 2 students. So, after our intervention we should have at least 2 * 3 = 6 students who receive a C+ or better.

So, our first question is, "How big of an effect does our training need to have in order for 6 more students to get a C+ or better?" Let's simulate some data to see.
```{r}
set.seed(1337)
#Creates an empty data frame and renames the columns to be simulation1-20
matrixIET <- matrix(nrow = 20, ncol = 20)
dataInterventionEffectTest <- as.data.frame(matrixIET)
names(dataInterventionEffectTest) <- paste("Simulation", seq(1:20), sep = "")

#simulates our data
for (i in 1:20)
{
#We can use a loop to generate means that are 1 point higher every iteration and write it to the ith COLUMN.
#Notice that we are using i to represent columns here instead of rows because we want each sample to belong to a seperate column.
#Leaving the rows spot blank tells r we want all rows to be filled. In this case we have 20 cases, so they will each fill a seperate row.
dataInterventionEffectTest[,i] <- rnorm(20, mean = 70 + i, sd = 4)
}

```

We will then see how many scores are a C or above.
```{r}
set.seed(1337)
matrixAboveC <- matrix(nrow = 1, ncol = 20)

for (i in 1:20)
{
#We can then see how many people get grades of 75% or above.
matrixAboveC[,i] <- sum(dataInterventionEffectTest[,i] >= 75)
}

print(matrixAboveC)
```

Most of the later cases are quite overpowered for what we want. It would be great if we could get all 20 students to have a C or better, but our goal is to find the smallest intervention that will have at least 6 students with a C or better. The first 7 cases look promising. Let's see what their distributions look like.
```{r}
dataInterventionEffectTestLong <- gather(dataInterventionEffectTest, key = "key", value = "value", Simulation1:Simulation7, factor_key = FALSE)

ggplot(data=dataInterventionEffectTestLong, aes(x=value, group=key, fill=key)) +
    geom_density(alpha=.2) + 
    scale_fill_manual(values=cbPalette) + scale_colour_manual(values=cbPalette) + 
    geom_vline(xintercept = 75, linetype = "dashed", colour = "black", size= 1)
```

Based on the above results it doesn't seem clear which intervention is the best. We also have to consider that these results might be due to our small sample size of 20. What would happen if we did a hundred simulations? How many times would at least 6 of our students get a C or higher? Let's simulate 100 cases with a mean of 74 and sd of 4.
```{r}
matrixBlank <- matrix(nrow = 20, ncol = 100)
dataMean74Effect <- as.data.frame(matrixBlank)
names(dataMean74Effect) <- paste("Simulation", seq(1:100), sep = "")

#simulates our data
for (i in 1:100)
  {
  #Notice we add i to our seed so it is different every loop
  set.seed(1000 + i)
  dataMean74Effect[,i] <- rnorm(20, mean = 74, sd = 4)
  }
```

Now we'll write a loop to check if the sum of cases greater than 75 is greater than 6.
```{r}
matrixPercentAboveC <- matrix(nrow = 100, ncol = 1)

for (i in 1:100)
{
  if(6 <= sum(dataMean74Effect[,i] >= 75))
  {
     matrixPercentAboveC[i,1] <- 1
  }
  else{
    matrixPercentAboveC[i,1] <- 0
  }
}
sum(matrixPercentAboveC)
```

So, only 88% of the time do our samples contain at least 6 kids who received a C or better. Based on a traditional alpha value of .05 we would expect that ~95 cases will have 6 or more kids who get a C or better if our intervention is truly effective. Since this only happened 88% of the time let's see if simulating data with a mean of 75 produces the desired results.

```{r}
matrixBlank <- matrix(nrow = 20, ncol = 100)
dataMean75Effect <- as.data.frame(matrixBlank)
names(dataMean74Effect) <- paste("Simulation", seq(1:100), sep = "")

#simulates our data
for (i in 1:100)
  {
  set.seed(1000 + i)
  dataMean75Effect[,i] <- rnorm(20, mean = 75, sd = 4)
}

#Calculates number of cases that are 75 or greater.
matrixPercentAboveC <- matrix(nrow = 100, ncol = 1)

for (i in 1:100)
{
  if(6 <= sum(dataMean75Effect[,i] >= 75))
  {
     matrixPercentAboveC[i,1] <- 1
  }
  else{
    matrixPercentAboveC[i,1] <- 0
  }
}
sum(matrixPercentAboveC)
```

Now 100% of our simulated data sets have at least 6 kids with a C or better. So, we would conclude that we need an intervention that raises our mean by at least 5 points. If we wanted to put this in terms of effect size we would say we need an intervention that has at least a d of 1.25.

We can also alter the paramters of our simulation to test different thresholds of scores. For example, what if we want an intervention that will ensure no students are failing the class, where failing means a D- or below.
```{r}
matrixBlank <- matrix(nrow = 20, ncol = 100)
dataMean74Effect <- as.data.frame(matrixBlank)
names(dataMean77Effect) <- paste("Simulation", seq(1:100), sep = "")

#simulates our data
for (i in 1:100)
  {
  set.seed(1000 + i)
  dataMean74Effect[,i] <- rnorm(20, mean = 74, sd = 4)
}

#Calculates number of cases that are 62 or less.
matrixPercentBelowD <- matrix(nrow = 100, ncol = 1)

for (i in 1:100)
{
  if(1 <= sum(dataMean74Effect[,i] <= 63))
  {
     matrixPercentBelowD[i,1] <- 1
  }
  else{
    matrixPercentBelowD[i,1] <- 0
  }
}
sum(matrixPercentBelowD)
```

It turns out that an intervention of d = 1, or a mean shift of 4 points, produces an outcome where 95% of the time all students are above a D-. Note that in this case we want an outcome where are sum is 5 or lower since we are coding times where at least one student got a D- or lower as 1.

Last, if we wanted to run a power analysis of our first analysis we could. The below power analysis tests whether the simulated rnorm samples with a mean of 75 and sd of 4 differs from 70. [Chapter 5 of this book from the Crump Lab](https://crumplab.github.io/programmingforpsych/simulating-and-analyzing-data-in-r.html#power-analysis-and-sample-size-planning-by-monte-carlo-simulation) contains examples of lots of other power analyses for more complex designs.
```{r}
sims_to_run <- 1000
#Vector we use to save a p-value from each run
save_p <- vector(length=sims_to_run)

for(i in 1:sims_to_run){
set.seed(1000 + i)
sim_data <- rnorm(20,mean = 75,sd = 4)
t.out <-t.test(sim_data,mu=70)
#Saves the p-value from our t-test output to our save_p vector
save_p[i] <- t.out$p.value
}
#Converts the vector to a data frame for ggplot graphing
dataP <- as.data.frame(save_p)

ggplot(dataP, aes(x=save_p)) +
  geom_histogram(color="black", fill="white")
```
We can see all our p-values are well below the .05 point.

#### Go to part 4 of the exercise, creating your own study.

### Final Thoughts

We have gone over some basics of how to simulate data so far, but there's a lot we have not discussed. One of the main differences between this tutorial and the actual use of simulations is that we did a lot more leg work than was needed and wrote code that was longer than it needed to be. For example, in practice we would use the pwr package to run a power analysis instead of writing our own script. We would probably use the `replicate()` function in place of some of our `for()` loops. We would probably use the simstudy package to help us more easily run our simulations, or make use of other existing packages. We may also write our own functions to make it faster to run certain common processes. We would also only need to use the `set.seed()` function once at the start of our script instead of running it for every single simulation.

The main reason we didn't use these easier methods here is to give you a better sense of how all these processes work. You might use faster methods to simulate data in the future, but should now have a rough idea of how those processes work and could hopefully replicate those processes yourself if you needed to. Everything we've covered should also extend to more complex designs than the simple t-test's or ANOVA's we've covered here.

Another common simulation method we did not cover here is simulations over time. For example, simulations are currently being used to [model the spread of CoronaVirus](https://www.youtube.com/watch?v=gxAaO2rsdIs).

Something you'll hear the guy in the video say a lot though is that simulations are 'toy models'. This is an idea that has been around for a while where people essentially say that simulations aren't actually useful for anything other than playing around with hypothetical examples that will never happen. From this viewpoint, simulations are about as useful for making research decisions as a Lego model would be for making decisions about designing a skyscraper. Basically, they can be fun toys, but don't have any place in the real world. I think this viewpoint is a big reason why simulations are not more common in research today. I also think this viewpoint is, bluntly, kind of ignorant. Simulations can be a great way to move beyond a purely conceptual model and determine what is and is not possible. Recently, power analysis has become a very popular way to use data simulation as part of the research process. It saves people lots of time and effort by determining what our probability is of observing an effect in our experiment if that effect actually exists. I think we can also use other types of data simulation as part of the research process as well. While it is important to keep in mind that simulations do not reflect real world data and cannot stand in for that data, that doesn't mean they cannot help us make decisions and save us resources. I think [this paper](https://journals.aom.org/doi/10.5465/amr.2007.26586485) provides a good introduction to thinking about how and when to incorporate data simulation into your research and where it sits in the research process in general.

<!--chapter:end:s013_simulation.Rmd-->

# Reproducibility and Coding Best Practices

Today, we are going to return to the topic of *reproducibility* that we have 
talked about all semester and discuss several important best practices you can
follow to help ensure that:

  1. Your analyses are _reproducible_: They always return the same results, in
     the same way, every time.
  2. You can easily and painlessly update your analyses and output (e.g., if you
     get new data or fix a bug or error).
     - Jenny Bryan: *"If the thought of re-running your analysis makes you ill, 
       you're not doing it right."*
  3. People can easily read your code, understand what it is doing, and follow
     your thinking as a data analyst.
  4. Future-you can easily easily read your code, understand what it is doing, 
     and follow past-you's thinking as a data analyst.
  5. Someone else (e.g., a future RA) could easily take over and keep working on
     the project or analysis.
     
## Resources

  - [Wilson, G., et al. (2014). Best practices for scientific computing. *PLoS Biology*](https://doi.org/10.1371/journal.pbio.1001745)
    - A great guide for both coding and project management practices to help make
      your analyses more reproducible.
  - The [tidyverse style guide](http://style.tidyverse.org/) for R. 
    

## Reproducibility

The principle of reproducibility has been central to the way we've approached 
programming in this course. Your analyses should be be able to run wihtout 
any manual input from you. Reproducibility has two major components.

### 1. How easily someone can reproduce output

Example: Generating a report with a plot, a table, and some results in text.

  - Worst case scenario: No source code available; have to redo everything from scratch.
  - Common scenario: Source is available, but it's incomplete (e.g.,)
  - Better scenario: Full source is available, but output is arranged in the report manually.
  - Best case scenario: Complete report output is regenerated with the click of a button.

Key concepts to live by:

  - Source is real. Output is transient. 
    - Source >> Output
    - Think of your source files (your .R files, .Rmd files, etc.) as "real".
      Think about output files (figures, CSV tables, HTML, PDF, or Word documents,
      etc.) as "transient"—things that are temporary and likely to be overwritten
      by the next run of your code.
    - Don't make changes in your output files that matter. Make those changes
      in the source.
  - When you are building a script…
    - (i.e., when you are writing code, then immediately running it and looking
      at the results)
    - This is called "working interactively"
    - Frequently kill your R session and re-run your script from the top. It should
      still work.
      - In RStudio, Session → Restart R
    - Don't save your workspace or history when exiting from R
      - Disable this in the RStudio settings. 

**What versions of packages are you using?**

Software packages are often updated, and their inputs or outputs might change
across versions. It's important to tell your readers (and future-you) what 
versions of each package you are using. The best way to do this is to include
one of these functions at the end of your report or in an appendix:

```
sessionInfo()
devtools::session_info()
```

The `devtools` version has some additional useful info and is somewhat more nicely
formatted.


### 2. How easily someone can reproduce your _frame of mind_ or _thought proccess_

_Why in tarnation did they do **that**?!?_

Beyond just being able to reproduce the same numbers, tables, figures, etc., 
reproducibility is also concerned with you others being able to reproduce the
_thinking_ that lead to code, analyses, and results. 

What do the data and variables mean? Why did you choose these analyses? Why did 
you write your code this way versus another? Being able to answer these questions
is critical to being able to trust the results. By clicking the "Knit" button,
you might be able to reproduce the numbers in the table, but are those numbers
_right_? Is there a bug in the code, does a function not work the way that the
author thought, etc.?

The way that you make sure that readers or future-you can figure out is by providing
extensive and clear _documentation_, both _within_ files (comments) and 
_between_ files (codebooks, READMEs).

Think about your documentation at three levels:

  1. The big picture
     - What is this script or function doing overall?
     - What is the broad organization of your files and folders?
  2. The walkthrough
     - For each section of code, what is it broadly doing?
     - Think of this like the headings or summaries for the block.
     - Don't get too detailed or overdescribe here. "Compute predictor
       composites" is fine.
  3. The nitty gritty
     - What exactly is a specific line of code doing?
     - Reserve this level of detail for when something is unusual or would look
       odd to someone else/future-you. 
     - If your code needs a _lot_ of comments to explain, consider re-writing
       it to be clearer.
       
For resources on documenting code, see the [tidyverse style guide](https://style.tidyverse.org/).

For resources on documenting data files, check out the [codebook](https://rubenarslan.github.io/codebook/)
package. It can automatically produce codebooks for a data file, saving you a 
lot of time. Its vignettes (e.g., for 
[SPSS](https://rubenarslan.github.io/codebook/articles/codebook_sav.html),
[formr](https://rubenarslan.github.io/codebook/articles/codebook.html), and  [Qualtrics](https://rubenarslan.github.io/codebook/articles/codebook_qualtrics.html))
are a great resource for thinking about the type of information that a codebook
should contain. 


## Good Coding Practices

### Naming

Your variables, functions, and other objects should have clear, concise, and 
unambiguous names.

1. Pick a style for naming variables and stick with it consistently:
   - Some people use camelCase, snake_case, or period.case
   - Don't use period.case! (it can mess with some R functionality)
2. Follow consistent rules for different types of objects
   - Functions should be verbs (based on the one main thing that it does)
   - Objects (data, models, results, figures, etc.) should be nouns
   - Functions that return functions should be adverbs
3. Always use descriptive names
   - Not `foo`, `dat2`, `model_3`, etc.
4. Don't over-create
   - The more objects there are in your global environment, the more confusing 
     it will be to try to keep track of them
   - Especially if you don't follow consistent rules for when you make objects
     and how you name them
   - Use the pipe `%>%` to avoid creating unnecessary intermediate objects 
5. Don't under-create
   - If you will re-use an object more than a few times, make it once and save it
   - Avoid "magic numbers"
     - Numbers in your code that are present without any explanation as to what
       they are or where they came from
     - Bad: 
        ```
        x <- rnorm(100)
        y <- x + rnorm(100)
        ```
     - Good:
        ```
        n <- 100
        x <- rnorm(n)
        y <- x + rnorm(n)
        ```
6. Disambiguate from left-to-right, not right-to-left
   - This makes it easier to figure out what an object is
   - It also makes it easier to complete typing a name by typing `Tab` on your keyboard
   - Bad: `canada_gdp` and `china_gdp`. 
   - Good: `gdp_canada` and `gdp_china`.


### Documenting code 

Write your code for humans. Someone reading your code should be able to figure out
what it does. This includes both writing explanatory comments and also writing
the code itself in a way that is clear about what it does.

Code that is clear and speaks for itself is called "self-documenting code". Some
ideas that make code more self-documenting include using clear variable names
and doing things one step at a time, rather than combining multiple operations
into one line. Using the tidyverse functions (e.g., `dplyr`) can also help new
R users to figure out what your code is doing in my experience.

    - Base R: 
        ```
        mtcars[mtcars$cyl < 8, c("cyl", "mpg")]
        ```
    - tidyverse: 
        ```
        mtcars %>% 
            filter(cyl < 8) %>% 
            select(cyl, mpg)
        ```

Think carefully about how detailed your comments need to be! Overly detailed
comments could be more confusing than no comments at all. They also can be hard
to keep accurate as you revise your code or if you move things around. Focus on
the high-level decisions about the programming/analysis.

  - Bad: 
    ```
    # Lag the negative affect variable twice.
    ```
  - Good: 
    ```
    # Create lagged predictors for modeling.
    ```
Also think carefully about how you need to explain your code to someone who isn't
already familiar with it—don't use say _what_ your code does, but also _why_ it 
is written the way it is. 

Don't use comments to describe what your code is doing on a low-level.

  - Bad: 
    ```
    # make data frame of cylinders less than 8, with variables 'cyl' and 'mpg'
    ```
  - Maybe okay: No comment (if the code is clear what it is doing)
    - Using the tidyverse will often be helpful here.
  - Maybe okay:
    ```
    # Select relevant cases and variables for analyses
    ```
  
  
### The DRY principle

**DRY = Don't repeat yourself**

Avoid repeating or copy-pasting the same lines of code over and over, then making
minor changes. This is prone to typos, errors, and breakage down the line. 

If you are going to do something more than once, then use functions or write 
functions to do the repetition for you.

Example: Running an analysis by subgroup

  - Bad:
    ```
    mod_wt_4cyl <- 
      mtcars %>% 
      filter(cyl == 4) %>% 
      lm(mpg ~ wt, data = .)
    mod_wt_6cyl <- 
      mtcars %>% 
      filter(cyl == 6) %>% 
      lm(mpg ~ wt, data = .)
    mod_wt_8cyl <- 
      mtcars %>% 
      filter(cyl == 6) %>% 
      lm(mpg ~ wt, data = .)
    ```
  - Good:
    ```
    # Requires dplyr >0.8.99 or >1.0.0
    # Install from GitHub if you don't have this version:
    #   devtools::install_github("tidyverse/dplyr")
    mods_wt <-
      mtcars %>% 
      nest_by(cyl) %>% 
      summarize(mods_wt = list(lm(mpg ~ wt, data = data)))
    ```
  - Good:
    ```
    mtcars %>%
      split(.$cyl) %>%
      map(~ lm(mpg ~ wt, data = .x))
    ```
  - Good:
    ```
    model_cyl_subgroup <- function(data, cyl, formula) {
      data %>% 
        select(cyl == cyl) %>% 
        lm(formula, data = .)
    }
    mods_wt <- map(c(4, 6, 8), 
                   ~ model_cyl_subgroup(data = mtcars, 
                                        cyl = .x, 
                                        formula = mpg ~ wt)
    ```
    
Example: Running an analysis for each predictor

  - Bad:
    ```
    # Make some data
    dat_big_five <- psych::bfi %>% 
      select(age, O = O1, C = C1, E = E1, A = A1, N = N1) %>% 
      slice(sample(1:nrow(.), 150)) %>% 
      na.omit()
    
    mod_age_O <- lm(age ~ O, data = dat_big_five)
    mod_age_C <- lm(age ~ C, data = dat_big_five)
    mod_age_E <- lm(age ~ E, data = dat_big_five)
    mod_age_A <- lm(age ~ A, data = dat_big_five)
    mod_age_N <- lm(age ~ N, data = dat_big_five)
    ```
  - Good:
    ```
    vars_big_five <- c("O", "C", "E", "A", "N")
    mods_age <-
      dat_big_five %>% 
      summarize(across(all_of(vars_big_five), 
                ~ list(lm(age ~ .x)),
                .names = "mod_age_{col}"))
    ```
  - Good:
    ```
    model_age <- function(data, predictor) {
      data %>% 
        select(age, predictor) %>% 
        lm(age ~ . , data = .)
    }
    mods_age <- map(vars_big_five, 
                    ~ model_age(data = dat_big_five, 
                                predictor = .x))
    ```
    
These are obviously very simple toy functions, but imagine a case where you
have much more complex models or series of analyses that you will need to repeat
over and over.


### Write Tests

Make sure that your code produces the correct results. This is best done by writing
a "unit test"—give a function/bit of code some input with a known expected output
and make sure they are the same. You should write automatic tests for your code
to be sure it produces the write result. 
  - Check out the `testthat` package.
  

### Code Styling

Format your code so that it is easy to read. For example:
  - Include spaces between object names
  - Line up parallel lines of code (see the arguments in `map()` above
  - Use indentation to guide the reader through how to read your code
  
Following a style guide, such as the [tidyverse style guide](http://style.tidyverse.org/)
is a good practice for making your code readable.


## Activity

Be sure that your final project follows these coding practice guidelines!

<!--chapter:end:s014_coding-practices.Rmd-->

# Preparing Documents in RMarkdown

We've been using RMarkdown all semester to organize and document our analyses.
This module will focus on steps we can take to go from using RMarkdown just
as a way to run analsyes to using it to prepare polished, ready-to-submit
reports and documents.

## General formatting guidelines

There are a few things you should keep in mind when writing reports regardless
of your output format.

### Let the computer handle the formatting

Let Markdown/pandoc handle the document formatting for you. This lets you focus 
on the content as you write and will help to ensure that everything looks nice 
regardless of the format you output to. For example:

  1. Use `#`, `##`, `###` to insert headings, rather than manually entering 
     things in bold, italics, centered, etc.
  2. Use pandoc citations (e.g., `[@Wiernik2020]`) instead of manually formatting
     citations and references
     - More on this next week

### Don't manually type statistical results

This is prone to errors and is a lot of work when you need to update analyses. 
Instead let R do this for you by using output from R code chunks and inline R
code:

````markdown
`r '\x60\x60\x60{r}\nx = 5  # radius of a circle\n\x60\x60\x60'`

For a circle with the radius `r '\x60r x\x60'`, its area is `r '\x60r pi * x^2\x60'`.
````

## HTML Output

Unless you have a reason not to, use HTML. It is the most flexible format—someone
can easily read it on a computer, mobile device, table, or even print it out if
they like. You can also include many types of information displays (e.g., searchable
and filterable tables with `DT::datatable()`, movies and animations, interactive 
plots with `plotly`, etc.).

### Themes and Templates

You can change the styling of your HTML documents using themes. Specify a theme
at the top of your document in the YAML header like this:

````markdown
---
title: "Habits"
output:
  html_document:
    theme: paper
---
````

RMarkdown comes with several built-in themes. You can also design and customize
your own theme. See [here](https://bookdown.org/yihui/bookdown/theming.html)
for more details. 

Many R packages also come with templates that have specific styling. For example,
the [`tufte` package](https://bookdown.org/yihui/rmarkdown/tufte-handouts.html)
comes with templates in the styling of Edward Tufte, which look quite nice:

````markdown
---
title: "Habits"
output:
  tufte::tufte_html: default
---
````

### CSS styling

In addition to controlling your styling with themes or templates, you can also 
customize your styling further for a specific document by adding CSS code chunks. 
CSS code chunks are just like R code chunks, except that they start with `{css` 
instead of `{r`. Inside these code chunks, you can add CSS code to control the 
styling of different types of elements in your document. 

For example, let's say that you wanted to include a short caption above the code
chunks in your RMarkdown document describing the code, but without distracting
from the main text. You could do that like this:

````markdown
`r ''````{css, Style for Code Chunk Captions, echo = FALSE}
h6 {margin-bottom: 0;}
```

###### Description of the code chunk below
`r ''````{r, Chunk Label}
x = rnorm(100)
```
````

The CSS code chunk at the top adds styling for level-6 headings (written in Markdown
with 6 `######`). `margin-bottom: 0` says to not put any space between the bottom
of a level-6 heading and the next paragraph. Then, when I want to write a description
for a code chunk, I add it to the Markdown with 6 `#`. 

(Note, depending on your theme, you might need to specify additional style
information for `h6` to override heading defaults.)

For a nice introduction to styling with CSS, see [this guide](https://www.w3schools.com/Css/css_intro.asp).


### Showing and hiding code chunks

When you include code chunks in your RMarkdown documents, you can control whether
the code is shown in your rendered document using the `echo` argument:

````markdown
`r ''````{r, chunk-label, echo = TRUE}
````

If you set `echo = TRUE`, the code will be included in your rendered document, 
along with its output. If you set, `echo = FALSE`, the code will be hidden and
only the output will be shown. 

Its nice to include the code to show how the results were produced, but this can
be ugly and distracting as part of a larger document that isn't intended to
necessarily be educational. 

One of the nice things about HTML output is that you can have it both ways by
enabling the `code_folding` option in your document's YAML header at the top 
of the document. 

````markdown
---
title: "Habits"
output:
  html_document:
    code_folding: hide
---
````

`code_folding: hide` will include the code (when `echo = TRUE`), but have it
hidden by default. `code_folding: show` will show all code blocks by default, 
but still allow readers to hide them if they wish.


### Tables

So far in class, we have formatted tables using `knitr::kable()` for small 
tables and `DT::datatable()` for larger tables. Sometimes you want more control
over the formatting of your tables. There are many packages available for
customizing table formatting. See [here](https://gt.rstudio.com/#how-gt-fits-in-with-other-packages-that-generate-display-tables)
for a list of options. 

Here are a few packages that I think you should explore.


| Package | Notes |
|---------|-------|
| [`kableExtra`](https://haozhu233.github.io/kableExtra/) | Very popular. Can output to HTML or PDF, not Word (though can output to an image). |
| [`gt`](https://gt.rstudio.com/) | New package from RStudio. Designed to have a ggplot-like syntax, but for tables. Can output to HTML currently. PDF output is planned. Not yet mature. |
| [`flextable`](https://davidgohel.github.io/flextable/) | Can output to Word, PowerPoint, and HTML (and PDF sort of). |
| [`huxtable`](https://hughjonesd.github.io/huxtable/) | Can output to HTML, PDF, and Word/PowerPoint (via internal calls to the `flextable` package. |
| [`formattable`](https://renkun-ken.github.io/formattable/) | Has very useful `color_tile()` and `color_bar()` functions. Can be [combined with `kableExtra`](https://haozhu233.github.io/kableExtra/use_kableExtra_with_formattable.html). |
| `apaTables` | Formats a variety of statistical models according to APA style guidelines. |


As an example, here is a table customized using `kableExtra` to:

  1. Center the columns.
  2. Make the table the only as wide as it needs to be (instead of the full
     width of the page).
  3. Add several types of footnotes. 
  
```{r}
library(knitr)
library(kableExtra)
dt <- mtcars[1:5, 1:6]
kable(dt, align = "c") %>%
  kable_styling(full_width = FALSE) %>%
  footnote(general = "Here is a general comments of the table. ",
           number = c("Footnote 1; ", "Footnote 2; "),
           alphabet = c("Footnote A; ", "Footnote B; "),
           symbol = c("Footnote Symbol 1; ", "Footnote Symbol 2")
           )
```


## PDF Output

PDF is a format if the document is intended to be printed. It's also the format
most folks expect "final" versions of papers to be distributed in. It's not as
flexible as HTML, unfortunately.

### TeX

PDF output requires TeX, a widely-used but very fiddly, typesetting engine. The
easiest way to install and work with TeX using RMarkdown is the `tinytex` package.
Install the `tinytex` R package, then use the `tinytex::install_tinytex()` function
to install the TeX program on your computer.

```{r, eval = FALSE}
install.packages("tinytex")
tinytex::install_tinytex()
```

After you install tinytex, you can tell RMarkdown to output to PDF, either using
default `pdf_document` or `beamer_presentation` formats, or other templates, such
as `tufte::tufte_handout` or `papaja::apa6_pdf`.

````markdown
---
title: "Habits"
output:
  pdf_document:
    latex_engine: lualatex
  tufte::tufte_handout:
    latex_engine: lualatex
---
````

When you output to a PDF format, always set the `latex_engine: lualatex` argument
to get the best output and to avoid problems with mathematical and non-English
symbols.

### Tables

For PDF output, your best options for tables beyond `knitr` currenly are probably 
`kableExtra` and `huxtable`. `gt` might be a good option in the future.

### Customizing LaTeX Templates

You can control additional parameters of your PDF output (e.g., whether a table
of contents is included, formatting of various types of text or objects) by 
adding arguments to your YAML header or even supplying a custom LaTeX template.
See [here](https://bookdown.org/yihui/rmarkdown/pdf-document.html) for more details.


## Word and PowerPoint Output

RMarkdown can also output to Word or PowerPoint formats. Generally, you should 
regard these as _output_ formats, not source files—if you need to edit a document,
do it in the RMarkdown, not in the Word or PowerPoint output. 

That said, a lot of folks (even me!) find it more comfortable to write in Word
or make presentations in PowerPoint. For example, when presenting at conferences,
you might have to use PowerPoint instead of a PDF viewer. 

With that in mind, you should strive to make your Office documents as
_reproducible as possible_. For example, consider writing your Methods and 
Results sections and your Tables and Figures in RMarkdown to take advantage of 
the reproducibility of RMarkdown.  When you update these sections, update them 
in RMarkdown, render to Word, then cut-and-paste them as a whole into your 
document in the right spot. This will make updating your results fast, while 
still letting you and your collaborators work in the Word program generally. 

### Basic Word or PowerPoint output

Basic output to Word or PowerPoint is done using the built-in `word_document`
and `powerpoint_presentation` output formats. You could also use other templates,
such as `papaja::apa6_word` or `redoc::redoc` (see below).

You can provide a template Word or PowerPoint file with customized heading styles,
template text, etc. (e.g., such as [this one](https://github.com/psychmeta/psychmeta/raw/master/inst/templates/reference_docx.docx))
by specifying the `reference_docx` or `reference_pptx` arguments.

````markdown
---
title: "Habits"
output:
  word_document:
    reference_docx: word_template.docx
  powerpoint_presentation:
    reference_pptx: powerpoint_template.pptx
---
````

### Tables

For Word or PowerPoint output, your best options for tables beyond `knitr` 
currenly are probably `flextable` and `huxtable`.

### `redoc`

[`redoc`](https://noamross.github.io/redoc/) is a great package if you (or one
of your collaborators) really prefers to write in Word instead of Markdown. With 
`redoc`, you can render your RMarkdown document to Word, send it for comments 
and revisions to your colleage, then re-import the marked-up document back into 
Markdown for further editing. It's a great way to combine the familiarity of 
Word with the reproducibility of Markdown. See [here](https://noamross.github.io/redoc/) 
for a walkthrough.

### `tidystats`

The [`tidystats` package](https://www.willemsleegers.com/tidystats.html) takes
a somewhat different approach to reproducibility with Word. Instead of rendering
the whole Word document, it outputs the statistical results to a computer-readable
file Then, using a companion [Word plugin](https://github.com/WillemSleegers/tidystats-Word-add-in), 
you insert a holder for the results into your document. If you later update the 
results file by running new analyses in R, the results in Word will also update. 
It's pretty slick.

### `officedown`

If you want even more control over your Word or PowerPoint documents, the 
[`officedown`](https://github.com/davidgohel/officedown) package provides a 
variety of features to control each aspect of your file. The 
[`officer` package](https://davidgohel.github.io/officer/) provides even more 
extensive control, but doesn't use Markdown. `officer` can be useful if you output 
to Word using Markdown, but then need to make a few adjustments to the rendered 
file afterward.


## APA-style Manuscripts

If you are writing an APA-style manuscript, the [papaja package](https://crsh.github.io/papaja_man/)
can do a lot of the formatting work for you to output to Word (`papaja::apa6_word`)
or PDF (`papaja::apa6_pdf`), as well as a lot of the work to format your 
statistical results according to the APA Style Manual. Check it out!

     

<!--chapter:end:s015_preparing-documents.Rmd-->

# Citations with Pandoc and Zotero

Formatting and managing citations and bibliographies is often one of the more
annoying and tedious parts of writing a paper or report. Fortunately, you can
automate this process using RMarkdown. pandoc, the program that converts your
document from Markdown to other formats, has a built-in citation formatter.
This week, you will practice using Markdown to insert and format citations.

## Resources

- [The RStudio tutorial on RMarkdown citations](https://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html)
- [Zotero](https://www.zotero.org/), a free and powerful citation management program
  - My professional hobby (I wrote and maintain the Zotero/CSL style for APA)
  - [Download the Zotero app and browser connector](https://zotero.org/download)
  - [Zotero Style Repository](https://zotero.org/)
  - [Zotero Quick Start guide](https://www.zotero.org/support/quick_start_guide)
- [The BetterBibTeX Zotero plugin](https://github.com/retorquere/zotero-better-bibtex/releases),
  how we connect RStudio to Zotero
- [The `rbbt` RStudio add-in](https://github.com/paleolimbot/rbbt)
  - Connect RStudio to Zotero
  - Search your Zotero library for citations
  - Format your citations and generate your bibliography file for you
- [The `citr` RStudio add-in](https://cran.r-project.org/web/packages/citr/index.html)
  - Connect RStudio to Zotero
    - Generate your bibliography file for you
    - Doesn't work well if you have a very large Zotero library
  - Search your Zotero library or a standalone bibliography file for citations
  - Format your citations for you
  

## Basics of Citing with Markdown

You add citations to your Markdown document by typing something like this:
`[@Wiernik2016IntraindividualPersonality]`. The `@` sign tells pandoc that you 
are inserting a citation. The `Wiernik2016IntraindividualPersonality` part is 
the "citation key"—an ID label used to indicate what reference you are citing. 
When pandoc formats this citation, it will look up this citation key in your
bibliography file (see below) for all of the item data (authors, title, year, 
etc.). pandoc will go through your whole document, formatting all of the 
citations according the the rules of your chosen citation style, then add the bibliography to the end. It's like magic.


## Your Bibliography File

To format your citations, you need to provide pandoc with a file that has your
bibliography in a structured, computer-readable format. The best format to use
is **CSL JSON** (.json).

This format looks like this:

```
[{
	"id": "Wiernik2016IntraindividualPersonality",
	"type": "article-journal",
	"author": [{
		"family": "Wiernik",
		"given": "Brenton M."
	}],
	"issued": {
		"date-parts": [
			[2016, 8, 1]
		]
	},
	"title": "Intraindividual personality profiles associated with Realistic interests",
	"container-title": "Journal of Career Assessment",
	"container-title-short": "J. Career Assess.",
	"volume": "24",
	"issue": "3",
	"page": "460-480",
	"DOI": "10.1177/1069072715599378",
	"language": "en"
}]
```

You can see that all of the information needed to make a reference is included
here (author, title, date, journal title, DOI, etc.). We will use a program 
to automatically generate this structure for us (see the 
[Zotero section below](#zotero)).

_Note:_ The name "CSL JSON" is two acronyms. CSL stands for "Citation Style 
Language"—the programming language the citation styles pandoc uses are written
in. JSON stands for "JavaScript Object Notation"—JSON is a standard  format used
by many computer programs to store data. They are lots of different types of JSON
formats; CSL JSON is specifically the format used to store bibliographic reference
information. CSL JSON is also sometimes called "citeproc JSON" or "JSON citeproc".

CSL JSON is the best format to use for storing bibliographic data for pandoc.
CSL JSON is pandoc's citation engine's native data format, so it can be imported
and processed without any data loss. You can use 
[other data formats](https://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html),
but you might lose some information or get incorrectly formatted citations if
you do. For example, the BibTeX/BibLaTeX (.bib) format is very popular, but is 
has major limitations if you need to cite anything other than a journal article, 
book, chapter, or website. I recommend you avoid it if possible. 

After you have compiled your bibliographic database, add it to the YAML header
in your RMarkdown file using the `bibliography` tag:

```
---
title: My Amazing Research Paper Title
author: Brenton M. Wiernik
output_format:
  html_document: 
    code_folding: hide
bibliography: references.json
```

When you knit the document, pandoc will look for the `references.json` file
in the folder where your .Rmd file is.


## Adding Citations to Your Document

To add a citation to a document, type them like this: `[@Wiernik2016]`. 
Citations go inside square brackets. For each citation, start with `@`, then
add the citation key from your bibliography file (see
[below](#zotero) for where to find the citation keys in Zotero). 

To cite multiple items in the same citation, separate the citekeys with a 
semicolon: 

`[@Wiernik2016; @Schmidt1977]`

To add page numbers, type `, p.` or `, pp.` after the citekey, then type the 
page number:

`[@Wiernik2016, p. 123-124]`

You can refer to other types of locators (e.g., chapter, section, paragraph, 
etc.) using the locator abbreviations listed ([here](https://zotero-odf-scan.github.io/zotero-odf-scan/)).

You can also add prefixes or suffixes to your citation:

`[e.g., see @Wiernik2016, p. 125, for a great example]`

`[@Wiernik2016, pp. 123-124; but see @Freud1913 for counterexample]`

To write an in-text citation (outside of parentheses), omit the square brackets:

`@Wiernik2016 [p. 126]`

You can also suppress the author by typing `-` before `@`:

`Wiernik said, "This is an excellent study" [-@Wiernik2016, p. 123]`

To add a citation to your bibliography without citing it in text, add a 
`nocite:` field to your YAML metadata at the top of the document and enter
the citekeys there:

```
---
nocite: |
  @Hunter1988, @Ones1993
---
```


## Setting a Citation Style

pandoc formats references using Citation Style Language (.csl) styles.
You can find styles for thousands of different journals at the
[Zotero Style Repository](https://zotero.org/styles).

To specify the style for your document, add a `csl:` filed to your YAML
metadata and give the URL for the CSL style:

```
---
csl: https://www.zotero.org/styles/apa
---
```

You can find the URL for a style in the Zotero Style Repository by searching
for it, right-clicking on the style name in the search results, and selecting
"Copy Link".

I've written some additional versions of APA style (e.g., for meta-analyses)
that are available [here](https://github.com/bwiernik/zotero-tools). To use one
of these styles, click on the file name, then click the `Raw` button, then 
copy the URL for the page (e.g., [](https://raw.githubusercontent.com/bwiernik/zotero-tools/master/apa-meta-analysis.csl)).

(You can also give the path to a local `.csl` file saved on your computer, such
as in the same folder as your `.Rmd` file, but if you have an internet
connection, I usually find it easier to just point to the style online.)


## Zotero and BetterBibTeX

The easiest way to manage bibliography files and citekeys for writing is to use
the free and powerful program **Zotero**. Zotero is a reference management program.
You can use it to organize refernce information for items, organize PDFs and notes
for your research, share references with collaborators, and automatically format
references and citations in Word, Google Docs, LibreOffice, BibTeX, and Markdown.

(Zotero is also a personal hobby of mine. I wrote and maintain the Zotero CSL
style for APA style.)

### Installing Zotero

To install Zotero, download the installer from [here](https://zotero.org/download)
and run it. Also install the Zotero Connector for your web browser.

After installing Zotero, open up the [Zotero Preferences window](https://zotero.org/support/preferences)
to the "Cite" pane. Check the box for the "Include URLs for paper articles" option.

On the Zotero website, make a user account, then enter your account information
on the "Sync" pane of Zotero Preferences. This isn't strictly necessary, but it
is how you sync your items and files across computers and share items in Zotero
Groups.

### Saving Items to your Zotero Library

To add items to your library, the best method is to click the 
[Save to Zotero button](https://www.zotero.org/support/adding_items_to_zotero) 
in your web browser while on the publisher webpage. 

See [this page](https://www.zotero.org/support/adding_items_to_zotero) for other
options for adding items to your library.

**Important:** _Always_ double check the item data after importing. Some sites
provide low-quality data, and it's important to check and correct this right away.

A few notes on item data:
  - Always store authors' full names (not just initials) if possible. This will
    ensure proper disambiguation behavior in APA and other styles.
  - Always store item titles and book titles in sentence case (only first word
    and proper nouns capitalized; all other words in lowercase, including after
    a colon or dash). pandoc will make words uppercase as prescribed by the style
    rules.
  - Always be sure that the item has a DOI number. This is the single most-important
    part of a referecne, so be sure that
    - Install my [DOI Manager plugin](https://github.com/bwiernik/zotero-shortdoi/releases)
      to automatically retrieve a DOI for items on import if it is missing.
    - Right click on the plugin's `.xpi` file and choose Save Link As. Then,
      in Zotero, click Tools → Add-ons. Then, drag the `.xpi` file you downloaded
      onto the window that pops up.
      
See [here](https://www.zotero.org/support/kb/item_types_and_fields) for a description
of Zotero item types and fields.
      
### Installing and Setting Up BetterBibTeX

BetterBibTeX is a plugin for Zotero that expands its ability to work with Markdown
and LaTeX. 


After installing, open the [Zotero Preferences window](https://zotero.org/support/preferences)
to the BetterBibTeX pane. Here, you can set a few options. Some useful ones to
look at are:

On the Citation Keys tab:
  1. Citation key format
    - You can control what your citation keys look like. It's important for your
      keys to be unique, so don't do just `AuthorYear`. Instead, also include
      some words from the title. I use this format:
      - `[auth][year][veryshorttitle2_2]`
      - This looks like `Schaufeli2004JobDemands`
  2. QuickCopy format
    - Set this to "Pandoc"
  3. Surround Pandoc citations with brackets
    - Should pandoc citations be copied with or without brackets?

After installing BetterBibTeX, you can see the citation key for an item at the
top of the item information pane on the right side of the Zotero window. You
can click on this to select the key and copy it.

You can also manually set the citation key for an item by typing it like this
in the Extra field at the bottom of the item information pane:
  - `Citation key: Freud1913InterpretationDreams`

### Other Plugins

There are several other useful Zotero plugins you might want to use. See a
list of available plugins [here](https://zotero.org/support/plugins)

### Getting Help with Zotero

I am happy to answer Zotero questions. You can also ask questions on the
[Zotero forums](https://forums.zotero.org/) (I will probably be the one to
answer your questions there, too). You might also find the 
[Zotero documentation](https://zotero.org/support/) helpful.


## Inserting Citations with `rbbt`

You can manually copy the citekeys for your items from your Zotero library
to your Markdown document, but there are also two different RStudio plugins 
that can make this easier.

The first plugin is the `rbbt` R package. This package can connect RStudio with
BetterBibTeX, letting you use the same interface to insert citations into RMarkdown
as Zotero uses in Word and Google Docs.

First, install `rbbt` from GitHub:

```
remotes::install_github("paleolimbot/rbbt")
```

Then, in RStudio, click Tools → Addins → Browse Addins… Click Keyboard Shortcuts…
and search for Zotero. Add a shortcut for the Insert Zotero Citation command
(I use Alt + Z). Click Okay.

To insert a citation, first make sure Zotero is open. Then, type the shortcut 
you just set. The Zotero Add Citation window will pop up. You can search for an 
item by author, title, and/or year.

You can add page numbers, prefixes, or suffix from this popup window (see 
[here](https://www.zotero.org/support/word_processor_plugin_usage#customizing_cites) 
for details), or you can manually type them in your Markdown document
(but always add them from the popup window if you are writing in Word, LibreOffice,
or Google Docs).

When you are finishing writing and are ready to knit, add this line to your YAML
metadata: ``bibliography: "`r rbbt::bbt_write_bib("bibliography.json", overwrite = TRUE)`"`` 


## Inserting Citations with `citr`

The `citr` package is similar to `rbbt`, but it builds its own interface for
searching a .bib file or your Zotero libary. 

There are a couple of important limitations to `citr` that make me recommend `rbbt`
over it:

  1. It exports items as BibLaTeX instead of CSL JSON, so citations for items 
     that aren't journal articles, books, or chapters will often be wrong.
  2. It can have a hard time loading and running if your Zotero library is very
     large. 
     
If you do want to use `citr`, you can install it from CRAN:

```
install.packages('citr')
```

Then, in RStudio, click Tools → Addins → Browse Addins… Click Keyboard Shortcuts…
and search for Insert citation. Add a shortcut for the Insert citations command. 
Click Okay.

To insert a citation, first make sure Zotero is open. Then, type the shortcut 
you just set. Click the "Connect and Load Libraries" link at the bottom of the 
popup window. (If you are a member of many Group libraries, first click the 
Settings button at the bottom of the page and select only the libraries you
want to search; this can help to speed `citr` up.)

After your libraries are loaded, search for an item by author, title, and/or year.

`citr` will automatically build a `.bib` file with the references you cite as you
go. When you are done writing, remove unneeded references from your `.bib` file
by running the `tidy_bib_file()` function. 

Be sure to add `bibliography: references.bib` to your YAML metadata.


## Activities

- Import some journal journal articles or other items into your Zotero library
- Add citations to these items into an RMarkdown document
  - You can do this manually or using `rbbt` or `citr`
- Knit your Markdown document and see the formatted citations and bibliography
- When you write your final report for class, insert citations using Zotero

<!--chapter:end:s016_citations.Rmd-->

---
title: "Scraps"
author: "Brenton M. Wiernik"
date: "1/18/2021"
output: html_document
---

## Welcome to R

Now, we'll get you up to speed with a minimum "need to know" about using R and 
RStudio. We're going to assume you know nothing and start from the basics. We will
explore more of the R/RStudio landscape as the semester goes on.

The format of today's notes aim to teach R by exploration, so is essentially an 
activity guide with prompts for exploration. These are mostly all exercises 
we'll be doing together in class. 


## Why R?

Why R? Some points taken from [adv-r: intro](http://adv-r.had.co.nz/Introduction.html):

- Free, platform-wide
- Open source
- Comprehensive set of "add on" packages for analysis
- Huge community
- …

Alternatives exist for data analysis, python being another excellent tool, 
especially these days as more and more R-like functionality is added to it. The 
good thing about python is that it's faster and has better support for machine 
learning models. 



## Rmd Presentations

You can also make presentation slides using Rmd. A great resource is Yihui's 
[Rmd book, "Presentations" section](https://bookdown.org/yihui/rmarkdown/presentations.html).

Some types of formats:

- ioslides
- [xaringan](https://slides.yihui.name/xaringan/#1)
- [slidy](https://www.w3.org/Talks/Tools/Slidy2/#(1))
- [reveal.js](https://revealjs.com/#/)
- ...


### Activity: Exploring ioslides

Let's turn the file we've been working on into slides.

Together:

1. In RStudio, go to "File" -> "New File" -> "RMarkdown" -> 
   "Presentation" -> "ioslides". Explore!
2. Clear everything below the YAML header.
3. Copy and paste the tibble exploration we've been working on 
   (without the YAML header), and turn them into slides.

<!--chapter:end:z000.Rmd-->

# Miscellaneous topics

## GitHub Pages

You can turn your GitHub repository into a website, by enabling __GitHub Pages__
on that repo. This is useful for something as small as being able to display 
HTML files without downloading a local copy of the repository, to something as 
big as making a full fledged website like the [course website](https://wiernik-datasci.netlify.com).

- If you make a GitHub repo called `yourusername.github.io`, and enable GitHub 
  Pages on that repo, then the URL of the website will be `https://yourusername.github.io/`.
- If you enable GitHub pages on any other repo, the URL for that repo will be
  `https://yourusername.github.io/name_of_other_repo`.
- Your homework repo's website URL will be: 
  `https://usf-psych-datasci-2020.github.io/DataSci-hw-<your-username>/`.

Learn more with GitHub's [GitHub Pages](https://pages.github.com/) tutorial.

### Practice with HTML file linking

We'll practice linking to an HTML file for today's exercise, by following the instructions on the (new!) ["Viewing and Linking to HTML Files"](https://wiernik-datasci.netlify.com/evaluation/assignments/#viewing-and-linking-to-html-files) on the assignments home page.

<!--chapter:end:z001.Rmd-->

# Making ggplots interactive with plotly

## Plotly demo

You can make interactive graphs and plots in R using the plotly library. 
This is a demo of what plotly is and why it's useful, and then you can try 
converting a static ggplot graph into an interactive plotly graph.

*For this demo, make sure you have the following packages installed and loaded:*

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(gapminder)
library(plotly) 
```

### Make `ggplot2` graphs interactive

It's very easy to convert an existing ggplot2 graph into an interactive graph with `plotly::ggplotly`

On the below graph, explore the interactive options:

- *Hover* your cursor over individual points
- *Zoom* in and out by dragging across / using the zoom tool
- *Single-* and *double-click* items on the legend to isolate groups of points
- While zoomed-in, use the *pan* tool to "move" around the plot, google maps style!

```{r}
p <- gapminder %>%
    ggplot(aes(x = gdpPercap, y = lifeExp, color = continent)) +
    geom_point() 

p %>%
    ggplotly()
```

### Make interactive plots with `plotly::plot_ly`

We can also make interactive graphs using the the `plotly::plot_ly` function:

```{r}
p <- gapminder %>%
    plot_ly(x = ~gdpPercap,
            y = ~lifeExp,
            color = ~continent,
            
            # mode specifies the geometric object e.g. "markers" for points, "line" for lines
            mode = 'markers',
            
            # type controls the "type" of graph e.g. 'bar', 'scatter'
            type = 'scatter'
            )

p
```

### Share with others

To share with others:

1. Create a plotly account @ [plot.ly](plot.ly)
2. Navigate to settings, and take in the following information:
- your user name
- api key 

Now, we will tell R your account information so that we can upload these plots 
to the web.

Note that once we run `api_create()`, the browser will open to a webpage 
displaying your interactive plot. You can share this page with others, but they 
will only be able to **view**. If you want others to be able to **edit** the 
graph, you can invite others to "*collaborate*" in the "*Sharing link*" option. 

```{r eval = FALSE}
# fill in the below with your information
Sys.setenv("plotly_username"="your_plotly_username")
Sys.setenv("plotly_api_key"="your_api_key")

# upload our plots to the website
api_create(p, filename = 'name-of-your-plot')
```

<!--chapter:end:z002.Rmd-->

# Final Project Guidelines

Your final project 

You GitHub repo should:

  1. Be well-organized and documented with README files
  2. Include scripts to:
     1. Import your data
        - If you are simulating data, then code for that instead
     2. Wrangle data
     3. Do analyses and data exploration
     4. Generate visualizations, tables, and other output
     5. Save output tables and/or figures as separate files
     6. Render a document describing your analyses and conclusions
        - Treat this like a report someone might actually like to read
        - You don't have to go very in depth; the project is more about the coding
          than the science here, so it can be a short report.
  3. Your rendered document should be rendered to HTML and Word
     - Bonus: Also render to PDF
  4. Your rendered document must
     1. Include at least 1 figure presenting useful data in an effective
        and appealing way
     2. Include at least 1 table formatted effectively
     3. Have your code following a consistent and readable style
        - Code blocks should be no more than 100 characters wide to prevent 
          overflow off the page
     4. Insert all statistical results using code blocks or in-line R code
        - Don't manually type results into your document!
     5. Include your R `devtools::session_info()` output
     6. Bonus: Include citations using pandoc/Zotero
     

<!--chapter:end:z003.Rmd-->

